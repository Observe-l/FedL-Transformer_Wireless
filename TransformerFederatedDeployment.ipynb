{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Iym7D4X9YL5N"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-20 16:09:08.480301: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-05-20 16:09:08.499061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-20 16:09:08.499077: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-20 16:09:08.499557: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-20 16:09:08.502865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-20 16:09:08.875351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKiSl2fMXucI"
      },
      "outputs": [],
      "source": [
        "# Transformer Definition (Dependencies)\n",
        "\n",
        "# Multi-head attention with Q, K, V\n",
        "class multiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, key_dim, num_heads):\n",
        "        super(multiHeadAttention, self).__init__()\n",
        "        self.key_dim = key_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.WqL = []\n",
        "        self.WkL = []\n",
        "        self.WvL = []\n",
        "\n",
        "        for i in range(self.num_heads):\n",
        "            Wq_init = tf.random_normal_initializer()\n",
        "            Wq = tf.Variable(initial_value=Wq_init(shape=(int(input_shape[-1]), self.key_dim), dtype=\"float32\"), trainable=True)\n",
        "            self.WqL.append(Wq)\n",
        "\n",
        "            Wk_init = tf.random_normal_initializer()\n",
        "            Wk = tf.Variable(initial_value=Wk_init(shape=(int(input_shape[-1]), self.key_dim), dtype=\"float32\"), trainable=True)\n",
        "            self.WkL.append(Wk)\n",
        "\n",
        "            Wv_init = tf.random_normal_initializer()\n",
        "            Wv = tf.Variable(initial_value=Wv_init(shape=(int(input_shape[-1]), int(input_shape[-1])), dtype=\"float32\"), trainable=True)\n",
        "            self.WvL.append(Wv)\n",
        "\n",
        "        Wlt_init = init = tf.random_normal_initializer()\n",
        "        self.Wlt = tf.Variable(initial_value=Wlt_init(shape=((self.num_heads * int(input_shape[-1])), int(input_shape[-1])), dtype=\"float32\"), trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        # inputs : batch_size x time_steps x dim\n",
        "        x = inputs\n",
        "\n",
        "        # transform for generating Q,K,V : (batch_size * time_steps) x dim\n",
        "        x_tran = tf.reshape(x, [-1])\n",
        "        x_tran = tf.reshape(x_tran, [-1, int(inputs.shape.as_list()[-1])])\n",
        "\n",
        "        a_xL = []\n",
        "\n",
        "        # Generate Query, Key and Value corresponding to each attention head\n",
        "        for i in range(self.num_heads):\n",
        "\n",
        "            # Query : batch_size x time_steps x dq\n",
        "            xq = tf.matmul(x_tran, self.WqL[i])\n",
        "            xq = tf.reshape(xq, [-1, int(inputs.shape.as_list()[-2]), int(xq.shape.as_list()[-1])])\n",
        "\n",
        "            # Key : batch_size x time_steps x dk\n",
        "            xk = tf.matmul(x_tran, self.WkL[i])\n",
        "            xk = tf.reshape(xk, [-1, int(inputs.shape.as_list()[-2]), int(xk.shape.as_list()[-1])])\n",
        "\n",
        "            # Value : batch_size x time_steps x dv\n",
        "            xv = tf.matmul(x_tran, self.WvL[i])\n",
        "            xv = tf.reshape(xv, [-1, int(inputs.shape.as_list()[-2]), int(xv.shape.as_list()[-1])])\n",
        "\n",
        "            # Transposing each key in a batch (xk_t : batch_size x dk x time_steps)\n",
        "            xk_t = tf.transpose(xk, perm=[0, 2, 1])\n",
        "\n",
        "            # Computing scaled dot product self attention of each time step in each training sample (s_a : batch_size x time_steps x time_steps)\n",
        "            s_a = tf.math.multiply(tf.keras.layers.Dot(axes=(1, 2))([xk_t, xq]), (1/self.key_dim))\n",
        "\n",
        "            # Applying Softmax Layer to the self attention weights for proper scaling (sft_s_a : batch_size x time_steps x time_steps)\n",
        "            sft_s_a = tf.keras.layers.Softmax(axis=2)(s_a)\n",
        "\n",
        "            # Computing attention augmented values for each time step and each training sample (a_x : batch_size x time_steps x dim)\n",
        "            a_xL.append(tf.keras.layers.Dot(axes=(1, 2))([xv, sft_s_a]))\n",
        "\n",
        "        # Concatenate and applying linear transform for making dimensions compatible\n",
        "        a_x = tf.concat(a_xL, -1)\n",
        "\n",
        "        # Transform to shape a_x_tran : ((batch_size x time_steps) x (dim x num_heads))\n",
        "        a_x_tran = tf.reshape(a_x, [-1])\n",
        "        a_x_tran = tf.reshape(a_x_tran, [-1, (self.num_heads*int(inputs.shape.as_list()[-1]))])\n",
        "\n",
        "        # Get the dimensions compatible after applying linear transform\n",
        "        a_x_tran = tf.matmul(a_x_tran, self.Wlt)\n",
        "        a_x_tran = tf.reshape(a_x_tran, [-1, int(inputs.shape.as_list()[-2]), int(inputs.shape.as_list()[-1])])\n",
        "\n",
        "        return a_x_tran\n",
        "\n",
        "\n",
        "# Transformer Block implemented as a Layer\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = multiHeadAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class PositionEmbeddingLayer(layers.Layer):\n",
        "    def __init__(self, sequence_length, output_dim, **kwargs):\n",
        "        super(PositionEmbeddingLayer, self).__init__(**kwargs)\n",
        "        self.position_embedding_layer = layers.Embedding(\n",
        "            input_dim=(sequence_length), output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def call(self, inputs):\n",
        "        position_indices = tf.range(self.sequence_length)  #tf.range(1, self.sequence_length + 1, 1)\n",
        "        embedded_words = inputs\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PILGxmbEZE-M"
      },
      "outputs": [],
      "source": [
        "# Creating the model\n",
        "# Initializing the transformer model\n",
        "def get_transformer_model(num_features, num_attn_heads, hidden_layer_dim, num_transformer_blocks, time_dim):\n",
        "  transformer_blocks = []\n",
        "\n",
        "  for i in range(num_transformer_blocks):\n",
        "      transformer_blocks.append(TransformerBlock(num_features, num_attn_heads, hidden_layer_dim))\n",
        "\n",
        "  # Model\n",
        "  inputs = layers.Input(shape=(time_dim, num_features,))\n",
        "  x = inputs\n",
        "\n",
        "  # Trainable Embedding\n",
        "  embedding_layer = PositionEmbeddingLayer(50, num_features)\n",
        "  x = embedding_layer(x)\n",
        "\n",
        "  for i in range(num_transformer_blocks):\n",
        "      x = transformer_blocks[i](x)\n",
        "\n",
        "  x = layers.GlobalAveragePooling1D()(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  x = layers.Dense(32, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.2)(x)\n",
        "  outputs = layers.Dense(1)(x)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  optim = keras.optimizers.SGD(learning_rate=0.0001)\n",
        "  model.compile(optimizer=optim, loss='mse', metrics=['mse'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# Can use following input arguments\n",
        "#num_attn_heads = 3\n",
        "#hidden_layer_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "#num_transformer_blocks = 3\n",
        "#num_features : Number of features at each time step (in the case of RUL prediction its number of sensors). Last dimension\n",
        "#               of the dataset\n",
        "#              Extract Using: num_features = np.asarray(X_tr).astype(np.float32).shape[-1]\n",
        "\n",
        "#time_dim : History window size (Secondlast dimension of the dataset)\n",
        "#              Extract Using: time_dim = np.asarray(X_tr).astype(np.float32).shape[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBZpFz3_jvHr"
      },
      "outputs": [],
      "source": [
        "#Extract Weights Code\n",
        "for l in range(len(model.weights)):\n",
        "  weight_l = model.weights[l].numpy()\n",
        "\n",
        "# Applying Weights Code\n",
        "for l in range(len(model.weights)):\n",
        "  model.weights[l].assign(new_weight_l)   #Replace new weight l with the target weight\n",
        "\n",
        "# Here l is not necessarily the layer, Tensorflow keeps model weights in a list its the index in the list. Some layers like\n",
        "# transfomers use multiple positions in the list to save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGrelyHuSfaJ"
      },
      "outputs": [],
      "source": [
        "#Client Node\n",
        "\n",
        "# FL parameters (Set These)\n",
        "B = None\n",
        "E = None\n",
        "num_comm_rounds = None\n",
        "\n",
        "###Define model\n",
        "# Can use following input arguments\n",
        "#num_attn_heads = 3\n",
        "#hidden_layer_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "#num_transformer_blocks = 3\n",
        "#num_features : Number of features at each time step (in the case of RUL prediction its number of sensors). Last dimension\n",
        "#               of the dataset\n",
        "#              Extract Using: num_features = np.asarray(X_tr).astype(np.float32).shape[-1]\n",
        "\n",
        "#time_dim : History window size (Secondlast dimension of the dataset)\n",
        "#              Extract Using: time_dim = np.asarray(X_tr).astype(np.float32).shape[-2]\n",
        "\n",
        "local_model = get_transformer_model(num_features, num_attn_heads, hidden_layer_dim, num_transformer_blocks, time_dim)\n",
        "\n",
        "##Put code to load local model data\n",
        "X_train, Y_train = None, None\n",
        "\n",
        "###Communication Rounds Loop\n",
        "for i in range(num_comm_rounds):\n",
        "\n",
        "  # Get global model weights from Server#\n",
        "  #----\n",
        "  # Put code here to get model weights from server (TCP/UPD) and deserialize\n",
        "  global_model = None\n",
        "  #----\n",
        "\n",
        "  # Update Local Model\n",
        "  for k in range(len(local_model.weights)):\n",
        "    local_model.weights[k].assign(global_model.weights[k].numpy())\n",
        "\n",
        "  # Train One Communication Round\n",
        "  local_model.fit(X_tr, Y_tr, batch_size=B, epochs=E)\n",
        "\n",
        "  # Send Client Weights to server\n",
        "  #---\n",
        "  # Put Code Here (Use local_model.weights to get local model weights as a list)\n",
        "  #---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4mxdBhcbgUH"
      },
      "outputs": [],
      "source": [
        "#Server Side\n",
        "\n",
        "# FL parameters (Set These)\n",
        "C = None\n",
        "num_total_clients = None\n",
        "num_clients_per_round = C * num_total_clients\n",
        "num_comm_rounds = None\n",
        "\n",
        "###Define model\n",
        "# Can use following input arguments\n",
        "#num_attn_heads = 3\n",
        "#hidden_layer_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "#num_transformer_blocks = 3\n",
        "#num_features : Number of features at each time step (in the case of RUL prediction its number of sensors). Last dimension\n",
        "#               of the dataset\n",
        "#              Extract Using: num_features = np.asarray(X_tr).astype(np.float32).shape[-1]\n",
        "\n",
        "#time_dim : History window size (Secondlast dimension of the dataset)\n",
        "#              Extract Using: time_dim = np.asarray(X_tr).astype(np.float32).shape[-2]\n",
        "\n",
        "global_model = get_transformer_model(num_features, num_attn_heads, hidden_layer_dim, num_transformer_blocks, time_dim)\n",
        "\n",
        "###Communication Rounds Loop\n",
        "for i in range(num_comm_rounds):\n",
        "\n",
        "  # Get Weights from all Clients\n",
        "  for j in num_clients_per_round:\n",
        "\n",
        "    ###Put Code Below\n",
        "    #---\n",
        "    # TCP/UPD\n",
        "    #---\n",
        "\n",
        "    #Weight Aggregation Rule\n",
        "    aggregate_weight = None\n",
        "\n",
        "  for k in range(len(global_model.weights)):\n",
        "    global_model.weights[k].assign(aggregate_weight.weights[k].numpy())\n",
        "\n",
        "  # Send Updated Model Weights to Client\n",
        "  #---\n",
        "  # Put code here, serialize aggregate_weight and send via TCP/UDP\n",
        "  #---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
