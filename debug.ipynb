{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torchvision.transforms import ToTensor\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "# from numba import njit, jit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cython_decoder import cython_sc_decoding\n",
    "from decoder_cuda import decoder_cuda\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind((host, port))\n",
    "num_nodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes = 20\n",
    "# server_socket.listen((num_nodes+1)*2)\n",
    "# node_s = []\n",
    "# node_r = []\n",
    "\n",
    "# try:\n",
    "#     while True:\n",
    "#         client_socket, addr = server_socket.accept()\n",
    "#         server_socket.settimeout(10)\n",
    "#         data = client_socket.recv(1024).decode()\n",
    "#         if data == \"Server-R\":\n",
    "#             server_s = client_socket\n",
    "#         elif data == \"Server-S\":\n",
    "#             server_r = client_socket\n",
    "#         elif data == \"Node-R\":\n",
    "#             node_s.append(client_socket)\n",
    "#         elif data == \"Node-S\":\n",
    "#             node_r.append(client_socket)\n",
    "#         client_socket.sendall(struct.pack('I',len(b\"start\"))+b\"start\")\n",
    "# except socket.timeout:\n",
    "#     print('Timeout')\n",
    "#     server_socket.settimeout(None)\n",
    "\n",
    "# for tmp_socket in node_r:\n",
    "#     tmp_socket.recv(1024)\n",
    "# server_r.recv(65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tmp_socket in node_r:\n",
    "#     tmp_socket.close()\n",
    "# for tmp_socket in node_s:\n",
    "#     tmp_socket.close()\n",
    "# server_s.close()\n",
    "# server_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"cifar100\",\n",
    "    partitioners={\n",
    "        \"train\": DirichletPartitioner(\n",
    "            num_partitions=50,\n",
    "            partition_by=\"fine_label\",\n",
    "            alpha=0.1,\n",
    "            seed=42,\n",
    "            min_partition_size=0,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "  transforms = transform_train\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "def test_transforms(batch):\n",
    "    transforms = transform_test\n",
    "    batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "train_loader=[]\n",
    "test_loader=[]\n",
    "for i in range(50):\n",
    "    partition_train_test = fds.load_partition(i, \"train\").train_test_split(0.1)\n",
    "    partition_train = partition_train_test[\"train\"].with_transform(train_transforms)\n",
    "    partition_test = partition_train_test[\"test\"].with_transform(test_transforms)\n",
    "    # centralized_dataset = fds.load_split(\"test\").with_transform(test_transforms)\n",
    "    train_loader.append(DataLoader(partition_train, batch_size=256, shuffle=True, num_workers=16))\n",
    "    test_loader.append(DataLoader(partition_test, batch_size=128, shuffle=False, num_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_small import ViT\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = []\n",
    "optimizer = []\n",
    "scheduler = []\n",
    "criterion = []\n",
    "scaler = []\n",
    "for i in range(num_nodes):\n",
    "    net.append(ViT(\n",
    "        image_size = 32,\n",
    "        patch_size = 4,\n",
    "        num_classes = 100,\n",
    "        dim = 32,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 32,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    ).to(device))\n",
    "\n",
    "\n",
    "    optimizer.append(optim.Adam(net[i].parameters(), lr=0.001))\n",
    "    scheduler.append(torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[i], 5))\n",
    "    criterion.append(nn.CrossEntropyLoss())\n",
    "    # scaler.append(torch.cuda.amp.GradScaler(enabled=True))\n",
    "    scaler.append(torch.amp.GradScaler(device='cuda:0',enabled=True))\n",
    "\n",
    "server_net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 32,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 32,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                criterion: nn.Module, \n",
    "                device: torch.device, \n",
    "                scaler: torch.amp.GradScaler, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                nodes: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"fine_label\"].to(device)\n",
    "        with torch.amp.autocast(device_type='cuda:0',enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Nodes: {nodes}, Epoch: {epoch},Train Loss: {total_loss / total_samples:.4f}, Train Accuracy: {total_correct / total_samples:.4f}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   test_loader: DataLoader, \n",
    "                   criterion: nn.Module, \n",
    "                   device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"fine_label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Validation Loss: {total_loss / total_samples:.4f}, Validation Accuracy: {total_correct / total_samples:.4f}\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cli in range(1):\n",
    "    start_time = time.time()\n",
    "    for i in range(10):\n",
    "        train_model(net[cli], train_loader[cli], criterion[cli], device, scaler[cli], optimizer[cli], i, cli)\n",
    "        # evaluate_model(net[cli], test_loader[cli], criterion[cli], device)\n",
    "        scheduler[cli].step()\n",
    "    print(f\"Time taken: {time.time()-start_time}\")\n",
    "    scheduler[cli] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[cli], 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import socket\n",
    "\n",
    "# host = '192.168.1.243'\n",
    "# port = 12345\n",
    "# s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# s.connect((host, port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "# addr = (host, 12346)\n",
    "# i = 0\n",
    "# for tmp_data in weights_dict.values():\n",
    "#     print(i)\n",
    "#     i += 1\n",
    "#     udp_socket.sendto(tmp_data.tobytes(), addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for tmp_data in weights_dict.values():\n",
    "#     print(i)\n",
    "#     i += 1\n",
    "#     s.sendall(tmp_data.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gn(length:int, device):\n",
    "    n = int(np.log2(length))\n",
    "    f=np.array([[1,0],[1,1]], dtype=bool)\n",
    "    g_n = f\n",
    "    for _ in range(n-1):\n",
    "        g_n = np.kron(g_n, f)\n",
    "    g_order = rvsl(np.arange(length))\n",
    "    g_n = g_n[:, g_order]\n",
    "    g_n = torch.tensor(g_n, dtype=torch.float32).to(device)\n",
    "    return g_n\n",
    "\n",
    "def rvsl(y: np.ndarray) -> np.ndarray:\n",
    "    N = y.shape[0]\n",
    "    if N == 2:\n",
    "        return y\n",
    "    else:\n",
    "        return np.concatenate((rvsl(y[0:N:2]), rvsl(y[1:N:2])))\n",
    "\n",
    "def data_process(array, data_idx):\n",
    "    bit_array = np.frombuffer(array.tobytes(), dtype=np.uint8)\n",
    "    info_len = int(len(data_idx) / 8)\n",
    "    array_len = bit_array.shape[0] * 8\n",
    "    if bit_array.shape[0] % info_len != 0:\n",
    "        padding = 255 * np.ones((info_len - bit_array.shape[0] % info_len), dtype=bit_array.dtype)\n",
    "        bit_array = np.concatenate((bit_array, padding))\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array.reshape(-1, len(data_idx))\n",
    "    current_array = np.zeros((bit_array.shape[0], 1024), dtype=np.uint8)\n",
    "    current_array[:, data_idx] = bit_array\n",
    "    current_idx = bit_array.shape[0]\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def udp_packet_to_numpy_array(packet, codeword_num):\n",
    "    bit_array = np.frombuffer(packet, dtype=np.uint8)\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array[:codeword_num]\n",
    "    return bit_array\n",
    "\n",
    "def codeword_generate(array_dict, data_idx, gn):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "    for name in array_dict:\n",
    "        current_array, current_idx, array_len = data_process(array_dict[name], data_idx)\n",
    "        split_bit.extend(current_array)\n",
    "        codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "        bit_array_len.append(array_len)\n",
    "    split_bit = torch.tensor(np.array(split_bit), dtype=torch.float32).to(\"cuda:0\")\n",
    "    tmp_codeword = torch.matmul(split_bit, gn) % 2\n",
    "    codeword = np.array(tmp_codeword.cpu().detach().numpy(), dtype=np.int8)\n",
    "    del split_bit, tmp_codeword\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return codeword, codeword_idx, bit_array_len\n",
    "\n",
    "def packet_diffusion(codeword):\n",
    "    if codeword.shape[0] % 8 != 0:\n",
    "        padding = np.ones((8 - codeword.shape[0] % 8, 1024), dtype=codeword.dtype)\n",
    "        udp_numpy = np.concatenate((codeword, padding)).T\n",
    "    udp_numpy = np.packbits(udp_numpy.flatten()).reshape(1024,-1)\n",
    "    udp_packet = [struct.pack(\"I\", idx) + udp_numpy[idx].tobytes() for idx in range(udp_numpy.shape[0])]\n",
    "    return udp_packet\n",
    "\n",
    "def encoder_udp(array_dict, data_idx, gn):\n",
    "    codeword, codeword_idx, bit_array_len = codeword_generate(array_dict, data_idx, gn)\n",
    "    udp_packet = packet_diffusion(codeword)\n",
    "    return udp_packet, codeword_idx, bit_array_len, codeword.shape[0]\n",
    "\n",
    "def packet_aggregation(udp_packet, codeword_num, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    udp_idx = []\n",
    "    packet_del = []\n",
    "    for tmp_packet in udp_packet:\n",
    "        udp_idx.append(struct.unpack(\"I\", tmp_packet[:4])[0])\n",
    "        packet_del.append(udp_packet_to_numpy_array(tmp_packet[4:], codeword_num))\n",
    "    packet_del = np.array(packet_del)\n",
    "    packet_data = np.ones((1024, codeword_num)) * 0.5\n",
    "    packet_data[udp_idx] = packet_del\n",
    "\n",
    "    restore_codeword = packet_data.T\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        bit_array = np.packbits(tmp_array)\n",
    "        restore_array.append(np.frombuffer(bit_array.tobytes(), dtype=np.float32))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "def decoding(bit_array, freeze_idx, data_idx):\n",
    "    # Prepare the necessary arrays and values\n",
    "    bit_array = 1-2*bit_array\n",
    "    lr0 = np.exp(-(bit_array - 1)**2)\n",
    "    lr1 = np.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "    delete_num = 1024 - len(bit_array)\n",
    "    hd_dec = np.zeros(1024, dtype=np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    pro_prun = np.zeros((1, 2 * 1024 + 1), dtype=np.float64)\n",
    "\n",
    "    # Call the optimized Cython function\n",
    "    i_scen_sum, hd_dec_result = cython_sc_decoding(\n",
    "        lr0_post, lr1_post, freeze_idx.astype(np.float64),\n",
    "        hd_dec, 1024, 10, len(freeze_idx), frozen_val, delete_num, 0, pro_prun\n",
    "    )\n",
    "\n",
    "    # Extract the output for data_idx from hd_dec_result\n",
    "    data_out = hd_dec_result[data_idx]\n",
    "    return data_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all weights to numpy arrays\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}\n",
    "N = 1024\n",
    "n = 10\n",
    "rate = 1/8\n",
    "K = round(N*rate)\n",
    "c_1024 = np.load('c_1024.npy')\n",
    "coding_list = scipy.io.loadmat(\"1024-3db-d=2-mean.mat\")[\"count_number\"]\n",
    "coding_index = np.argsort(coding_list[:,1])\n",
    "info_idx = coding_index[:K]\n",
    "freeze_idx = coding_index[K:]\n",
    "\n",
    "# sort the final index\n",
    "info_ni = np.sort(info_idx)\n",
    "freeze_ni = np.sort(freeze_idx)\n",
    "gn = get_gn(1024, \"cuda:0\")\n",
    "udp_packet, codeword_idx, bit_array_len, codeword_num = encoder_udp(weights_dict, info_ni, gn)\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_array = packet_aggregation(udp_packet, codeword_num, info_ni, freeze_ni, codeword_idx, bit_array_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_cuda(udp_packet, codeword_num, data_idx, freeze_idx, codeword_idx, bit_array_len, chunk_size=10000):\n",
    "    udp_idx = []\n",
    "    packet_del = []\n",
    "    for tmp_packet in udp_packet:\n",
    "        udp_idx.append(struct.unpack(\"I\", tmp_packet[:4])[0])\n",
    "        packet_del.append(udp_packet_to_numpy_array(tmp_packet[4:], codeword_num))\n",
    "    packet_del = np.array(packet_del)\n",
    "    packet_data = np.ones((1024, codeword_num)) * 0.5\n",
    "    packet_data[udp_idx] = packet_del\n",
    "    restore_codeword = packet_data.T\n",
    "\n",
    "    codeword_torch = torch.tensor(restore_codeword, dtype=torch.float32).to(\"cuda:0\")\n",
    "    bit_array = 1 - 2 * codeword_torch.flatten()\n",
    "    lr0 = torch.exp(-(bit_array - 1)**2)\n",
    "    lr1 = torch.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "\n",
    "    lr0_post = lr0_post.cpu().detach().numpy().astype(np.float64)\n",
    "    lr1_post = lr1_post.cpu().detach().numpy().astype(np.float64)\n",
    "    delete_num = 1024 - restore_codeword.shape[1]\n",
    "    hd_dec = np.zeros_like(bit_array.cpu().detach().numpy()).astype(np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    del codeword_torch, bit_array, lr0, lr1\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    hd_dec_result = decoder_cuda(lr0_post, lr1_post, freeze_idx.astype(np.float64), hd_dec, 1024, 10, len(freeze_idx), frozen_val, delete_num, 0, restore_codeword.shape[0], chunk_size)\n",
    "\n",
    "    hd_dec_result = hd_dec_result.reshape(restore_codeword.shape).astype(np.int8)\n",
    "    hd_dec_result = hd_dec_result[:,data_idx]\n",
    "\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(hd_dec_result[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        bit_array = np.packbits(tmp_array)\n",
    "        restore_array.append(np.frombuffer(bit_array.tobytes(), dtype=np.float32))\n",
    "    return restore_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_array = decoding_cuda(udp_packet, codeword_num, info_ni, freeze_ni, codeword_idx, bit_array_len,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_rate = 0.8\n",
    "received_idx = np.random.choice(1024, int(1024*(1-lost_rate)), replace=False)\n",
    "received_packet = [udp_packet[idx] for idx in received_idx]\n",
    "# restore_array = packet_aggregation(received_packet, codeword_num, info_ni, freeze_ni, codeword_idx, bit_array_len)\n",
    "cuda_array = decoding_cuda(received_packet, codeword_num, info_ni, freeze_ni, codeword_idx, bit_array_len, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors\n",
    "tensor1 = torch.arange(0, 1024).repeat(10, 1)  # Shape: (10, 1024), rows are 0~1023\n",
    "tensor2 = torch.arange(1024, 2048).repeat(10, 1)  # Shape: (10, 1024), rows are 1024~2047\n",
    "\n",
    "# Stack along a new dimension (dim=2) and interleave\n",
    "interleaved = torch.stack((tensor1, tensor2), dim=2).reshape(10, -1)\n",
    "\n",
    "# Output the result\n",
    "print(\"Interleaved Tensor:\\n\", interleaved)\n",
    "print(\"Shape:\", interleaved.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 22],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin'],\n",
    "    'Salary': [50000, 60000, 75000, 65000, 55000]\n",
    "})\n",
    "\n",
    "# Demonstrate some basic pandas operations\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nGrouped by City:\")\n",
    "print(df.groupby('City')['Salary'].mean())\n",
    "\n",
    "# Sort by Age\n",
    "print(\"\\nSorted by Age:\")\n",
    "print(df.sort_values('Age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
