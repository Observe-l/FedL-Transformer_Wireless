{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lwh/anaconda3/envs/wireless/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import struct\n",
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torchvision.transforms import ToTensor\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "from numba import njit, jit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cython_decoder import cython_sc_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind((host, port))\n",
    "num_nodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 20\n",
    "server_socket.listen((num_nodes+1)*2)\n",
    "node_s = []\n",
    "node_r = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        client_socket, addr = server_socket.accept()\n",
    "        server_socket.settimeout(10)\n",
    "        data = client_socket.recv(1024).decode()\n",
    "        if data == \"Server-R\":\n",
    "            server_s = client_socket\n",
    "        elif data == \"Server-S\":\n",
    "            server_r = client_socket\n",
    "        elif data == \"Node-R\":\n",
    "            node_s.append(client_socket)\n",
    "        elif data == \"Node-S\":\n",
    "            node_r.append(client_socket)\n",
    "        client_socket.sendall(struct.pack('I',len(b\"start\"))+b\"start\")\n",
    "except socket.timeout:\n",
    "    print('Timeout')\n",
    "    server_socket.settimeout(None)\n",
    "\n",
    "for tmp_socket in node_r:\n",
    "    tmp_socket.recv(1024)\n",
    "server_r.recv(65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp_socket in node_r:\n",
    "    tmp_socket.close()\n",
    "for tmp_socket in node_s:\n",
    "    tmp_socket.close()\n",
    "server_s.close()\n",
    "server_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"cifar10\",\n",
    "    partitioners={\n",
    "        \"train\": DirichletPartitioner(\n",
    "            num_partitions=num_nodes,\n",
    "            partition_by=\"label\",\n",
    "            alpha=0.1,\n",
    "            seed=42,\n",
    "            min_partition_size=0,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "  transforms = transform_train\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "def test_transforms(batch):\n",
    "    transforms = transform_test\n",
    "    batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "train_loader=[]\n",
    "test_loader=[]\n",
    "for i in range(num_nodes):\n",
    "    partition_train_test = fds.load_partition(i, \"train\").train_test_split(0.1)\n",
    "    partition_train = partition_train_test[\"train\"].with_transform(train_transforms)\n",
    "    partition_test = partition_train_test[\"test\"].with_transform(test_transforms)\n",
    "    # centralized_dataset = fds.load_split(\"test\").with_transform(test_transforms)\n",
    "    train_loader.append(DataLoader(partition_train, batch_size=512, shuffle=True, num_workers=16))\n",
    "    test_loader.append(DataLoader(partition_test, batch_size=100, shuffle=False, num_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_small import ViT\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = []\n",
    "optimizer = []\n",
    "scheduler = []\n",
    "criterion = []\n",
    "scaler = []\n",
    "for i in range(num_nodes):\n",
    "    net.append(ViT(\n",
    "        image_size = 32,\n",
    "        patch_size = 4,\n",
    "        num_classes = 10,\n",
    "        dim = 32,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 32,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    ).to(device))\n",
    "\n",
    "\n",
    "    optimizer.append(optim.Adam(net[i].parameters(), lr=0.001))\n",
    "    scheduler.append(torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[i], 20))\n",
    "    criterion.append(nn.CrossEntropyLoss())\n",
    "    scaler.append(torch.cuda.amp.GradScaler(enabled=True))\n",
    "\n",
    "server_net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 32,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 32,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                criterion: nn.Module, \n",
    "                device: torch.device, \n",
    "                scaler: torch.cuda.amp.GradScaler, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                nodes: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Nodes: {nodes}, Epoch: {epoch},Train Loss: {total_loss / total_samples:.4f}, Train Accuracy: {total_correct / total_samples:.4f}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   test_loader: DataLoader, \n",
    "                   criterion: nn.Module, \n",
    "                   device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Validation Loss: {total_loss / total_samples:.4f}, Validation Accuracy: {total_correct / total_samples:.4f}\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 0, Epoch: 0,Train Loss: 0.0027, Train Accuracy: 0.5330\n",
      "Validation Loss: 0.0120, Validation Accuracy: 0.5836\n",
      "\t\n",
      "Nodes: 0, Epoch: 1,Train Loss: 0.0022, Train Accuracy: 0.5812\n",
      "Validation Loss: 0.0111, Validation Accuracy: 0.5998\n",
      "\t\n",
      "Nodes: 0, Epoch: 2,Train Loss: 0.0021, Train Accuracy: 0.5907\n",
      "Validation Loss: 0.0106, Validation Accuracy: 0.6041\n",
      "\t\n",
      "Nodes: 0, Epoch: 3,Train Loss: 0.0020, Train Accuracy: 0.6033\n",
      "Validation Loss: 0.0103, Validation Accuracy: 0.6030\n",
      "\t\n",
      "Nodes: 0, Epoch: 4,Train Loss: 0.0019, Train Accuracy: 0.6191\n",
      "Validation Loss: 0.0094, Validation Accuracy: 0.6278\n",
      "\t\n",
      "Nodes: 0, Epoch: 5,Train Loss: 0.0018, Train Accuracy: 0.6457\n",
      "Validation Loss: 0.0095, Validation Accuracy: 0.6440\n",
      "\t\n",
      "Nodes: 0, Epoch: 6,Train Loss: 0.0018, Train Accuracy: 0.6622\n",
      "Validation Loss: 0.0087, Validation Accuracy: 0.6742\n",
      "\t\n",
      "Nodes: 0, Epoch: 7,Train Loss: 0.0017, Train Accuracy: 0.6709\n",
      "Validation Loss: 0.0086, Validation Accuracy: 0.6807\n",
      "\t\n",
      "Nodes: 0, Epoch: 8,Train Loss: 0.0017, Train Accuracy: 0.6867\n",
      "Validation Loss: 0.0084, Validation Accuracy: 0.6936\n",
      "\t\n",
      "Nodes: 0, Epoch: 9,Train Loss: 0.0016, Train Accuracy: 0.6969\n",
      "Validation Loss: 0.0083, Validation Accuracy: 0.6936\n",
      "\t\n",
      "Nodes: 0, Epoch: 10,Train Loss: 0.0016, Train Accuracy: 0.6978\n",
      "Validation Loss: 0.0078, Validation Accuracy: 0.7184\n",
      "\t\n",
      "Nodes: 0, Epoch: 11,Train Loss: 0.0016, Train Accuracy: 0.7032\n",
      "Validation Loss: 0.0077, Validation Accuracy: 0.7174\n",
      "\t\n",
      "Nodes: 0, Epoch: 12,Train Loss: 0.0016, Train Accuracy: 0.7057\n",
      "Validation Loss: 0.0076, Validation Accuracy: 0.7109\n",
      "\t\n",
      "Nodes: 0, Epoch: 13,Train Loss: 0.0015, Train Accuracy: 0.7142\n",
      "Validation Loss: 0.0076, Validation Accuracy: 0.7195\n",
      "\t\n",
      "Nodes: 0, Epoch: 14,Train Loss: 0.0015, Train Accuracy: 0.7103\n",
      "Validation Loss: 0.0074, Validation Accuracy: 0.7249\n",
      "\t\n",
      "Nodes: 0, Epoch: 15,Train Loss: 0.0015, Train Accuracy: 0.7145\n",
      "Validation Loss: 0.0074, Validation Accuracy: 0.7174\n",
      "\t\n",
      "Nodes: 0, Epoch: 16,Train Loss: 0.0015, Train Accuracy: 0.7103\n",
      "Validation Loss: 0.0074, Validation Accuracy: 0.7249\n",
      "\t\n",
      "Nodes: 0, Epoch: 17,Train Loss: 0.0015, Train Accuracy: 0.7199\n",
      "Validation Loss: 0.0074, Validation Accuracy: 0.7228\n",
      "\t\n",
      "Nodes: 0, Epoch: 18,Train Loss: 0.0015, Train Accuracy: 0.7121\n",
      "Validation Loss: 0.0073, Validation Accuracy: 0.7282\n",
      "\t\n",
      "Nodes: 0, Epoch: 19,Train Loss: 0.0015, Train Accuracy: 0.7162\n",
      "Validation Loss: 0.0074, Validation Accuracy: 0.7271\n",
      "\t\n",
      "Nodes: 1, Epoch: 0,Train Loss: 0.0035, Train Accuracy: 0.3229\n",
      "Validation Loss: 0.0172, Validation Accuracy: 0.3486\n",
      "\t\n",
      "Nodes: 1, Epoch: 1,Train Loss: 0.0032, Train Accuracy: 0.4012\n",
      "Validation Loss: 0.0154, Validation Accuracy: 0.4405\n",
      "\t\n",
      "Nodes: 1, Epoch: 2,Train Loss: 0.0029, Train Accuracy: 0.4504\n",
      "Validation Loss: 0.0144, Validation Accuracy: 0.4996\n",
      "\t\n",
      "Nodes: 1, Epoch: 3,Train Loss: 0.0028, Train Accuracy: 0.4803\n",
      "Validation Loss: 0.0136, Validation Accuracy: 0.5317\n",
      "\t\n",
      "Nodes: 1, Epoch: 4,Train Loss: 0.0027, Train Accuracy: 0.5023\n",
      "Validation Loss: 0.0131, Validation Accuracy: 0.5450\n",
      "\t\n",
      "Nodes: 1, Epoch: 5,Train Loss: 0.0027, Train Accuracy: 0.5136\n",
      "Validation Loss: 0.0129, Validation Accuracy: 0.5485\n",
      "\t\n",
      "Nodes: 1, Epoch: 6,Train Loss: 0.0026, Train Accuracy: 0.5293\n",
      "Validation Loss: 0.0125, Validation Accuracy: 0.5696\n",
      "\t\n",
      "Nodes: 1, Epoch: 7,Train Loss: 0.0025, Train Accuracy: 0.5420\n",
      "Validation Loss: 0.0121, Validation Accuracy: 0.5818\n",
      "\t\n",
      "Nodes: 1, Epoch: 8,Train Loss: 0.0025, Train Accuracy: 0.5444\n",
      "Validation Loss: 0.0119, Validation Accuracy: 0.5826\n",
      "\t\n",
      "Nodes: 1, Epoch: 9,Train Loss: 0.0024, Train Accuracy: 0.5512\n",
      "Validation Loss: 0.0117, Validation Accuracy: 0.5904\n",
      "\t\n",
      "Nodes: 1, Epoch: 10,Train Loss: 0.0024, Train Accuracy: 0.5623\n",
      "Validation Loss: 0.0116, Validation Accuracy: 0.5970\n",
      "\t\n",
      "Nodes: 1, Epoch: 11,Train Loss: 0.0024, Train Accuracy: 0.5677\n",
      "Validation Loss: 0.0115, Validation Accuracy: 0.5923\n",
      "\t\n",
      "Nodes: 1, Epoch: 12,Train Loss: 0.0023, Train Accuracy: 0.5766\n",
      "Validation Loss: 0.0112, Validation Accuracy: 0.6107\n",
      "\t\n",
      "Nodes: 1, Epoch: 13,Train Loss: 0.0023, Train Accuracy: 0.5763\n",
      "Validation Loss: 0.0113, Validation Accuracy: 0.5966\n",
      "\t\n",
      "Nodes: 1, Epoch: 14,Train Loss: 0.0023, Train Accuracy: 0.5798\n",
      "Validation Loss: 0.0110, Validation Accuracy: 0.6135\n",
      "\t\n",
      "Nodes: 1, Epoch: 15,Train Loss: 0.0023, Train Accuracy: 0.5821\n",
      "Validation Loss: 0.0110, Validation Accuracy: 0.6123\n",
      "\t\n",
      "Nodes: 1, Epoch: 16,Train Loss: 0.0023, Train Accuracy: 0.5871\n",
      "Validation Loss: 0.0110, Validation Accuracy: 0.6107\n",
      "\t\n",
      "Nodes: 1, Epoch: 17,Train Loss: 0.0023, Train Accuracy: 0.5850\n",
      "Validation Loss: 0.0109, Validation Accuracy: 0.6154\n",
      "\t\n",
      "Nodes: 1, Epoch: 18,Train Loss: 0.0023, Train Accuracy: 0.5887\n",
      "Validation Loss: 0.0109, Validation Accuracy: 0.6107\n",
      "\t\n",
      "Nodes: 1, Epoch: 19,Train Loss: 0.0023, Train Accuracy: 0.5869\n",
      "Validation Loss: 0.0109, Validation Accuracy: 0.6115\n",
      "\t\n",
      "Nodes: 2, Epoch: 0,Train Loss: 0.0033, Train Accuracy: 0.4716\n",
      "Validation Loss: 0.0132, Validation Accuracy: 0.5735\n",
      "\t\n",
      "Nodes: 2, Epoch: 1,Train Loss: 0.0024, Train Accuracy: 0.6104\n",
      "Validation Loss: 0.0123, Validation Accuracy: 0.5906\n",
      "\t\n",
      "Nodes: 2, Epoch: 2,Train Loss: 0.0023, Train Accuracy: 0.6142\n",
      "Validation Loss: 0.0119, Validation Accuracy: 0.5820\n",
      "\t\n",
      "Nodes: 2, Epoch: 3,Train Loss: 0.0022, Train Accuracy: 0.6175\n",
      "Validation Loss: 0.0118, Validation Accuracy: 0.5835\n",
      "\t\n",
      "Nodes: 2, Epoch: 4,Train Loss: 0.0022, Train Accuracy: 0.6189\n",
      "Validation Loss: 0.0110, Validation Accuracy: 0.6049\n",
      "\t\n",
      "Nodes: 2, Epoch: 5,Train Loss: 0.0021, Train Accuracy: 0.6311\n",
      "Validation Loss: 0.0108, Validation Accuracy: 0.6291\n",
      "\t\n",
      "Nodes: 2, Epoch: 6,Train Loss: 0.0020, Train Accuracy: 0.6694\n",
      "Validation Loss: 0.0095, Validation Accuracy: 0.6847\n",
      "\t\n",
      "Nodes: 2, Epoch: 7,Train Loss: 0.0018, Train Accuracy: 0.7112\n",
      "Validation Loss: 0.0090, Validation Accuracy: 0.7004\n",
      "\t\n",
      "Nodes: 2, Epoch: 8,Train Loss: 0.0017, Train Accuracy: 0.7214\n",
      "Validation Loss: 0.0092, Validation Accuracy: 0.6947\n",
      "\t\n",
      "Nodes: 2, Epoch: 9,Train Loss: 0.0017, Train Accuracy: 0.7334\n",
      "Validation Loss: 0.0089, Validation Accuracy: 0.7076\n",
      "\t\n",
      "Nodes: 2, Epoch: 10,Train Loss: 0.0017, Train Accuracy: 0.7349\n",
      "Validation Loss: 0.0084, Validation Accuracy: 0.7161\n",
      "\t\n",
      "Nodes: 2, Epoch: 11,Train Loss: 0.0016, Train Accuracy: 0.7374\n",
      "Validation Loss: 0.0082, Validation Accuracy: 0.7347\n",
      "\t\n",
      "Nodes: 2, Epoch: 12,Train Loss: 0.0016, Train Accuracy: 0.7441\n",
      "Validation Loss: 0.0081, Validation Accuracy: 0.7347\n",
      "\t\n",
      "Nodes: 2, Epoch: 13,Train Loss: 0.0016, Train Accuracy: 0.7485\n",
      "Validation Loss: 0.0082, Validation Accuracy: 0.7304\n",
      "\t\n",
      "Nodes: 2, Epoch: 14,Train Loss: 0.0016, Train Accuracy: 0.7463\n",
      "Validation Loss: 0.0080, Validation Accuracy: 0.7404\n",
      "\t\n",
      "Nodes: 2, Epoch: 15,Train Loss: 0.0016, Train Accuracy: 0.7501\n",
      "Validation Loss: 0.0079, Validation Accuracy: 0.7432\n",
      "\t\n",
      "Nodes: 2, Epoch: 16,Train Loss: 0.0016, Train Accuracy: 0.7482\n",
      "Validation Loss: 0.0080, Validation Accuracy: 0.7418\n",
      "\t\n",
      "Nodes: 2, Epoch: 17,Train Loss: 0.0016, Train Accuracy: 0.7515\n",
      "Validation Loss: 0.0079, Validation Accuracy: 0.7418\n",
      "\t\n",
      "Nodes: 2, Epoch: 18,Train Loss: 0.0015, Train Accuracy: 0.7509\n",
      "Validation Loss: 0.0079, Validation Accuracy: 0.7461\n",
      "\t\n",
      "Nodes: 2, Epoch: 19,Train Loss: 0.0015, Train Accuracy: 0.7548\n",
      "Validation Loss: 0.0079, Validation Accuracy: 0.7461\n",
      "\t\n",
      "Nodes: 3, Epoch: 0,Train Loss: 0.0046, Train Accuracy: 0.2299\n",
      "Validation Loss: 0.0198, Validation Accuracy: 0.3849\n",
      "\t\n",
      "Nodes: 3, Epoch: 1,Train Loss: 0.0033, Train Accuracy: 0.4012\n",
      "Validation Loss: 0.0187, Validation Accuracy: 0.3717\n",
      "\t\n",
      "Nodes: 3, Epoch: 2,Train Loss: 0.0031, Train Accuracy: 0.4008\n",
      "Validation Loss: 0.0180, Validation Accuracy: 0.3947\n",
      "\t\n",
      "Nodes: 3, Epoch: 3,Train Loss: 0.0031, Train Accuracy: 0.4092\n",
      "Validation Loss: 0.0172, Validation Accuracy: 0.3947\n",
      "\t\n",
      "Nodes: 3, Epoch: 4,Train Loss: 0.0030, Train Accuracy: 0.4191\n",
      "Validation Loss: 0.0170, Validation Accuracy: 0.4671\n",
      "\t\n",
      "Nodes: 3, Epoch: 5,Train Loss: 0.0029, Train Accuracy: 0.4290\n",
      "Validation Loss: 0.0163, Validation Accuracy: 0.4243\n",
      "\t\n",
      "Nodes: 3, Epoch: 6,Train Loss: 0.0029, Train Accuracy: 0.4389\n",
      "Validation Loss: 0.0159, Validation Accuracy: 0.4671\n",
      "\t\n",
      "Nodes: 3, Epoch: 7,Train Loss: 0.0029, Train Accuracy: 0.4430\n",
      "Validation Loss: 0.0157, Validation Accuracy: 0.4704\n",
      "\t\n",
      "Nodes: 3, Epoch: 8,Train Loss: 0.0028, Train Accuracy: 0.4481\n",
      "Validation Loss: 0.0153, Validation Accuracy: 0.4868\n",
      "\t\n",
      "Nodes: 3, Epoch: 9,Train Loss: 0.0028, Train Accuracy: 0.4697\n",
      "Validation Loss: 0.0152, Validation Accuracy: 0.5000\n",
      "\t\n",
      "Nodes: 3, Epoch: 10,Train Loss: 0.0028, Train Accuracy: 0.4668\n",
      "Validation Loss: 0.0147, Validation Accuracy: 0.5066\n",
      "\t\n",
      "Nodes: 3, Epoch: 11,Train Loss: 0.0028, Train Accuracy: 0.4694\n",
      "Validation Loss: 0.0147, Validation Accuracy: 0.5066\n",
      "\t\n",
      "Nodes: 3, Epoch: 12,Train Loss: 0.0027, Train Accuracy: 0.4727\n",
      "Validation Loss: 0.0147, Validation Accuracy: 0.4901\n",
      "\t\n",
      "Nodes: 3, Epoch: 13,Train Loss: 0.0027, Train Accuracy: 0.4760\n",
      "Validation Loss: 0.0146, Validation Accuracy: 0.4836\n",
      "\t\n",
      "Nodes: 3, Epoch: 14,Train Loss: 0.0027, Train Accuracy: 0.4873\n",
      "Validation Loss: 0.0145, Validation Accuracy: 0.5164\n",
      "\t\n",
      "Nodes: 3, Epoch: 15,Train Loss: 0.0027, Train Accuracy: 0.4826\n",
      "Validation Loss: 0.0145, Validation Accuracy: 0.5033\n",
      "\t\n",
      "Nodes: 3, Epoch: 16,Train Loss: 0.0027, Train Accuracy: 0.4866\n",
      "Validation Loss: 0.0146, Validation Accuracy: 0.4967\n",
      "\t\n",
      "Nodes: 3, Epoch: 17,Train Loss: 0.0027, Train Accuracy: 0.4917\n",
      "Validation Loss: 0.0145, Validation Accuracy: 0.5099\n",
      "\t\n",
      "Nodes: 3, Epoch: 18,Train Loss: 0.0027, Train Accuracy: 0.4983\n",
      "Validation Loss: 0.0145, Validation Accuracy: 0.5033\n",
      "\t\n",
      "Nodes: 3, Epoch: 19,Train Loss: 0.0027, Train Accuracy: 0.4892\n",
      "Validation Loss: 0.0145, Validation Accuracy: 0.5066\n",
      "\t\n",
      "Nodes: 4, Epoch: 0,Train Loss: 0.0024, Train Accuracy: 0.6704\n",
      "Validation Loss: 0.0065, Validation Accuracy: 0.8755\n",
      "\t\n",
      "Nodes: 4, Epoch: 1,Train Loss: 0.0011, Train Accuracy: 0.8854\n",
      "Validation Loss: 0.0059, Validation Accuracy: 0.8755\n",
      "\t\n",
      "Nodes: 4, Epoch: 2,Train Loss: 0.0009, Train Accuracy: 0.8878\n",
      "Validation Loss: 0.0052, Validation Accuracy: 0.8696\n",
      "\t\n",
      "Nodes: 4, Epoch: 3,Train Loss: 0.0009, Train Accuracy: 0.8852\n",
      "Validation Loss: 0.0050, Validation Accuracy: 0.8677\n",
      "\t\n",
      "Nodes: 4, Epoch: 4,Train Loss: 0.0008, Train Accuracy: 0.8880\n",
      "Validation Loss: 0.0049, Validation Accuracy: 0.8696\n",
      "\t\n",
      "Nodes: 4, Epoch: 5,Train Loss: 0.0008, Train Accuracy: 0.8897\n",
      "Validation Loss: 0.0047, Validation Accuracy: 0.8735\n",
      "\t\n",
      "Nodes: 4, Epoch: 6,Train Loss: 0.0007, Train Accuracy: 0.8865\n",
      "Validation Loss: 0.0045, Validation Accuracy: 0.8833\n",
      "\t\n",
      "Nodes: 4, Epoch: 7,Train Loss: 0.0008, Train Accuracy: 0.8938\n",
      "Validation Loss: 0.0046, Validation Accuracy: 0.8735\n",
      "\t\n",
      "Nodes: 4, Epoch: 8,Train Loss: 0.0007, Train Accuracy: 0.8895\n",
      "Validation Loss: 0.0043, Validation Accuracy: 0.8852\n",
      "\t\n",
      "Nodes: 4, Epoch: 9,Train Loss: 0.0007, Train Accuracy: 0.8906\n",
      "Validation Loss: 0.0043, Validation Accuracy: 0.8813\n",
      "\t\n",
      "Nodes: 4, Epoch: 10,Train Loss: 0.0007, Train Accuracy: 0.8971\n",
      "Validation Loss: 0.0043, Validation Accuracy: 0.8813\n",
      "\t\n",
      "Nodes: 4, Epoch: 11,Train Loss: 0.0007, Train Accuracy: 0.8981\n",
      "Validation Loss: 0.0043, Validation Accuracy: 0.8833\n",
      "\t\n",
      "Nodes: 4, Epoch: 12,Train Loss: 0.0007, Train Accuracy: 0.9029\n",
      "Validation Loss: 0.0042, Validation Accuracy: 0.8872\n",
      "\t\n",
      "Nodes: 4, Epoch: 13,Train Loss: 0.0007, Train Accuracy: 0.9029\n",
      "Validation Loss: 0.0042, Validation Accuracy: 0.8911\n",
      "\t\n",
      "Nodes: 4, Epoch: 14,Train Loss: 0.0006, Train Accuracy: 0.9044\n",
      "Validation Loss: 0.0041, Validation Accuracy: 0.8852\n",
      "\t\n",
      "Nodes: 4, Epoch: 15,Train Loss: 0.0007, Train Accuracy: 0.9020\n",
      "Validation Loss: 0.0041, Validation Accuracy: 0.8930\n",
      "\t\n",
      "Nodes: 4, Epoch: 16,Train Loss: 0.0007, Train Accuracy: 0.9059\n",
      "Validation Loss: 0.0042, Validation Accuracy: 0.8930\n",
      "\t\n",
      "Nodes: 4, Epoch: 17,Train Loss: 0.0006, Train Accuracy: 0.9107\n",
      "Validation Loss: 0.0040, Validation Accuracy: 0.8911\n",
      "\t\n",
      "Nodes: 4, Epoch: 18,Train Loss: 0.0007, Train Accuracy: 0.9074\n",
      "Validation Loss: 0.0040, Validation Accuracy: 0.8949\n",
      "\t\n",
      "Nodes: 4, Epoch: 19,Train Loss: 0.0006, Train Accuracy: 0.9087\n",
      "Validation Loss: 0.0040, Validation Accuracy: 0.8949\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "for cli in range(num_nodes):\n",
    "    for i in range(20):\n",
    "        train_model(net[cli], train_loader[cli], criterion[cli], device, scaler[cli], optimizer[cli], i, cli)\n",
    "        evaluate_model(net[cli], test_loader[cli], criterion[cli], device)\n",
    "        scheduler[cli].step()\n",
    "    scheduler[cli] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[cli], 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['pos_embedding', 'cls_token', 'to_patch_embedding.to_patch_tokens.1.weight', 'to_patch_embedding.to_patch_tokens.1.bias', 'to_patch_embedding.to_patch_tokens.2.weight', 'to_patch_embedding.to_patch_tokens.2.bias', 'transformer.layers.0.0.norm.weight', 'transformer.layers.0.0.norm.bias', 'transformer.layers.0.0.fn.temperature', 'transformer.layers.0.0.fn.to_qkv.weight', 'transformer.layers.0.0.fn.to_out.0.weight', 'transformer.layers.0.0.fn.to_out.0.bias', 'transformer.layers.0.1.norm.weight', 'transformer.layers.0.1.norm.bias', 'transformer.layers.0.1.fn.net.0.weight', 'transformer.layers.0.1.fn.net.0.bias', 'transformer.layers.0.1.fn.net.3.weight', 'transformer.layers.0.1.fn.net.3.bias', 'transformer.layers.1.0.norm.weight', 'transformer.layers.1.0.norm.bias', 'transformer.layers.1.0.fn.temperature', 'transformer.layers.1.0.fn.to_qkv.weight', 'transformer.layers.1.0.fn.to_out.0.weight', 'transformer.layers.1.0.fn.to_out.0.bias', 'transformer.layers.1.1.norm.weight', 'transformer.layers.1.1.norm.bias', 'transformer.layers.1.1.fn.net.0.weight', 'transformer.layers.1.1.fn.net.0.bias', 'transformer.layers.1.1.fn.net.3.weight', 'transformer.layers.1.1.fn.net.3.bias', 'transformer.layers.2.0.norm.weight', 'transformer.layers.2.0.norm.bias', 'transformer.layers.2.0.fn.temperature', 'transformer.layers.2.0.fn.to_qkv.weight', 'transformer.layers.2.0.fn.to_out.0.weight', 'transformer.layers.2.0.fn.to_out.0.bias', 'transformer.layers.2.1.norm.weight', 'transformer.layers.2.1.norm.bias', 'transformer.layers.2.1.fn.net.0.weight', 'transformer.layers.2.1.fn.net.0.bias', 'transformer.layers.2.1.fn.net.3.weight', 'transformer.layers.2.1.fn.net.3.bias', 'transformer.layers.3.0.norm.weight', 'transformer.layers.3.0.norm.bias', 'transformer.layers.3.0.fn.temperature', 'transformer.layers.3.0.fn.to_qkv.weight', 'transformer.layers.3.0.fn.to_out.0.weight', 'transformer.layers.3.0.fn.to_out.0.bias', 'transformer.layers.3.1.norm.weight', 'transformer.layers.3.1.norm.bias', 'transformer.layers.3.1.fn.net.0.weight', 'transformer.layers.3.1.fn.net.0.bias', 'transformer.layers.3.1.fn.net.3.weight', 'transformer.layers.3.1.fn.net.3.bias', 'transformer.layers.4.0.norm.weight', 'transformer.layers.4.0.norm.bias', 'transformer.layers.4.0.fn.temperature', 'transformer.layers.4.0.fn.to_qkv.weight', 'transformer.layers.4.0.fn.to_out.0.weight', 'transformer.layers.4.0.fn.to_out.0.bias', 'transformer.layers.4.1.norm.weight', 'transformer.layers.4.1.norm.bias', 'transformer.layers.4.1.fn.net.0.weight', 'transformer.layers.4.1.fn.net.0.bias', 'transformer.layers.4.1.fn.net.3.weight', 'transformer.layers.4.1.fn.net.3.bias', 'transformer.layers.5.0.norm.weight', 'transformer.layers.5.0.norm.bias', 'transformer.layers.5.0.fn.temperature', 'transformer.layers.5.0.fn.to_qkv.weight', 'transformer.layers.5.0.fn.to_out.0.weight', 'transformer.layers.5.0.fn.to_out.0.bias', 'transformer.layers.5.1.norm.weight', 'transformer.layers.5.1.norm.bias', 'transformer.layers.5.1.fn.net.0.weight', 'transformer.layers.5.1.fn.net.0.bias', 'transformer.layers.5.1.fn.net.3.weight', 'transformer.layers.5.1.fn.net.3.bias', 'mlp_head.0.weight', 'mlp_head.0.bias', 'mlp_head.1.weight', 'mlp_head.1.bias'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_bit(array: np.ndarray) -> np.ndarray:\n",
    "    tmp_byte=np.frombuffer(array.tobytes(),dtype=np.uint8)\n",
    "    bit_stream = ''.join(format(byte, '08b') for byte in tmp_byte)\n",
    "    bit_array = np.array([int(bit) for bit in bit_stream], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def bit_to_numpy(bit_array: np.ndarray) -> np.ndarray:\n",
    "    bit_stream = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    byte_array_back = np.array([int(bit_stream[i:i+8], 2) for i in range(0, len(bit_stream), 8)], dtype=np.uint8)\n",
    "    int_array_back = np.frombuffer(byte_array_back.tobytes(), dtype=np.float32)\n",
    "    return int_array_back\n",
    "\n",
    "'''\n",
    "Encoding and decoding function\n",
    "'''\n",
    "@jit(nopython=True)\n",
    "def encode(u: np.ndarray) -> np.ndarray:\n",
    "    N = u.shape[0]  # Get the length of u\n",
    "    n = int(np.log2(N))  # Calculate the log base 2 of N\n",
    "\n",
    "    if n == 1:\n",
    "        x = np.array([(u[0] + u[1]) % 2, u[1]],dtype=np.int8)\n",
    "        return x\n",
    "    else:\n",
    "        x1 = encode(np.mod(u[:N//2] + u[N//2:], 2))\n",
    "        x2 = encode(u[N//2:])\n",
    "        x = np.concatenate((x1, x2))\n",
    "        return x\n",
    "\n",
    "@jit(nopython=True)\n",
    "def rvsl(y: np.ndarray) -> np.ndarray:\n",
    "    N = y.shape[0]\n",
    "    if N == 2:\n",
    "        return y\n",
    "    else:\n",
    "        return np.concatenate((rvsl(y[0:N:2]), rvsl(y[1:N:2])))\n",
    "\n",
    "def data_process(array):\n",
    "    bit_array = numpy_to_bit(array)\n",
    "    array_len = len(bit_array)\n",
    "    current_array = []\n",
    "    for i in range(0, array_len, 512):\n",
    "        sub_array = bit_array[i:i+512]\n",
    "        if len(sub_array) < 512:\n",
    "            padding = np.ones((512 - len(sub_array)), dtype=bit_array.dtype)\n",
    "            sub_array = np.concatenate((sub_array, padding))\n",
    "        current_array.append(sub_array)\n",
    "    current_idx = i // 512 + 1\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def numpy_array_to_udp_packet(bit_array):\n",
    "    # Convert the numpy array of bits to a string of bits\n",
    "    bit_string = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    # Convert the bit string to bytes\n",
    "    byte_array = bytearray(int(bit_string[i:i+8], 2) for i in range(0, len(bit_string), 8))\n",
    "    return byte_array\n",
    "\n",
    "def udp_packet_to_numpy_array(packet):\n",
    "    # Convert the byte array back to a bit string\n",
    "    bit_string = ''.join(format(byte, '08b') for byte in packet)\n",
    "    # Convert the bit string to a numpy array of floats\n",
    "    bit_array = np.array([int(bit) for bit in bit_string], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def data_generate(bit_array:np.ndarray, data_idx:np.ndarray) -> np.ndarray:\n",
    "    u=np.zeros(1024,dtype=np.int8)\n",
    "    u[data_idx] = bit_array\n",
    "    x = encode(u)\n",
    "    x = rvsl(x)\n",
    "    return x\n",
    "\n",
    "def codeword_generate(array_dict, data_idx):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "    array_process = partial(data_process)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for current_array, current_idx, array_len in executor.map(array_process, [array_dict[name] for name in array_dict]):\n",
    "            split_bit.extend(current_array)\n",
    "            codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "            bit_array_len.append(array_len)\n",
    "    executor.shutdown(wait=True)\n",
    "    del executor, array_process\n",
    "    time1 = time.time()\n",
    "    encode_partial = partial(data_generate, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        codeword = list(executor.map(encode_partial, split_bit))\n",
    "    executor.shutdown(wait=True)\n",
    "    time2 = time.time()\n",
    "    print(f'Encode time: {time2-time1}')\n",
    "\n",
    "    del executor, encode_partial\n",
    "    return np.array(codeword,dtype=np.int8), codeword_idx, bit_array_len\n",
    "\n",
    "def packet_diffusion(codeword, block_len, packet_idx):\n",
    "    codeword_len = codeword[0].shape[0]\n",
    "    udp_packet = []\n",
    "    for idx, i in enumerate(range(0, codeword_len, block_len)):\n",
    "        # tmp_packet = np.concatenate([tmp_codeword[i:i+block_len] for tmp_codeword in codeword])\n",
    "        tmp_packet = codeword[:, packet_idx[i:i+block_len]].flatten()\n",
    "        tmp_udp_packet = struct.pack(\"I\",idx) + numpy_array_to_udp_packet(tmp_packet)\n",
    "        udp_packet.append(tmp_udp_packet)\n",
    "    return udp_packet\n",
    "\n",
    "def encoder_udp(array_dict, data_idx, block_len, packet_idx):\n",
    "    codeword, codeword_idx, bit_array_len = codeword_generate(array_dict, data_idx)\n",
    "    udp_packet = packet_diffusion(codeword, block_len, packet_idx)\n",
    "    return udp_packet, codeword_idx, bit_array_len\n",
    "\n",
    "\n",
    "\n",
    "def packet_aggregation(udp_packet, packet_idx, block_len, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    sort_idx = [struct.unpack(\"I\", tmp_packet[:4])[0] for tmp_packet in udp_packet]\n",
    "    packet_data_del = np.array([udp_packet_to_numpy_array(tmp_packet[4:]) for _, tmp_packet in sorted(zip(sort_idx, udp_packet))])\n",
    "    packet_data = np.ones((int(1024/block_len), len(packet_data_del[0])))*0.5\n",
    "    for i, tmp_idx in enumerate(sorted(sort_idx)):\n",
    "        packet_data[tmp_idx] = packet_data_del[i]\n",
    "    \n",
    "    restore_codeword = []\n",
    "    inverse_packet_idx = np.argsort(packet_idx)\n",
    "    for i in range(0, packet_data.shape[1],block_len):\n",
    "        tmp_codeword = packet_data[:,i:i+block_len].flatten()\n",
    "        restore_codeword.append(tmp_codeword[inverse_packet_idx])\n",
    "\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        restore_array.append(bit_to_numpy(tmp_array))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "def decoding(bit_array, freeze_idx, data_idx):\n",
    "    # Prepare the necessary arrays and values\n",
    "    bit_array = 1-2*bit_array\n",
    "    lr0 = np.exp(-(bit_array - 1)**2)\n",
    "    lr1 = np.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "    delete_num = 1024 - len(bit_array)\n",
    "    hd_dec = np.zeros(1024, dtype=np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    pro_prun = np.zeros((1, 2 * 1024 + 1), dtype=np.float64)\n",
    "\n",
    "    # Call the optimized Cython function\n",
    "    i_scen_sum, hd_dec_result = cython_sc_decoding(\n",
    "        lr0_post, lr1_post, freeze_idx.astype(np.float64),\n",
    "        hd_dec, 1024, 10, 512, frozen_val, delete_num, 0, pro_prun\n",
    "    )\n",
    "\n",
    "    # Extract the output for data_idx from hd_dec_result\n",
    "    data_out = hd_dec_result[data_idx]\n",
    "    return data_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_array = np.random.randint(2, size=(1024,))\n",
    "bool_array = np.array(random_array, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.array([[1,0],[1,1]], dtype=bool)\n",
    "n = int(np.log2(1024))\n",
    "g_n = f\n",
    "for _ in range(n-1):\n",
    "    f = np.kron(f, g_n)\n",
    "range_n = rvsl(np.arange(1024))\n",
    "f = f[:,range_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(random_array, f) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(rvsl(encode(random_array)) == np.dot(random_array, f) % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 5.147207975387573\n"
     ]
    }
   ],
   "source": [
    "# Export all weights to numpy arrays\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}\n",
    "N = 1024\n",
    "n = 10\n",
    "rate = 0.5\n",
    "K = round(N*rate)\n",
    "c_1024 = np.load('c_1024.npy')\n",
    "coding_list = scipy.io.loadmat(\"1024-3db-d=2-mean.mat\")[\"count_number\"]\n",
    "coding_index = np.argsort(coding_list[:,1])\n",
    "info_idx = coding_index[:K]\n",
    "freeze_idx = coding_index[K:]\n",
    "\n",
    "# sort the final index\n",
    "info_ni = np.sort(info_idx)\n",
    "freeze_ni = np.sort(freeze_idx)\n",
    "\n",
    "udp_packet, codeword_idx, bit_array_len = encoder_udp(weights_dict, info_ni, 8, c_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 9.226799011230469e-05\n",
      "(26103, 1024)\n"
     ]
    }
   ],
   "source": [
    "def get_gn(length:int) -> np.ndarray:\n",
    "    n = int(np.log2(length))\n",
    "    f=np.array([[1,0],[1,1]], dtype=bool)\n",
    "    g_n = f\n",
    "    for _ in range(n-1):\n",
    "        g_n = np.kron(g_n, f)\n",
    "    g_order = rvsl(np.arange(length))\n",
    "    g_n = g_n[:, g_order]\n",
    "    return g_n\n",
    "\n",
    "def matrix_process(array, data_idx):\n",
    "    bit_array = np.frombuffer(array.tobytes(), dtype=np.uint8)\n",
    "    array_len = bit_array.shape[0] * 8\n",
    "    if bit_array.shape[0] % 64 != 0:\n",
    "        padding = 255 * np.ones((64 - bit_array.shape[0] % 64), dtype=bit_array.dtype)\n",
    "        bit_array = np.concatenate((bit_array, padding))\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array.reshape(-1, 512)\n",
    "    current_array = np.zeros((bit_array.shape[0], 1024), dtype=np.uint8)\n",
    "    current_array[:, data_idx] = bit_array\n",
    "    current_idx = bit_array.shape[0]\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def matrix_packet_diffusion(codeword):\n",
    "    if codeword.shape[0] % 8 != 0:\n",
    "        padding = np.ones((8 - codeword.shape[0] % 8, 1024), dtype=codeword.dtype)\n",
    "        udp_numpy = np.concatenate((codeword, padding)).T\n",
    "    udp_numpy = np.packbits(udp_numpy.flatten()).reshape(1024,-1)\n",
    "    udp_packet = [struct.pack(\"I\", idx) + udp_numpy[idx].tobytes() for idx in range(udp_numpy.shape[0])]\n",
    "    return udp_packet\n",
    "    \n",
    "\n",
    "def matrix_encode(array_dict, data_idx):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "\n",
    "    for name in array_dict:\n",
    "        current_array, current_idx, array_len = matrix_process(array_dict[name], data_idx)\n",
    "        split_bit.extend(current_array)\n",
    "        codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "        bit_array_len.append(array_len)\n",
    "    split_bit = torch.tensor(np.array(split_bit), dtype=torch.float32).to(\"cuda:0\")\n",
    "    \n",
    "    f=np.array([[1,0],[1,1]], dtype=np.int8)\n",
    "    n = int(np.log2(1024))\n",
    "    g_n = f\n",
    "    \n",
    "    for _ in range(n-1):\n",
    "        f = np.kron(g_n,f)\n",
    "    g_order = rvsl(np.arange(1024))\n",
    "    f = f[:,g_order]\n",
    "    f = torch.tensor(f, dtype=torch.float32).to(\"cuda:0\")\n",
    "    \n",
    "    time1 = time.time()\n",
    "    # codeword = np.dot(split_bit, f) % 2\n",
    "    tmp_codeword = torch.matmul(split_bit, f) % 2\n",
    "    time2 = time.time()\n",
    "    codeword = np.array(tmp_codeword.detach().cpu().numpy(), dtype=np.int8)\n",
    "    print(f'Encode time: {time2-time1}')\n",
    "    print(codeword.shape)\n",
    "    return codeword, codeword_idx, bit_array_len\n",
    "\n",
    "def matrix_udp(array_dict, data_idx):\n",
    "    codeword, codeword_idx, bit_array_len = matrix_encode(array_dict, data_idx)\n",
    "    udp_packet = matrix_packet_diffusion(codeword)\n",
    "    return udp_packet, codeword_idx, bit_array_len, codeword.shape[0]\n",
    "\n",
    "def matrix_udp_numpy(packet, codeword_num):\n",
    "    bit_array = np.frombuffer(packet, dtype=np.uint8)\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array[:codeword_num]\n",
    "    return bit_array\n",
    "\n",
    "def matrix_packet_aggregation(udp_packet, codeword_num, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    # sort_idx = [struct.unpack(\"I\", tmp_packet[:4])[0] for tmp_packet in udp_packet]\n",
    "    # packet_data_del = np.array([matrix_udp_numpy(tmp_packet[4:], codeword_num) for _, tmp_packet in sorted(zip(sort_idx, udp_packet))])\n",
    "    udp_idx = []\n",
    "    packet_del = []\n",
    "    for tmp_packet in udp_packet:\n",
    "        udp_idx.append(struct.unpack(\"I\", tmp_packet[:4])[0])\n",
    "        packet_del.append(matrix_udp_numpy(tmp_packet[4:], codeword_num))\n",
    "    packet_del = np.array(packet_del)\n",
    "    packet_data = np.ones((1024, codeword_num)) * 0.5\n",
    "    packet_data[udp_idx] = packet_del\n",
    "\n",
    "    restore_codeword = packet_data.T\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        bit_array = np.packbits(tmp_array)\n",
    "        restore_array.append(np.frombuffer(bit_array.tobytes(), dtype=np.float32))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "# tmp_a, tmp_b, tmp_c = matrix_encode(weights_dict, info_ni)\n",
    "tmp_a, tmp_b, tmp_c, tmp_d = matrix_udp(weights_dict, info_ni)\n",
    "tmp_restore = matrix_packet_aggregation(tmp_a, tmp_d, info_ni, freeze_ni, tmp_b, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.05029435, -0.65978384, -1.098759  , ...,  2.1404471 ,\n",
       "         2.199186  ,  0.6585182 ], dtype=float32),\n",
       " array([-1.1163503 , -1.0636296 ,  1.5093652 ,  0.4060311 , -0.44801205,\n",
       "         0.53724015,  1.2016169 ,  1.82592   ,  1.4598283 , -0.74908495,\n",
       "        -0.65649754,  0.9782436 ,  0.18316017, -0.4990122 ,  0.6454968 ,\n",
       "        -0.52529734, -0.48584813, -0.867446  ,  1.5666617 ,  0.6169514 ,\n",
       "        -0.4245442 ,  1.5554371 ,  1.9133943 , -1.0920482 , -0.17747894,\n",
       "        -0.30192098,  1.3211392 ,  0.41508555,  1.455115  ,  0.34988657,\n",
       "        -0.0812578 ,  0.18749887], dtype=float32),\n",
       " array([0.99704266, 1.007392  , 0.988594  , 1.0160578 , 0.99178565,\n",
       "        1.0287938 , 0.99701715, 1.0152564 , 0.9966321 , 1.0044732 ,\n",
       "        1.0104657 , 1.005381  , 0.9918451 , 0.9976841 , 1.0081103 ,\n",
       "        0.9714542 , 0.9979299 , 1.0080383 , 1.0065551 , 1.0107318 ,\n",
       "        0.9979431 , 1.0044748 , 1.0134437 , 1.0181808 , 0.99924445,\n",
       "        1.0226794 , 1.0551108 , 1.0017291 , 0.9802047 , 1.013473  ,\n",
       "        0.99904716, 1.0169852 , 1.001998  , 1.0028152 , 0.9930673 ,\n",
       "        1.0001869 , 1.010384  , 0.9869591 , 1.0064994 , 1.0325574 ,\n",
       "        1.0411592 , 1.040828  , 1.0095364 , 1.0283991 , 1.0051137 ,\n",
       "        1.007721  , 1.0193539 , 0.9838165 , 0.97508234, 0.9917511 ,\n",
       "        0.99956065, 1.0143942 , 0.99456066, 1.0467666 , 0.9771799 ,\n",
       "        0.99361485, 1.0188938 , 1.0113279 , 0.9758633 , 1.0216613 ,\n",
       "        1.0033863 , 1.0516697 , 1.003671  , 1.0050669 , 1.0079542 ,\n",
       "        1.0302103 , 0.99645096, 1.0360405 , 0.98686916, 1.0273815 ,\n",
       "        1.0376986 , 1.0067354 , 1.0126123 , 1.0275658 , 1.049659  ,\n",
       "        0.9934688 , 0.9955296 , 0.994754  , 0.96735126, 1.0184996 ,\n",
       "        1.0190064 , 0.99252367, 1.0430572 , 1.0058806 , 1.015865  ,\n",
       "        1.0082262 , 1.0101417 , 0.9990696 , 1.0167633 , 0.99390084,\n",
       "        1.0011328 , 1.032334  , 1.0080167 , 0.99072874, 1.0007354 ,\n",
       "        0.9910963 , 1.008869  , 1.0296758 , 1.0086274 , 0.9994374 ,\n",
       "        1.0128309 , 0.9974891 , 0.99079025, 1.0222888 , 1.0157379 ,\n",
       "        0.98685634, 1.0051215 , 1.0137498 , 0.99351007, 1.0260496 ,\n",
       "        1.0107279 , 1.0430316 , 1.0050726 , 1.019621  , 1.0013388 ,\n",
       "        0.9781975 , 1.0237578 , 0.98517376, 1.0284187 , 1.0053312 ,\n",
       "        0.9927973 , 1.0019403 , 1.0092795 , 1.0282954 , 1.0340948 ,\n",
       "        0.9955052 , 1.0012072 , 1.0045962 , 0.9784681 , 1.0147566 ,\n",
       "        0.9854093 , 1.0229993 , 0.9970817 , 1.0120355 , 1.0185281 ,\n",
       "        1.0041101 , 1.0148132 , 1.0135005 , 1.0057545 , 1.0172243 ,\n",
       "        1.0081182 , 0.9891053 , 1.0049471 , 1.0085862 , 0.9854479 ,\n",
       "        1.0019825 , 0.9949348 , 1.0115997 , 0.99914634, 0.99330693,\n",
       "        0.9961593 , 1.0105723 , 0.99866635, 0.99939317, 0.99276817,\n",
       "        1.0055522 , 0.9992524 , 1.0111841 , 1.0122232 , 1.0092298 ,\n",
       "        1.0499536 , 1.0013176 , 1.0128293 , 1.0067909 , 1.0018044 ,\n",
       "        1.005066  , 0.9982405 , 1.0374516 , 1.0065283 , 1.020526  ,\n",
       "        0.97638565, 1.0210048 , 1.0015628 , 1.0244583 , 1.0032638 ,\n",
       "        0.99981254, 1.0081627 , 1.0050207 , 1.0014212 , 0.9911331 ,\n",
       "        0.9961853 , 1.0043492 , 1.0239298 , 1.0090388 , 1.0271411 ,\n",
       "        1.0209026 , 1.0118756 , 0.98964417, 1.0289897 , 1.0000858 ,\n",
       "        1.0182744 , 1.0146313 , 1.0153695 , 1.0335134 , 1.0301768 ,\n",
       "        0.996311  , 1.0173762 , 1.009241  , 0.9957408 , 0.98780483,\n",
       "        1.0204294 , 1.0159217 , 0.9839107 , 0.99329686, 1.0223047 ,\n",
       "        1.0017947 , 1.0063577 , 1.000515  , 1.0222481 , 0.9987528 ,\n",
       "        0.99184644, 1.0106459 , 1.0038041 , 1.0097663 , 0.9987921 ,\n",
       "        1.0146145 , 0.9996608 , 1.0182716 , 1.0125018 , 0.98431677,\n",
       "        1.0303957 , 1.0155652 , 1.0006431 , 0.98854274, 1.0179236 ,\n",
       "        0.98675436, 1.0160264 , 1.0184922 , 0.9947331 , 0.99771434,\n",
       "        1.005344  , 1.0154108 , 1.022265  , 1.0629354 , 1.0104413 ,\n",
       "        0.9974274 , 1.0038846 , 1.0127723 , 1.0191215 , 1.0200973 ],\n",
       "       dtype=float32),\n",
       " array([ 1.68643054e-02,  5.97806182e-03,  1.44960340e-02, -2.04566726e-03,\n",
       "         1.10671669e-02, -2.03387672e-03,  6.33669738e-03, -2.74952478e-03,\n",
       "         5.63442183e-04,  1.66602712e-03, -1.72348432e-02, -1.54714640e-02,\n",
       "         1.74405165e-02, -4.42984747e-03, -2.93721375e-03,  1.84886418e-02,\n",
       "         2.07345956e-03, -3.29711940e-04,  1.41169243e-02,  9.29430034e-03,\n",
       "         9.62239318e-03, -2.03393470e-03,  1.71943363e-02,  6.94451015e-03,\n",
       "        -3.05216038e-03, -2.81027928e-02, -1.26390737e-02,  1.67373233e-02,\n",
       "        -9.34814382e-03,  4.71274275e-03,  1.07140113e-02,  9.38194618e-03,\n",
       "        -2.68014614e-04, -1.96984783e-03,  4.05940274e-03,  6.43697195e-03,\n",
       "         1.25272842e-02, -3.22589069e-03,  1.06816292e-02, -8.54559592e-04,\n",
       "        -2.39103497e-03, -3.01573635e-03, -8.31779651e-03,  4.68667224e-03,\n",
       "         4.68637887e-03, -4.40676551e-04,  1.11027446e-03, -8.57796613e-03,\n",
       "        -7.71147711e-03,  1.16506042e-02,  8.36907048e-03,  6.41132938e-03,\n",
       "         7.95416534e-03,  3.66516388e-03, -1.55194476e-02, -7.56809441e-03,\n",
       "        -2.57811649e-03, -1.09894313e-02,  2.05887426e-02, -1.24848606e-02,\n",
       "         6.60053082e-03,  1.44928778e-02,  2.58787386e-02, -1.37377484e-02,\n",
       "        -7.53263617e-03,  2.03112047e-02, -1.19113317e-03,  7.78522436e-03,\n",
       "         1.71575453e-02,  1.56110497e-02,  1.72987673e-02,  1.38405222e-03,\n",
       "        -3.72683234e-03,  8.79034586e-03,  1.73345823e-02, -1.31405517e-02,\n",
       "        -1.28166927e-02, -1.37576703e-02, -1.66202858e-02,  7.21242325e-03,\n",
       "         9.82544664e-03, -2.10988149e-02,  2.23331880e-02, -1.01725906e-02,\n",
       "        -1.17804147e-02,  2.83808750e-03, -5.77466912e-04,  3.95869697e-03,\n",
       "         1.17098903e-02,  1.38264652e-02, -3.99911683e-03, -1.81183312e-02,\n",
       "        -1.98454177e-03,  1.06900102e-02, -1.93580277e-02, -3.21126077e-03,\n",
       "        -5.13236877e-03,  1.73717104e-02, -1.67958774e-02, -4.20606550e-04,\n",
       "         1.40355807e-03, -6.18052483e-03, -8.68483167e-03, -6.23176713e-03,\n",
       "        -5.71244676e-03,  3.71440285e-04,  7.05063064e-03, -1.13770599e-02,\n",
       "         2.35429918e-03, -9.40520596e-03, -3.81860114e-03,  7.11009605e-03,\n",
       "        -4.27499972e-03, -6.63912622e-03,  3.63733922e-03,  1.07310163e-02,\n",
       "        -4.01893491e-03,  4.28463705e-03, -6.13753265e-03, -1.86475867e-03,\n",
       "         1.78704315e-04,  1.57434717e-02,  5.96273318e-03, -4.22541285e-03,\n",
       "        -1.31909363e-02,  1.42978346e-02, -6.88145217e-03,  5.44258067e-03,\n",
       "        -8.56253318e-03, -4.42143576e-03,  4.96542174e-03,  5.27163688e-03,\n",
       "        -9.17888526e-03,  5.59876487e-03, -7.07494514e-03, -2.09238986e-03,\n",
       "         2.70513818e-03, -1.60204375e-03,  1.27503211e-02,  1.68506261e-02,\n",
       "        -5.56416856e-03, -9.17407405e-03, -4.46198182e-03,  1.02352621e-02,\n",
       "         2.75158300e-03, -3.49347503e-03, -2.83653033e-03,  7.43566197e-04,\n",
       "         6.65955804e-03,  4.35690116e-03, -2.39268299e-02, -3.05561791e-03,\n",
       "         6.61302544e-03, -1.36492187e-02, -1.22440362e-03,  2.25306880e-02,\n",
       "        -3.10637697e-04, -2.60475697e-03,  3.99026787e-03, -2.56930869e-02,\n",
       "        -8.83196422e-04,  1.90972686e-02, -7.94074091e-04, -6.68018544e-03,\n",
       "         6.62352564e-03, -1.72857987e-03, -5.58419293e-03,  7.41625344e-03,\n",
       "         1.11181270e-02, -7.90996663e-03,  4.29140590e-03, -1.06494001e-03,\n",
       "        -6.99308235e-03, -1.94704870e-03, -1.17327236e-02,  1.69700244e-03,\n",
       "        -4.38350579e-03, -2.87289661e-03, -5.63554280e-03,  9.27177910e-03,\n",
       "        -6.26007281e-03, -1.13841053e-02, -9.08549409e-03, -1.82162113e-02,\n",
       "        -3.02392337e-03,  1.67329551e-03, -6.14434201e-03,  1.06278257e-02,\n",
       "         7.11466931e-03,  7.14230584e-03,  1.22271839e-03, -9.74009105e-04,\n",
       "        -7.72206625e-03, -1.11057367e-02, -6.63202838e-04, -1.12886764e-02,\n",
       "         4.24742140e-03,  9.69437324e-03,  9.09090741e-04,  7.51514593e-03,\n",
       "         1.90817391e-05,  1.10219186e-03,  1.16536971e-02, -2.93426355e-03,\n",
       "        -1.00157941e-02, -7.14524370e-03,  4.34605638e-03,  9.80163179e-03,\n",
       "        -1.75668336e-02, -1.50481355e-03, -1.01335524e-02,  1.28819635e-02,\n",
       "         1.65755744e-03,  1.59640005e-03,  5.95046952e-03,  5.60073089e-03,\n",
       "        -2.41787098e-02, -4.78353555e-04, -1.53353121e-02, -7.68990815e-03,\n",
       "        -1.55161903e-03,  2.62896717e-03,  4.75154351e-03,  4.66054212e-03,\n",
       "        -9.50760394e-03, -6.35668077e-03, -8.27073399e-03, -5.51471580e-03,\n",
       "        -1.03770783e-02,  5.64006995e-03, -1.90747320e-03, -2.16308492e-03,\n",
       "         1.90466759e-03, -1.06662270e-02, -1.44816525e-02, -2.23182440e-02,\n",
       "        -3.83631373e-03,  1.13641575e-03, -2.27423431e-03, -3.26439552e-03],\n",
       "       dtype=float32),\n",
       " array([ 0.0301929 , -0.01037267,  0.05971073, ..., -0.06449539,\n",
       "         0.04002342,  0.03256596], dtype=float32),\n",
       " array([-0.02847118,  0.01329529,  0.03493556, -0.04402305,  0.03081163,\n",
       "         0.01214585, -0.00463103,  0.01641244,  0.03040701, -0.02033938,\n",
       "        -0.02994574,  0.02116141,  0.03672748,  0.03855569,  0.03036082,\n",
       "        -0.06104339, -0.06431706,  0.04710603, -0.04005239, -0.02923575,\n",
       "        -0.04868924, -0.05494317,  0.00010647, -0.01816094,  0.03138261,\n",
       "         0.05685665, -0.04856842, -0.01618521, -0.00944469, -0.01619613,\n",
       "         0.06518805,  0.00582113], dtype=float32),\n",
       " array([1.0543609, 1.0623298, 1.0098058, 1.0744287, 1.0357946, 1.0429671,\n",
       "        1.0509101, 1.0249267, 1.0129887, 1.0158522, 1.0316285, 1.0783354,\n",
       "        1.057552 , 1.0283982, 1.0199358, 1.0066394, 1.0245774, 1.0242317,\n",
       "        1.0485477, 1.0106136, 1.0363175, 1.0306447, 1.0482582, 1.0301352,\n",
       "        1.0638363, 1.0382304, 1.0263307, 1.0337476, 1.0260221, 1.020465 ,\n",
       "        1.0253808, 1.0310968], dtype=float32),\n",
       " array([ 4.7587720e-03, -1.6238237e-03,  1.9534929e-03, -2.6170271e-03,\n",
       "         1.2274629e-02,  1.5208251e-02, -6.0461033e-03, -4.8787662e-04,\n",
       "         9.8364940e-03, -2.0818489e-03, -4.5362609e-03, -8.1005329e-03,\n",
       "        -9.7563602e-03, -8.4560347e-04, -9.8834857e-03,  1.4531709e-02,\n",
       "        -6.1364705e-03,  7.7704096e-04, -9.8415418e-03,  5.8851717e-04,\n",
       "         4.1245339e-03, -8.4702503e-03,  7.0663644e-03,  2.5461166e-04,\n",
       "         3.9792966e-02,  7.5467536e-03, -5.0503756e-03, -8.0461059e-06,\n",
       "        -2.7013829e-03,  7.0414855e-03, -3.0554782e-03, -5.7210554e-03],\n",
       "       dtype=float32),\n",
       " array([-1.9794853], dtype=float32),\n",
       " array([0.1046981 , 0.12908956, 0.1829004 , ..., 0.17253095, 0.15680832,\n",
       "        0.05897636], dtype=float32),\n",
       " array([-0.00267082,  0.03352175, -0.02055457, ...,  0.0087    ,\n",
       "        -0.02317228,  0.04715523], dtype=float32),\n",
       " array([-0.04629142,  0.01984669, -0.03733759, -0.03311045,  0.01553479,\n",
       "         0.03890249,  0.04243314,  0.03471268,  0.00289006,  0.00477831,\n",
       "         0.04687842, -0.03151022, -0.01323454, -0.0174391 ,  0.03567754,\n",
       "         0.00084023, -0.00937108,  0.03561166, -0.01922412,  0.01823711,\n",
       "         0.01837337,  0.03086475,  0.02943106,  0.01969845, -0.00906053,\n",
       "        -0.04099903, -0.02063166,  0.00838087,  0.034544  ,  0.02645252,\n",
       "         0.02874401,  0.04813079], dtype=float32),\n",
       " array([0.98957795, 1.0165851 , 0.9926724 , 1.0183977 , 1.0057286 ,\n",
       "        1.0108962 , 0.99028534, 1.0207834 , 1.023801  , 0.98799074,\n",
       "        1.0334871 , 1.0348724 , 0.99449635, 1.0118091 , 0.98520774,\n",
       "        1.0140188 , 1.0104473 , 0.98772234, 0.98083   , 0.9730186 ,\n",
       "        0.98521876, 0.9931158 , 1.0047847 , 0.9993499 , 1.0028394 ,\n",
       "        0.98612696, 0.98863083, 1.0333258 , 0.98671985, 1.011769  ,\n",
       "        1.0173221 , 0.9978351 ], dtype=float32),\n",
       " array([ 0.01170489,  0.02257278, -0.00105222,  0.00473904, -0.01628743,\n",
       "         0.00684894,  0.01387427,  0.00662109,  0.00017683,  0.0064663 ,\n",
       "        -0.00273574,  0.0073344 ,  0.00384737,  0.0013388 , -0.00168578,\n",
       "         0.01335176,  0.00834601,  0.00460688, -0.00802206, -0.00971201,\n",
       "         0.00225894,  0.00461309, -0.00633695,  0.00569729, -0.01221406,\n",
       "        -0.00278937, -0.02150978, -0.00921097, -0.00580517,  0.02202255,\n",
       "        -0.00152335,  0.00258235], dtype=float32),\n",
       " array([ 0.09481844, -0.00710294,  0.03080392, ...,  0.16001447,\n",
       "        -0.00044067,  0.15934546], dtype=float32),\n",
       " array([-0.00309654, -0.03274429,  0.04916716,  0.03927528, -0.08663448,\n",
       "         0.14040683, -0.08931994,  0.1339126 , -0.14962722, -0.0345398 ,\n",
       "         0.09335373,  0.0337508 ,  0.01235044, -0.17205283, -0.11754223,\n",
       "         0.02538079, -0.1393601 , -0.075517  , -0.12851019, -0.1315992 ,\n",
       "        -0.13267827, -0.08549618, -0.10642104, -0.05228112,  0.14271368,\n",
       "        -0.1323785 ,  0.15785034, -0.03547345,  0.03981784, -0.00970564,\n",
       "         0.12503268, -0.03871686], dtype=float32),\n",
       " array([ 0.09700194,  0.06934582,  0.16233703, ..., -0.096693  ,\n",
       "        -0.16124456, -0.0952474 ], dtype=float32),\n",
       " array([ 0.10815856, -0.05590677, -0.11745872, -0.06607746, -0.12014314,\n",
       "        -0.01716812,  0.03489819, -0.13184741,  0.17133334,  0.07979791,\n",
       "        -0.08674331, -0.15159886, -0.1044507 ,  0.09474354, -0.04816077,\n",
       "         0.12159634, -0.05295147, -0.09106677, -0.12868132, -0.18129301,\n",
       "        -0.12989277,  0.1759402 ,  0.08588562,  0.01001737,  0.04797846,\n",
       "        -0.10327046, -0.031287  , -0.0666276 , -0.07930421, -0.11545306,\n",
       "        -0.1241065 ,  0.17286569], dtype=float32),\n",
       " array([1.0381551, 1.0509892, 1.0328498, 1.037154 , 1.0347803, 1.0191044,\n",
       "        1.007469 , 1.0310766, 1.0305271, 1.0203456, 1.0126531, 1.0317557,\n",
       "        1.0307864, 1.0447806, 1.0183555, 1.0560244, 1.0110406, 1.0493648,\n",
       "        1.0148269, 1.0113845, 1.0239077, 1.0296304, 1.0176089, 1.0221463,\n",
       "        1.0137706, 1.0115578, 1.0127918, 1.016662 , 1.0314462, 1.0241715,\n",
       "        1.0303311, 1.029908 ], dtype=float32),\n",
       " array([-0.0184374 , -0.01826629,  0.00732533, -0.0026924 , -0.00209853,\n",
       "         0.0077059 ,  0.00801213,  0.00250418,  0.00738831, -0.00063783,\n",
       "        -0.00670971, -0.00060837,  0.00744078,  0.00153141,  0.00837327,\n",
       "        -0.01123857, -0.00539339, -0.00349648, -0.00572478, -0.00829392,\n",
       "        -0.00970067,  0.00473564,  0.01120222, -0.0174473 ,  0.00494299,\n",
       "         0.00286463, -0.00384045,  0.00510054, -0.00070829,  0.01417619,\n",
       "        -0.00538645,  0.00453821], dtype=float32),\n",
       " array([-2.021723], dtype=float32),\n",
       " array([ 0.08764898, -0.01132939, -0.0625816 , ..., -0.14786325,\n",
       "         0.04692417, -0.01305877], dtype=float32),\n",
       " array([ 0.05363803, -0.03204749,  0.00960356, ..., -0.00470475,\n",
       "        -0.01753081, -0.04147169], dtype=float32),\n",
       " array([ 0.01600202,  0.04480466,  0.02554111,  0.03556166,  0.02117219,\n",
       "        -0.03572193,  0.03176542, -0.00960182, -0.00783789, -0.00524372,\n",
       "         0.03138946, -0.03778027,  0.04614178,  0.01132369,  0.02739572,\n",
       "        -0.04232454,  0.01687333, -0.0234869 , -0.00256974, -0.00211477,\n",
       "         0.04447647, -0.00615249, -0.02162469,  0.00449162, -0.01253418,\n",
       "        -0.00265938, -0.01353268,  0.02899394,  0.03002707,  0.00734769,\n",
       "         0.00376014,  0.04568576], dtype=float32),\n",
       " array([0.9995346 , 1.0088879 , 1.0087744 , 1.0111398 , 1.0014738 ,\n",
       "        1.0048078 , 1.006345  , 1.0044831 , 0.98891735, 1.0120902 ,\n",
       "        0.997282  , 0.99740076, 0.99455625, 0.99415684, 0.9881059 ,\n",
       "        1.0245734 , 1.036016  , 0.9988743 , 0.9814527 , 1.0178295 ,\n",
       "        0.98610157, 0.9955268 , 1.0329517 , 0.9787283 , 0.9875708 ,\n",
       "        1.0103861 , 0.9996267 , 1.0164112 , 1.000561  , 1.0036838 ,\n",
       "        0.9927301 , 1.0058864 ], dtype=float32),\n",
       " array([ 0.00126106, -0.00995111, -0.00105432, -0.00410776, -0.00175961,\n",
       "        -0.00418487, -0.00166059,  0.00987996, -0.01479694, -0.0091189 ,\n",
       "         0.00892869, -0.00260633,  0.01672128,  0.00241499, -0.00589707,\n",
       "         0.0053129 ,  0.02726773,  0.00291854, -0.02292134,  0.01201242,\n",
       "        -0.00623263,  0.00366211, -0.00696867,  0.00590336,  0.00778371,\n",
       "         0.00244539,  0.01343364,  0.01105696,  0.00261213, -0.00432164,\n",
       "         0.01497048, -0.00885469], dtype=float32),\n",
       " array([-0.12322558, -0.00664347,  0.02314015, ...,  0.00320935,\n",
       "         0.05134724,  0.02759915], dtype=float32),\n",
       " array([-0.06769125,  0.15445164,  0.12908366,  0.08943732, -0.04836117,\n",
       "        -0.03897025,  0.08557693,  0.14920849,  0.13812174,  0.0185168 ,\n",
       "         0.13033348,  0.06746626, -0.13347138,  0.10402048,  0.03681364,\n",
       "         0.12322158, -0.0194998 ,  0.03472175, -0.11273111,  0.04536862,\n",
       "         0.11459476,  0.00147015,  0.1803871 , -0.07707051,  0.05050749,\n",
       "         0.05287874,  0.13432078,  0.11738475, -0.01220524, -0.04929574,\n",
       "         0.03593465,  0.08846621], dtype=float32),\n",
       " array([-0.05751168,  0.02132488,  0.1710516 , ..., -0.10594554,\n",
       "        -0.16792758, -0.16857207], dtype=float32),\n",
       " array([ 0.06702385, -0.02223056, -0.04173024, -0.17568067, -0.08164407,\n",
       "         0.03200922,  0.06774728,  0.10568926, -0.06404836,  0.09846278,\n",
       "         0.08761989,  0.12326969,  0.15500763,  0.11110439, -0.09181222,\n",
       "        -0.17545658,  0.09553746, -0.11758521, -0.07952856, -0.03795899,\n",
       "        -0.00371546, -0.04506514, -0.15100089, -0.05218128, -0.17661357,\n",
       "         0.04204578,  0.13343737, -0.11589281, -0.1152584 ,  0.1536893 ,\n",
       "        -0.13048556, -0.02515482], dtype=float32),\n",
       " array([1.0397134 , 1.0360564 , 1.0156138 , 0.9884616 , 1.027117  ,\n",
       "        1.0113975 , 1.0036372 , 1.0240996 , 1.0322311 , 1.0238061 ,\n",
       "        1.0434765 , 1.0633986 , 1.041035  , 1.0169104 , 1.0086396 ,\n",
       "        1.0246613 , 1.020269  , 1.0398608 , 1.0193558 , 0.99803627,\n",
       "        1.0031512 , 1.0353987 , 1.0063608 , 1.0042157 , 1.0319779 ,\n",
       "        1.0046766 , 1.025824  , 1.0258359 , 1.0226268 , 1.0127302 ,\n",
       "        1.0605794 , 1.038584  ], dtype=float32),\n",
       " array([-0.00118683, -0.00557361, -0.00116828,  0.01401458, -0.00355641,\n",
       "         0.00112038,  0.00218613,  0.00643464,  0.01746091, -0.01064302,\n",
       "         0.00224553, -0.00085569, -0.00381136, -0.00313738, -0.00669591,\n",
       "        -0.01033898,  0.0135265 , -0.00999865, -0.01035064,  0.00467288,\n",
       "        -0.00859803, -0.00636511,  0.01279149, -0.01709039,  0.01117568,\n",
       "         0.00791391, -0.00163419,  0.00229868,  0.01340969,  0.01048534,\n",
       "        -0.00288805, -0.01204227], dtype=float32),\n",
       " array([-2.0103302], dtype=float32),\n",
       " array([ 0.12241649, -0.10660449,  0.17549561, ..., -0.05348109,\n",
       "        -0.01452383, -0.12988281], dtype=float32),\n",
       " array([-0.00194141,  0.02589881, -0.03037514, ..., -0.00754311,\n",
       "        -0.06075472, -0.03379083], dtype=float32),\n",
       " array([-0.04088634,  0.02913183, -0.03612743, -0.04616424, -0.04067439,\n",
       "         0.0079542 , -0.01157914, -0.04366747, -0.0151046 , -0.03292144,\n",
       "         0.02210731, -0.00392987,  0.01686208, -0.0164911 ,  0.0252503 ,\n",
       "        -0.03331709,  0.02212314,  0.01209249, -0.0043841 , -0.0283968 ,\n",
       "        -0.00295448,  0.00459554, -0.03148745,  0.0085917 ,  0.00278201,\n",
       "        -0.00112356,  0.00047268, -0.02214845,  0.01191059, -0.03796038,\n",
       "        -0.03830084,  0.02503845], dtype=float32),\n",
       " array([0.99456507, 0.9966103 , 1.0424459 , 0.99495673, 1.0067877 ,\n",
       "        0.99314815, 1.0023026 , 1.0031432 , 0.9909617 , 1.0102311 ,\n",
       "        0.99826586, 1.0142379 , 1.0150396 , 0.9921485 , 1.0059283 ,\n",
       "        1.0072627 , 0.9872849 , 0.9921293 , 1.0057395 , 0.9951863 ,\n",
       "        0.98194087, 1.0000129 , 0.9865948 , 0.99105173, 1.0016098 ,\n",
       "        1.0210606 , 1.0116878 , 0.99439126, 0.99327   , 1.0343517 ,\n",
       "        0.9899196 , 0.99743694], dtype=float32),\n",
       " array([ 0.00428308,  0.00701679, -0.01299673, -0.0097565 , -0.00513786,\n",
       "        -0.00540209, -0.01190389, -0.00193552, -0.00768709, -0.00656132,\n",
       "         0.00610603,  0.00828034,  0.01076629,  0.00934397,  0.00600968,\n",
       "        -0.00752056, -0.00330429,  0.00552782,  0.0079286 , -0.00098885,\n",
       "         0.00825717,  0.00156999, -0.00190045,  0.00955413,  0.00261114,\n",
       "         0.00723719,  0.00676527,  0.004172  , -0.00758256,  0.00051237,\n",
       "         0.00909477,  0.00616445], dtype=float32),\n",
       " array([ 0.10892423,  0.1528287 , -0.11021499, ..., -0.19606352,\n",
       "         0.01078528,  0.09592649], dtype=float32),\n",
       " array([ 0.09012216, -0.10910741,  0.06716213,  0.02835815, -0.08075795,\n",
       "         0.08586757, -0.09659604, -0.13034338,  0.02445013, -0.11029066,\n",
       "         0.07501067, -0.04152753, -0.15406482,  0.12537763, -0.08515856,\n",
       "         0.11983804,  0.10372449,  0.04212542, -0.1338153 , -0.12261631,\n",
       "         0.11079007,  0.04421556, -0.08028109,  0.02296845,  0.1444235 ,\n",
       "        -0.18035609, -0.00601039, -0.07025724,  0.05627549,  0.11839302,\n",
       "         0.12644549, -0.07367486], dtype=float32),\n",
       " array([-0.07103003, -0.16112958, -0.0080869 , ...,  0.0046647 ,\n",
       "        -0.13639013,  0.16177244], dtype=float32),\n",
       " array([ 0.12782495, -0.08584601,  0.03952745,  0.05250581,  0.1597433 ,\n",
       "        -0.11762319, -0.08883954, -0.11016653, -0.14073427,  0.1691517 ,\n",
       "         0.16738555,  0.00024581,  0.12512401, -0.0950015 ,  0.03209921,\n",
       "         0.16660042, -0.08799843,  0.10107471,  0.05019651,  0.02284424,\n",
       "         0.16259481, -0.11424185, -0.05824362, -0.02905563,  0.09064201,\n",
       "         0.04158616, -0.05722928,  0.04516091, -0.1258488 ,  0.13967279,\n",
       "        -0.03246322,  0.09879778], dtype=float32),\n",
       " array([1.0176898 , 1.0342287 , 1.0113958 , 1.0070212 , 1.0116849 ,\n",
       "        0.9979331 , 1.0166203 , 1.0077075 , 1.0015236 , 1.0137587 ,\n",
       "        1.008853  , 1.042278  , 1.0185761 , 1.0145785 , 1.0121638 ,\n",
       "        0.99486   , 1.0151054 , 1.0031643 , 1.0221418 , 0.9847376 ,\n",
       "        1.0077634 , 1.0303844 , 1.0017195 , 1.004657  , 1.0030359 ,\n",
       "        1.0062616 , 0.99816895, 1.0321896 , 1.0109035 , 0.9967668 ,\n",
       "        1.0393738 , 1.0181305 ], dtype=float32),\n",
       " array([ 0.00717223, -0.01555925,  0.0141202 ,  0.00991148,  0.00052276,\n",
       "         0.01756853,  0.00012441,  0.00513316,  0.01492967, -0.01077067,\n",
       "        -0.0171135 , -0.01526776,  0.01593089,  0.00830987,  0.00173   ,\n",
       "        -0.00013912, -0.00215896,  0.00720884, -0.01516176,  0.01180556,\n",
       "         0.01057776,  0.0002929 ,  0.00229064,  0.00645482, -0.01004664,\n",
       "        -0.0163062 , -0.00186024, -0.01312319,  0.00735255,  0.01561389,\n",
       "         0.00362719,  0.01831672], dtype=float32),\n",
       " array([-2.03622], dtype=float32),\n",
       " array([-0.09594408, -0.09930847, -0.20664258, ..., -0.01436842,\n",
       "        -0.09127838,  0.14646973], dtype=float32),\n",
       " array([-0.05085621,  0.00834381,  0.01845008, ...,  0.06042161,\n",
       "        -0.04065337,  0.02290916], dtype=float32),\n",
       " array([-0.04076346,  0.04422327,  0.00601015,  0.00210187,  0.03846258,\n",
       "        -0.01498831,  0.0461727 , -0.03771768,  0.02818094,  0.03477593,\n",
       "         0.03012918, -0.02465662, -0.02316745, -0.00460478,  0.02625267,\n",
       "        -0.00780607,  0.03643164,  0.00964379,  0.00208753,  0.00790442,\n",
       "         0.01947026,  0.05293765, -0.03505671,  0.00053803, -0.00047452,\n",
       "        -0.0036691 ,  0.02035953,  0.01661989, -0.04811352, -0.00822738,\n",
       "         0.00400003,  0.04757879], dtype=float32),\n",
       " array([1.0098583 , 0.99801   , 1.001545  , 1.0087783 , 0.99335134,\n",
       "        0.9935679 , 0.97173584, 1.0271767 , 1.0007353 , 0.9891025 ,\n",
       "        0.9986608 , 0.9947401 , 1.0204195 , 1.0172557 , 1.0012143 ,\n",
       "        1.0169286 , 0.9731688 , 1.0151949 , 0.98866904, 1.011993  ,\n",
       "        1.0134258 , 1.0105011 , 0.9834452 , 0.98785716, 0.99294394,\n",
       "        1.012881  , 1.0373296 , 1.0148195 , 0.9907004 , 1.002407  ,\n",
       "        1.0021015 , 0.98861086], dtype=float32),\n",
       " array([-3.3242176e-03,  1.0343899e-02,  1.5471463e-02,  1.0037853e-03,\n",
       "         6.0514142e-03, -7.3252660e-03, -1.6810726e-02,  7.9476023e-03,\n",
       "        -1.3853167e-03,  7.6564648e-03,  8.9422939e-03,  1.3637253e-02,\n",
       "        -1.6052945e-02, -1.9040877e-02,  3.2620577e-03, -8.8264970e-03,\n",
       "        -1.5528357e-02, -8.0608157e-03, -1.4046316e-02,  4.0106708e-03,\n",
       "         5.4007489e-03,  1.0016360e-02, -3.1186831e-03,  8.5133361e-03,\n",
       "         1.4378817e-02, -1.6657822e-02,  2.2614369e-02,  6.2071667e-03,\n",
       "        -5.8565051e-03, -2.5616398e-03,  4.7055681e-05,  1.7697720e-03],\n",
       "       dtype=float32),\n",
       " array([ 0.09251676,  0.04994077, -0.13747904, ..., -0.08087809,\n",
       "         0.04728699,  0.04467866], dtype=float32),\n",
       " array([ 0.08731391,  0.1146094 , -0.15094681, -0.03004471,  0.12777512,\n",
       "        -0.05284437, -0.10029251,  0.07309463,  0.10691709, -0.04351174,\n",
       "         0.15827553, -0.01917716,  0.16791378,  0.01106759,  0.18552677,\n",
       "        -0.08383556,  0.0535153 ,  0.11735144, -0.08343211, -0.12778421,\n",
       "         0.09381544,  0.01826256, -0.15373805, -0.09191262, -0.09080613,\n",
       "        -0.13537037,  0.06431484,  0.01063441, -0.00301762, -0.01385779,\n",
       "         0.15036467,  0.07171933], dtype=float32),\n",
       " array([-0.11707721,  0.04183974, -0.06656744, ..., -0.16098796,\n",
       "         0.10197715,  0.05889653], dtype=float32),\n",
       " array([-0.13539854,  0.14216277, -0.07497276, -0.00098016,  0.16234252,\n",
       "        -0.03710089,  0.06123197, -0.14782381, -0.0349304 ,  0.03848796,\n",
       "         0.18438143,  0.02088561, -0.07738088,  0.1373755 , -0.09360298,\n",
       "         0.05922226, -0.12132006,  0.06355709, -0.06078677, -0.00513838,\n",
       "         0.03123726, -0.00413829,  0.15538177, -0.0299753 ,  0.02033518,\n",
       "        -0.10691273, -0.01999041, -0.02624104,  0.01575424, -0.07962833,\n",
       "        -0.01835806, -0.10581714], dtype=float32),\n",
       " array([1.0302602 , 1.0107586 , 0.98381823, 0.9998689 , 1.0063831 ,\n",
       "        0.9899273 , 1.0015205 , 1.0051608 , 0.9957417 , 1.0122274 ,\n",
       "        0.99212116, 0.9955691 , 1.0013586 , 1.0098099 , 0.98450327,\n",
       "        0.98814946, 1.0076177 , 1.0083561 , 1.0139639 , 1.0125611 ,\n",
       "        0.999429  , 1.0181583 , 0.995667  , 1.0028936 , 1.0038913 ,\n",
       "        1.0292805 , 1.0153422 , 1.0172046 , 1.0007945 , 1.0058533 ,\n",
       "        1.0153288 , 1.0155375 ], dtype=float32),\n",
       " array([-0.00073596,  0.00698547, -0.00093158, -0.00063276, -0.00679908,\n",
       "        -0.00244651, -0.01301655, -0.00105297,  0.01273182, -0.00665577,\n",
       "        -0.00045956, -0.01937642,  0.00278039,  0.0134857 , -0.01313331,\n",
       "        -0.00283306, -0.00449279,  0.01431082, -0.00087355,  0.01298067,\n",
       "        -0.00060269,  0.00277251, -0.00905241, -0.00345414,  0.00079396,\n",
       "        -0.01478152, -0.00032204, -0.00154881, -0.0058029 ,  0.0026593 ,\n",
       "         0.0073412 ,  0.02129688], dtype=float32),\n",
       " array([-2.0427485], dtype=float32),\n",
       " array([-0.0496952 ,  0.16036077,  0.03453469, ...,  0.19573757,\n",
       "        -0.05879696,  0.07428261], dtype=float32),\n",
       " array([-0.03025814, -0.0306597 , -0.04079103, ...,  0.04603536,\n",
       "        -0.03138965, -0.00221012], dtype=float32),\n",
       " array([-0.0296782 , -0.00324523, -0.01821469, -0.03911923,  0.045038  ,\n",
       "         0.01459591,  0.04485077, -0.04410154, -0.01058565,  0.0401541 ,\n",
       "         0.01503421,  0.03135538,  0.04385492,  0.00492561,  0.00803836,\n",
       "         0.01649202, -0.01205531,  0.02120985,  0.01038929, -0.04495941,\n",
       "         0.00252463,  0.03776439, -0.01384918,  0.01973398, -0.05032898,\n",
       "         0.02853504,  0.01377498, -0.00332397, -0.04407999, -0.04899406,\n",
       "        -0.00837414,  0.03736969], dtype=float32),\n",
       " array([1.0036968 , 1.0048833 , 1.0267125 , 0.98779476, 1.0086509 ,\n",
       "        0.9935156 , 0.97099197, 0.97661173, 0.9942599 , 1.0067112 ,\n",
       "        0.9877706 , 0.9905086 , 1.0109535 , 1.000407  , 1.0086321 ,\n",
       "        1.0079432 , 1.0257831 , 0.9617667 , 1.0110635 , 1.0288445 ,\n",
       "        1.0236746 , 0.9926768 , 0.9781577 , 0.9902693 , 0.9800609 ,\n",
       "        1.0163877 , 1.0044487 , 0.993989  , 0.9932169 , 1.0143279 ,\n",
       "        1.0091846 , 1.0099101 ], dtype=float32),\n",
       " array([-0.00020221, -0.01416606,  0.00342261, -0.00086389, -0.0011503 ,\n",
       "        -0.01431799, -0.02214391,  0.01380144, -0.01303289, -0.01576976,\n",
       "        -0.00999712, -0.00920655,  0.00371058,  0.0125574 ,  0.00878928,\n",
       "         0.00734654,  0.01276566,  0.00548936,  0.00950922, -0.00253176,\n",
       "         0.00131935,  0.00697284, -0.00123097,  0.01135261,  0.02146111,\n",
       "         0.00851021, -0.00173879, -0.0059525 , -0.00930217, -0.02379791,\n",
       "        -0.00931512, -0.00719395], dtype=float32),\n",
       " array([ 0.15410782,  0.12696871,  0.02877756, ..., -0.03543261,\n",
       "         0.00376195, -0.06065448], dtype=float32),\n",
       " array([-0.1037133 , -0.16832949, -0.11800648, -0.11699707,  0.11068609,\n",
       "         0.02599099,  0.03906133, -0.06580454, -0.1625106 ,  0.07863134,\n",
       "         0.16124412, -0.1159401 , -0.14640065, -0.11159142, -0.12005401,\n",
       "         0.10567959, -0.02954335, -0.03625239,  0.15899095, -0.02893827,\n",
       "        -0.08535264,  0.06861418, -0.0135768 , -0.16126458, -0.1099204 ,\n",
       "         0.07353006, -0.15114719, -0.14812677,  0.15595527,  0.01952767,\n",
       "         0.09752593, -0.00876099], dtype=float32),\n",
       " array([-0.02824925, -0.00874289,  0.13853234, ..., -0.06687214,\n",
       "        -0.0785018 , -0.05672644], dtype=float32),\n",
       " array([ 0.1679799 , -0.10055694, -0.03036526,  0.12451851,  0.00577962,\n",
       "         0.12103617, -0.0917898 , -0.04633261,  0.14695624,  0.09971904,\n",
       "         0.05746084,  0.1701852 , -0.05587967,  0.11501476, -0.10196249,\n",
       "        -0.1318249 ,  0.09634143,  0.00240791,  0.07507275, -0.01868908,\n",
       "         0.10357612,  0.11653627, -0.13345373, -0.08362858, -0.14640962,\n",
       "        -0.06620913,  0.18542528, -0.06359588,  0.0263601 ,  0.12068257,\n",
       "        -0.14595582, -0.14480795], dtype=float32),\n",
       " array([1.0039374 , 0.9949623 , 0.9893381 , 0.99352413, 1.0080194 ,\n",
       "        0.9933798 , 1.0055305 , 0.97743267, 1.0210463 , 1.0155905 ,\n",
       "        0.9969711 , 0.9947285 , 1.0007567 , 1.00808   , 0.99941415,\n",
       "        0.9684245 , 1.0082359 , 0.9991777 , 1.007148  , 1.00887   ,\n",
       "        1.0082861 , 0.9933598 , 0.97318256, 1.0129443 , 0.99958414,\n",
       "        1.02677   , 1.0126461 , 0.99060845, 1.0094966 , 1.0125703 ,\n",
       "        1.0208052 , 1.0252862 ], dtype=float32),\n",
       " array([ 0.01525626,  0.00014511,  0.01860235,  0.00538446, -0.00660721,\n",
       "         0.0029864 , -0.00150275,  0.00326665,  0.00488196,  0.00621175,\n",
       "         0.00534414,  0.00482248,  0.00200089,  0.00372999, -0.00252694,\n",
       "         0.01597194, -0.00434787,  0.00287253, -0.01286211,  0.00369479,\n",
       "         0.00545171,  0.01213506,  0.01959384, -0.00597301, -0.00607516,\n",
       "        -0.0181019 ,  0.01684524,  0.00814509, -0.00840093,  0.01518467,\n",
       "        -0.01865516,  0.0075924 ], dtype=float32),\n",
       " array([-2.0531857], dtype=float32),\n",
       " array([-0.12556231,  0.12697607,  0.02230321, ...,  0.10373671,\n",
       "        -0.09860355, -0.17498647], dtype=float32),\n",
       " array([-0.03453695,  0.0419487 , -0.04604743, ..., -0.01276587,\n",
       "         0.03155693,  0.00989414], dtype=float32),\n",
       " array([ 0.02992523, -0.02862153, -0.03090579, -0.04108427,  0.03045381,\n",
       "        -0.02169579,  0.01910605, -0.04161505,  0.0250085 ,  0.02011118,\n",
       "         0.03991507,  0.01108822, -0.03647574,  0.00241992, -0.03662302,\n",
       "        -0.05140369, -0.02032048,  0.00113754,  0.00819535,  0.00363488,\n",
       "         0.04689814, -0.00140132,  0.02138449, -0.03687465, -0.05084195,\n",
       "        -0.02378524, -0.01783269,  0.03795895,  0.03455656, -0.03884776,\n",
       "         0.00587923,  0.02554547], dtype=float32),\n",
       " array([1.0066203 , 1.0026755 , 1.0082343 , 1.0114313 , 1.0059099 ,\n",
       "        1.0045455 , 0.9945063 , 0.982818  , 0.99475306, 1.0082146 ,\n",
       "        1.0028342 , 1.0094522 , 0.975508  , 0.98767346, 1.0045185 ,\n",
       "        0.9922239 , 0.9961425 , 1.0249463 , 0.99767023, 1.0044246 ,\n",
       "        1.0004174 , 0.9974713 , 1.0135366 , 1.0046349 , 0.9891785 ,\n",
       "        0.9807932 , 1.0028144 , 0.9853536 , 0.99313074, 1.0038661 ,\n",
       "        1.0007885 , 0.9830283 ], dtype=float32),\n",
       " array([-0.00643742,  0.02594767,  0.00431969,  0.00522915, -0.00559229,\n",
       "         0.01409016,  0.00697098, -0.00269564, -0.01374575, -0.00561526,\n",
       "        -0.00073405,  0.00590663,  0.01129526,  0.00788614,  0.00659876,\n",
       "         0.00999822, -0.01108068, -0.01263622, -0.01188867,  0.00926318,\n",
       "        -0.00958779, -0.00571205,  0.00627978, -0.00652853,  0.01743457,\n",
       "        -0.00227877,  0.00588167, -0.0048806 , -0.00500866,  0.01006201,\n",
       "         0.00554736,  0.00048262], dtype=float32),\n",
       " array([-0.16614778, -0.07599632, -0.11857814, ...,  0.13690959,\n",
       "        -0.17916246,  0.01840238], dtype=float32),\n",
       " array([ 0.1295037 , -0.06027419, -0.04571208,  0.11568324, -0.13236985,\n",
       "        -0.14867114, -0.10206158,  0.11751109,  0.10586563, -0.0759277 ,\n",
       "        -0.04829034,  0.0359688 ,  0.15473524, -0.08943424, -0.12531984,\n",
       "         0.13397713,  0.13391688,  0.0371673 , -0.08728435,  0.08389685,\n",
       "         0.0264553 , -0.14625745, -0.13545434,  0.0074118 , -0.07944244,\n",
       "         0.14041924, -0.0327547 , -0.1610133 , -0.1867587 ,  0.17797926,\n",
       "        -0.09132447,  0.03816469], dtype=float32),\n",
       " array([ 0.06911961, -0.09802467,  0.1638142 , ..., -0.09545265,\n",
       "        -0.06247413, -0.03256719], dtype=float32),\n",
       " array([ 0.00704816,  0.05780771,  0.14702791,  0.06929162,  0.1288884 ,\n",
       "         0.12835139, -0.07898729, -0.13992786, -0.1414465 ,  0.04577697,\n",
       "        -0.04400901,  0.06866424, -0.10530593, -0.10763233, -0.09841356,\n",
       "         0.11225143,  0.00339045, -0.15577598,  0.15259118,  0.14241284,\n",
       "        -0.09079279,  0.09493367,  0.01024842,  0.17351396, -0.18139592,\n",
       "        -0.05282871,  0.07311082,  0.08391017, -0.01729327, -0.02519515,\n",
       "        -0.09954499, -0.10949122], dtype=float32),\n",
       " array([1.0739735, 1.0636394, 1.0833497, 1.03698  , 1.0183266, 1.0157089,\n",
       "        1.0797126, 1.0416815, 1.0961621, 1.0899501, 1.0898637, 1.0752814,\n",
       "        1.0851413, 1.1146846, 1.0564502, 1.0852135, 1.0868975, 1.0350362,\n",
       "        1.0350436, 1.0699891, 1.1212949, 1.0762428, 1.090435 , 1.0183947,\n",
       "        1.1037498, 1.1140113, 1.1042297, 1.0999942, 1.0787727, 1.0346438,\n",
       "        1.0700048, 1.0688536], dtype=float32),\n",
       " array([-0.06106897, -0.02338436, -0.06784511, -0.01321118, -0.04270261,\n",
       "        -0.04792038,  0.07416964, -0.00819153,  0.03292737, -0.06266003,\n",
       "         0.07151918,  0.06950691,  0.01707062, -0.06765813,  0.05319089,\n",
       "        -0.0611706 ,  0.06233479,  0.02060875,  0.0672126 ,  0.01198525,\n",
       "         0.07274675,  0.04715975, -0.06810802,  0.01618643, -0.08750316,\n",
       "        -0.0477704 ,  0.08993878,  0.04781753,  0.06209016,  0.00665118,\n",
       "        -0.07122637,  0.04406325], dtype=float32),\n",
       " array([-1.47172421e-01,  1.25859126e-01, -5.27897775e-02,  2.64887437e-02,\n",
       "        -2.58483887e-02, -8.79586563e-02,  6.22620322e-02,  1.33022442e-01,\n",
       "        -2.07770213e-01,  1.44794151e-01,  3.05205770e-02, -8.45499411e-02,\n",
       "        -1.72459871e-01,  8.63271803e-02, -5.52657619e-02,  8.62043425e-02,\n",
       "         1.53523192e-01, -1.44586667e-01,  1.14552200e-01,  1.36140212e-01,\n",
       "         1.77848011e-01,  2.31687859e-01, -9.06470492e-02, -4.88354340e-02,\n",
       "        -2.00889006e-01, -1.96967736e-01,  9.39968824e-02,  5.47176786e-03,\n",
       "        -1.72588855e-01, -1.37085930e-01, -2.24821702e-01,  3.81447114e-02,\n",
       "         1.89194933e-03,  1.63583115e-01, -2.44795214e-02, -1.29947051e-01,\n",
       "         1.13947056e-01,  1.17203265e-01, -9.02466625e-02,  1.72629178e-01,\n",
       "         6.62892386e-02,  8.06757435e-03, -2.43182108e-01,  1.23752113e-02,\n",
       "        -6.64597675e-02, -2.09076423e-02, -2.27632493e-01, -2.42338050e-02,\n",
       "        -1.23955607e-01,  4.59429324e-02,  9.08732414e-02,  4.97967266e-02,\n",
       "        -1.75511464e-02, -1.00615636e-01,  1.86950818e-01, -1.41655862e-01,\n",
       "         1.19024768e-01,  2.36752123e-01,  3.08933333e-02, -8.53726566e-02,\n",
       "         1.15467757e-02,  3.66321243e-02, -1.19184924e-03, -2.78698374e-02,\n",
       "         6.96978942e-02,  1.09185562e-01, -5.87858483e-02,  1.46672398e-01,\n",
       "        -1.27228752e-01,  8.85658413e-02,  5.73471002e-02, -1.53959349e-01,\n",
       "         1.17266439e-01, -9.06746015e-02,  1.25934362e-01,  1.53801739e-01,\n",
       "        -6.54359758e-02, -1.48415968e-01, -1.85775518e-01,  1.31415129e-02,\n",
       "        -6.69170991e-02, -1.11762209e-04, -6.18858747e-02,  1.51210487e-01,\n",
       "        -1.34602457e-01,  8.17849711e-02, -1.76341820e-03, -1.57106161e-01,\n",
       "        -7.06205890e-02,  7.71425944e-03,  7.20026493e-02, -3.18434201e-02,\n",
       "         1.02192953e-01,  1.83034062e-01, -1.21829778e-01, -1.21262439e-01,\n",
       "        -4.35313443e-03,  1.99227750e-01, -8.93351063e-03, -4.68413681e-02,\n",
       "         1.00275286e-01,  1.56719282e-01,  2.63662431e-02, -1.27321944e-01,\n",
       "        -1.80131599e-01,  7.70163834e-02,  7.37209991e-02, -1.53459296e-01,\n",
       "        -7.77145103e-02,  4.05102596e-02, -3.08604240e-02, -3.21648829e-02,\n",
       "        -2.39536673e-01,  1.00088172e-01, -6.70894012e-02, -1.27419859e-01,\n",
       "        -4.85823229e-02, -1.33427560e-01,  4.59335931e-02,  1.25509081e-02,\n",
       "         2.41251484e-01,  1.37851849e-01, -1.02245919e-01, -7.82937109e-02,\n",
       "        -9.53870118e-02, -1.50548099e-02,  1.50485173e-01, -7.87733942e-02,\n",
       "         4.66826744e-02, -8.70198160e-02,  6.94376752e-02, -5.04554249e-02,\n",
       "         2.31425226e-01, -8.35184678e-02, -5.63250296e-02,  8.73210728e-02,\n",
       "        -5.97969070e-03,  2.07556605e-01, -2.47046009e-01, -1.93993330e-01,\n",
       "        -1.21024467e-01,  5.29824905e-02, -2.33393669e-01,  1.56761438e-01,\n",
       "         4.21465673e-02, -1.06038235e-01, -9.72250104e-02, -2.43156850e-02,\n",
       "        -2.68556893e-01, -2.41276808e-02,  7.16996342e-02, -6.04592040e-02,\n",
       "        -3.84154427e-03, -2.18387507e-02, -1.64138123e-01,  1.24697708e-01,\n",
       "         2.28148587e-02,  1.88714266e-02,  2.29548380e-01,  1.83658972e-02,\n",
       "        -5.46220616e-02,  7.84652159e-02, -2.21030012e-01,  6.24493770e-02,\n",
       "        -2.61108782e-02, -9.99181867e-02,  1.55422777e-01, -1.86112091e-01,\n",
       "         1.72488540e-01,  3.90822552e-02,  5.03550582e-02,  1.32668987e-01,\n",
       "        -1.55035183e-01, -8.31234753e-02,  1.98848382e-01, -1.45509720e-01,\n",
       "         1.57461181e-01,  7.60406926e-02,  1.09495960e-01,  1.14387080e-01,\n",
       "        -3.24993464e-03, -1.42980099e-01, -1.90718681e-01, -1.64175317e-01,\n",
       "         9.35470778e-03,  1.53685138e-01,  2.10525170e-01,  1.96312025e-01,\n",
       "         2.20923677e-01, -7.61848316e-02,  7.38290995e-02, -1.36148170e-01,\n",
       "        -8.26547891e-02, -6.29106611e-02, -2.87386030e-02,  8.28787833e-02,\n",
       "        -7.51806498e-02,  5.22669926e-02, -3.97176482e-02,  8.02416876e-02,\n",
       "         1.02618717e-01, -1.11038638e-02, -1.82444066e-01, -1.58955500e-01,\n",
       "         1.18116967e-01,  2.12556049e-01, -2.90583037e-02,  1.87320843e-01,\n",
       "        -1.66962489e-01,  1.22432753e-01, -2.09234864e-01,  8.41882974e-02,\n",
       "         8.16920213e-03, -3.32875550e-02,  1.58064961e-01,  1.51846990e-01,\n",
       "         2.27457911e-01,  8.71953834e-03, -5.38063534e-02,  9.85919461e-02,\n",
       "        -1.47048920e-01, -1.25932068e-01,  1.67114865e-02, -2.27394611e-01,\n",
       "        -2.10451946e-01, -3.82073298e-02, -7.18526542e-02, -1.76160291e-01,\n",
       "        -9.26147867e-03,  9.58380699e-02,  1.49627581e-01,  9.44066122e-02,\n",
       "         1.32477805e-01, -1.03763990e-01,  2.18091145e-01,  1.73318356e-01,\n",
       "         1.89321473e-01, -1.25128821e-01,  1.98975444e-01, -1.54868349e-01,\n",
       "         1.83104083e-01,  1.63384661e-01,  8.76606479e-02, -4.89215814e-02,\n",
       "         1.48385584e-01,  1.19654961e-01, -6.31960183e-02,  1.79309502e-01,\n",
       "        -1.74652770e-01, -7.78450668e-02,  1.21070743e-01,  1.75510943e-01,\n",
       "         1.68630928e-01,  6.64992854e-02,  2.03210325e-03,  1.69860512e-01,\n",
       "        -1.35656893e-01,  7.50225857e-02,  1.01782694e-01,  2.07668036e-01,\n",
       "        -1.63409352e-01,  2.78366134e-02,  5.76816499e-02,  2.27411408e-02,\n",
       "        -2.24020518e-02,  1.68443188e-01, -2.21140329e-02, -1.41468123e-01,\n",
       "        -6.60449043e-02, -1.21141179e-02,  6.20286167e-02,  1.39315501e-01,\n",
       "        -1.26686364e-01, -2.03755811e-01,  2.42033303e-02,  2.03841418e-01,\n",
       "         1.62292778e-01,  6.07853644e-02,  1.53217390e-01, -1.07061885e-01,\n",
       "        -2.04317972e-01, -1.06021926e-01,  1.04204752e-01, -1.87103271e-01,\n",
       "         4.78588007e-02,  1.36198953e-01,  6.02438040e-02,  1.97245404e-01,\n",
       "         1.30410820e-01,  1.05908677e-01,  1.94814339e-01,  3.19673531e-02,\n",
       "        -1.18498888e-03,  1.96099132e-01, -1.33508861e-01,  3.82031128e-02,\n",
       "         7.08284453e-02,  1.90555230e-01,  8.00601691e-02,  1.02587327e-01,\n",
       "         8.28399286e-02,  9.56734177e-03,  2.82924473e-02,  2.21528113e-01,\n",
       "        -7.92433098e-02,  3.74808125e-02,  1.19243659e-01,  1.33479089e-01,\n",
       "        -2.33634800e-01,  1.19095147e-01,  1.42753899e-01,  5.37875667e-02,\n",
       "         2.88915774e-03,  1.85816765e-01,  1.07664848e-02, -2.82154814e-03,\n",
       "        -1.22318886e-01,  3.16858664e-02,  2.25897595e-01, -4.58153635e-02],\n",
       "       dtype=float32),\n",
       " array([ 0.03431936, -0.09984956, -0.13548297, -0.21054226, -0.03873204,\n",
       "         0.15945332,  0.05752334,  0.06666125, -0.05998993, -0.23275569],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_i = np.ones((1024,1000)) * 0.5\n",
    "b_i = np.zeros((3, 1000))\n",
    "b_idx = [3,6,9]\n",
    "a_i[b_idx] = b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_i[b_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26136"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_a[0]*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 6.217371940612793\n"
     ]
    }
   ],
   "source": [
    "tmp_d, tmp_e, tmp_f = codeword_generate(weights_dict, info_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_c == tmp_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp_a.shape[0] % 8 != 0:\n",
    "    padding = np.ones((8 - tmp_a.shape[0] % 8, 1024), dtype=tmp_a.dtype)\n",
    "    udp_numpy = np.concatenate((tmp_a, padding)).T\n",
    "udp_numpy = np.packbits(udp_numpy.flatten()).reshape(1024,-1)\n",
    "tmp_array = [udp_numpy[i].tobytes() for i in range(udp_numpy.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xa8S\\xdc\\xc0q5\\xd8Z\\xd9\\xea\\x12z\\x820\"\\xd4\\xd5\\xe0\\x95s\\xfd(\\x7f\\xb3\\'\\xa5w\\xd4K\\xa4\\xa7\\xfd\\x07bN\\xa9\\xfe\\x0e\\xa2\\xf6G\\xb1\\x9b~N\\xdf&D\\xc3\\xde\\x86\\xdf\\xbcJ\\xed@]WNV\\x1dO\\xd5\\x9d\\xaa\\x13\\xe5nzQQ\\x91V\\xbaq\\x9b\\xcd\\xa7U\\xbe\\xc6\\x16\\x1f^\\xaeg\\xaf\\xcb\\xa2t,\\x08\\x15e\\xab\\xa1\\x80H\\x84u\\x80\\xe6\\xde\\xe8\\xed\\xc4\\xa9\\x94\\x97\\xa6\\x8c\\xf6\\xc0\\x98\\xe5_\\xaf\\xf3\\xe3\\xb3x\\xb7\\xc8\\x14\\xc13\\xf8e\\xa2x\\xfc\\x8be\\xa0G\\xb48\\x9aCN^r\\xcc\\x13\\x8f\\xdd\\x8c\\xbbp/\\xc7R\\xc4<0q\\x95\\x00*\\xbf+\\x8dJ]\\xaf\\x13<cJ\\x04jP\\xca5f\\xdb\\xb6\\x96\\x17-n2QVlU9\\xee\\x85X\\xb7\\x16\\xb3G\\xbf\\xfb\\x7f\\xfdF9\\x8b\\xd2w\\xdd\\x1d\\xe6\\x08:G\\x11\\xe1\\xbe\\x8f~\\xca\\x84rO\\xc1\\xc4\\x96\\xf7\\xab+;.\\x8czM~{I{\\xec\\xb0\\xe6\\xb4_\\xca#\\x046\\xbd\\x9a\\xa7_\\x193\\xfd\\x04k\\x10P\\\\\\x80:\\xc1=bN\\x0by\\xc8\\xc2\\xfcA\\xa3\\xce\\xc0\\xd5\\x1f\\x81\\x82#\\x95\\x839&\\xad\\x83P\\x89p-V\\x88\\x8d\\xce@\\x02\\x81\\xb2{}=\\xf4\\xd0\\x7f\\xbc\\xdc\\x8ce\\xad\\xb9vL\\x80g\\xa4Q\\nu\\xe0\\x7f\\xcd\\x95t\\x1a\\xf6q\\xb5\\xfc\\x08\\xe5~\\xb9p*Q\\xb2\\x00m\\x92\\x82\\xd3\\xd6\\\\\\xcc\\t\\xa3q0\\xd1\\x17\\x85Tip\\xebz\\xd0I\\xe4\\x8e\\x0bJ3\\xaf\\xc8\\xa4\\x83>wY\\xb2L!\\xc6\\xf7\\xd8\\x18_8\\xade<g\\xa1XA\\xfc\\xaf\\x1a\\xc5\\xab^\\xa7\\\\\\x83\\x18p\\xae\\xb1\\xcd}_\\x08l\\xbb\\xbf\\x1e<(b\\xc4\\x8a\\x95\\xd6J\\x98J\\xb8\\x9aF\\xa6S\\x97\\x9dV#Z\\xae\\x90\\xef\\xd8\\x85\\x1fw\\x0e\\x91^\\xd0Yb\\xe7\\xc9\\x06J4\\xc4\\x192!3\\x14\\x14T\\xeb5K\\x97\\xe5\\x1c\\xcc&\\xe7\\xa2b\\xcaP\\xcfiF\\x0c\\x0b*\\xee\\x0e\\x86\\xf6\\xb9v\\xfe\\xd4\\xb4\\xe3\\x99l\\t\\xb6(\\xc2\\x13c\\xa6\\xddV\\xa6VXU^=\\xc1S\\xc95\\x9f&\\xf6|\\x83\\xc7\\x05 |\\xb4[\\x96%\\x17\\xe7\\x07\\xd7\\xbc\\x86\\xb7@\\t\\xb3[\\x04J\\xcd\\x89\\xb1\\xd9\\xe3\\xa7NI\\xdf\\x1cx\\x18\\xcf\\xdcc\\x85\\x8f\\x80%\\xb4\\x86\\xa8ul\\xb2\\xba$\\xd3\\x88\\x00j\\xfe\\xe4j\\x10@5`5H\\x01\\xa8\\x80:\\xe6rj\\x94\\x05\\x93s\\xfer\\x1dG5K2Ir\\xb4\\xc7\\xfeT^\\xacRV\\x8fkm|4j\\x86\\x03`Irc\\xd6e\\x05\\xb7\\xbb\\xaa\\x81R\\xf3P\\x93\\x04\\x01z\\xd7\\x97Uvn^\\x0e\\xd4i[\\xd3Q\\xf6N\\x1f\\x81\\xa8\\x1e\\x9d\\x10\\xed\\x1d\\x88+\\xb8\\xb3\\x03\\x90\\xa1\\xd9T\\xe6(\\xfa\\xf6O\\xf8\\x80\\x84`\\x1d\\x07\\x98b\\x96\\x05\\xf9r\\x1d0\\xf4\\xb1\\xc3\\xf3\\x80B\\xea0\\xac\\x10\\xe4W\\x92\\r\\xe5w\\xb0\\xc3\\xa3r\\xeb6\\xc9\\xed\\x92D\\xea\\xf7\\'AB\\x82\\xe7,`q\\xa4\\xc1\\xe4\\xba\\xf9\\xbdo;\\x05\\xd6\\xcd\\xfcO\\x80\\xfbJ5~\\xcb\\n\\xd3\\xa1\\xa0~\\xfd5\\xc6=\\xb2\\xaa\\xacK*@>\\xc4\\xd3\\x16M6\\x1dd|\\xe9\\xd5%\\x17\\x944\\xac&\\xb5Vn\\xf7\\xae\\xdeq\\x0b\\x1b\\xd65\\xdbU\\xfb\\xad\\xe7g\\x05*\\xa2V\\xcb\\xa9\\xeb\\xaa\\xfc\\x12OO\\x91\\x87\\xcb\\x91:gkrCJ\\xd3\\xd8\\xec!\\xd8\\xb9\"\\xb6\\x83rW6\\xf5\\x07\\xe4\\xa4\\xa2\\x12<)\\xe9\\x93\\xef\\xce3\\x85\\x11b7\\xf09\\x97v1Eo\\x0c.\\xf4]w\\xbe;pY\\xc5l{\\x0b\\xf9>\\xf7X*W\\x91\\xc1`\\xe1+\\xb5\\xb1\\xac\\xeck\\x89p\\xd2\\xa1\\x92\\xc1\\xc6\\xfe\\x13\\x95b\\xe2\\x03\\xa3\\xc8h\\x00\\x05\\x94\\x13\\xbb\\xe4\\r\\x0c\\x8aj\\x0b\\xda\\xee2\\xfd\\x0f\\xad\\xaf\\x8aP\\x19t\\xf7/*\\xdb\\xd6\\xae\\xef\\xe4\\xb5K\\xcbw\\xdc\\xbd1\\x96\\xc9\\xd2s\\x86A4X\\xd3,\\xd5!\\x95DE\\t\\xc8Qg\\x16\\xcc^\\x16\\x9a\\xad\\x18?\\xe8yC>C?B\\\\73\\x02od\\x15\\xd8\\x7fMu\\x8b\\x8azSb\\x15i7.u\\x17_\\xfbv;\\xe1\\xb9\\xdb\\xc0aA\\x90\\xe9I\\xcc\\xecG\\x1a5\\r\\\\\\x1d\\x16)[Q,\\xd4%\\x93\\xbf\\xe1\\xe45\\x0f]\\xe2\\x1b\\xe0\\xc6\\xfe#\\x95\\x90l\\xbf\\x99W\\x9a\\x11\\xa5\\x9c\\xacy\\xfe\\x1beI\\xbf\\xba\\xce\\xa8\\x82j\\xe97\\xe5\\x0e\\xb8r\\x02\\xdd\\xd8X\\x95\\xee\\x06fx\\xa0\\x1f\\xfb\\xe9c\\xa8\\x89Kwm\\x84\\xbd\\xc1[4\\xdc2\\xc4\\xc8\\\\\\\\\\x03-Q4\\xf6&\\xe2\\xfc=\\x99\\x99\\xe4\\xcfQl\\xc8\\xe5hg\\xfa|\\xf6\\xea-\\xed=\\xfc\\xd8\\x86N\\xc82\\xac39wC\\xd7H\\xf9tt\\x04\\x9bg\\xb4\\xe5\\xfb6x\\xa8L<\\xcb\\x82\\xb7\\n\\x10M\\x8cIf<\\x0b\\xd5\\xec\\x1d\\\\\\xcbg\\x1a\\x97\\xa8\\xee\\x9b{\\xa6J\\x97\\xd8\\xd4As>|\\xa8q\\x1d\\x96\\xa2-K\\x92j)0\\x8aA s\\xf32\\x00\\r\\xbc\\xfe\\xdb\\xdb\\x0e\\xf9s\\xabK^\\xc9(\\x82#\\xf1[\\x00\\x97\\x89e\\x9bm\\xfd<\\xe8\\xa4\\xe7\\x1b\\xff\\xed\\xfa<\\xdc\\xe2\\xd3)&\\xcd\\x93K\\x07\\xe5\\'\\x1c8\\xd7\\xbfp\\xa2I^\\x97\\xd3\\x05\\xaek\\xb4e4\\xdd\\xe5\\xbfn\\x13\\xaaV&_\\x1b\\x11\\xb5\\x82\\x88\\x07\\xc8\\x1c\\x16V\\x82A\\x99\\x13K\\xfd\\xce\\x18\\xe5\\x85\\x93\\x17\\xb7\\xf5\\xe2\\x9f\\x08\\x91\\xe0_\\xb1y\\xe2Whq\\x9f<\\xbb\\x00\\xf6\\x9c!zf\\x89*\\xb8\\xa2/\\x85\\x92\\xd0\\xed\\x0f\\x93\\x0e\\x1f\\xce\\xc83\\x9a\\xe1\\xb8\\xf0z\\x1b\\xbe\\x05\\\\\\xa3\\x01\\x1f]\\x12i\\x94S\\x1e\\xc5\\x19\\x0c:,\\xed\\xadY\\xd3\\x88IZQ)[\\xf3\\x1a\\xc8\\xf9\\x8fk=\\x8c\\x19|\\x8c\\x9f\\xbf|\\x82Acq\\xe93#\\xb2\\xb6:f\\xdfa\\xf2\\xa6r\\xd2\\xe1U\\xbfL\\xb7\\xd0\\xa5\\xb0E\\xa0\\xa2\\x987R/\\x1b*\\xfaN\\xe6\\xbaw$\\x91n\\xc1\\xac\\x1e\\x08x\\xe7\\xa4k\\x008\\x95\\xef\\xe2i.T<p\\n\\xce\\x19,t\\x99Yr\\x86\\x88\\x183\\xf3\\xa4N\\x1f\\x8b\\x0cP\\x8a\\xb52)Dq\\r\\xd5\\xbcS\\xa7\\xc8; FF\\xa8!\\neX\\xcf\\xa7\\xff\\xc0\\xd2i\\xb8\\xc3\\xe7\\xa7W\\xa2\\xfd\\x06\\xe0s\\xba\\x16\\xe3\\xe7\\x12\\x9a6\\x0b9\\xe3\\xd4\\xb6{&\\xef\\xe5M\\x13\\xdc\\x00g`y\\xf1\\xc0\\x14\\x9b\\x1d\\x9a\\xf5\\x1a3\\x14\\x19\\xba\\xcb\\xa1\\x16~fu\\xee\\xbf\\xcf\\xfe\\xbd\\x84\\xa9_s\\xac3\\x08\\xc8c\\xbe\\xee\\xc8Cu\\xf4\\xba8\\xdeO\\xbf\\xef-?N\\xbe\\xdb\\xfc\\xb6\\x19\\x7f\\x84\\x05Tq\\x07\\xda\\xd6\\xefNg\\xa3\\xf5\\xa7&g\\x06\\xabi]\\xcb\\xc1\\x86\\xa5\\x9cL\\xc5K\\xbb\\x01\\xa1\\xb2\\xd7\\xbb\\xa0w1)\\x17oR\\x9aj8:c\\xa3\\x0e\\xd7\\x15+\\xdd\\x84-\\x18\\xf6Xc\\xcf\\xa4O,r\\xb8!\\xc2\\xe1<\\x16\\x88R*\\xa1\\x1e\\xaeu\\xa06o\\x0eC\\x16\\xf8\\xc4\\x83G\\x93W\\xbaer\\x8f\\n\\xfd\\x95\\xd5\\x9f\\xe1D\\x82\\xda\\xf4I\\tb|:6\\xf0\\xc1\\t\\x18\\xf9\\xe9/\\xa8\\x05S0\\x7f~*\\xc4\\xfe\\xe3\\xf0xz\\x9d\\xe6\\xb8:HW\\x9bW\\xea:g\\x82@\\xc1\\x91\\x08\\x9e\\xa1Q\\x1d\\x06\\xf8\\x97I\\x93i\\xe5b\\xefr\\\\-68K\\xeb\\xe0q?R\\x9f\\xac\\x8b\\x0b\\xbb(\\x1b\\x7f\\xfa\\xdd4\\xfe\\x84\\x17\\x89!\\x0bc\\xfd\\xdcK\\xdb\\x932\\xe2\\xc6\\x94\\xa9\\xce\\xf9K\\x9efIq\\xd2ck\\x13\\x83\\x88\\xb4LJ\\x9f\\x82\\x18\\xd2J@\\xd7\\x94\\xfb\\x82\\xfb\\xdfi\\xb2\\xc6\\xfd\\x1b\\xf8L G\\xc9\\xa1\\x81\\x10\\xb9;\\x84\\xf9\\xd4\\x9d\\x93\\xdcdb\\xc3\\n[\\x00\\xa67\\x08\\x89\\xcd\\xf9e\\xa8\\xa6\\xfd\\x04\\xb1\\xa3\\x0fh`\\xccv\\xb9\\x8d~\\xd2\\x1b_\\xc1\\xab\\x04\\x17\\x15z\\x85I\\xbc\\xb7\\xc7\\xd9<\\xaa~S`\\x82\\x9d7\\xb5\\x86\\xa5/\\x16\\xea\\x8d\\x9cueS;\\x86T5\\x9bZ\\xc3\\xba+\\xd7\\x17\\x14\\xe4w\\x80\\xca\\xb2p\\x1b.\\xfcLKi \\\\\\xca\\xde\\x11\\xcd\\x03\\xe5\\x14\\xd5\\x12H\\x13\\x06)\\x01\\xe81\\xc9\\xc9\\xa3h%\\x9d\\xc2f:\\x1am\\xe7\\x1f\\xc3\\xb0\\x16\\xdf\\xe1\\x05\\xb3\\x87\\x1e\\x1c\\xdf\\xfb\\x84\\x80<Y\\x00N{U\\xb6\\xf7\\x8e\\x1e\\xea\\xb4\\xf1\\xe1\\x08q\\xd48\\xb4\\xd2\\xb1\\x0b.\\x96\\xe7Q|B\\x03\\xa0\\x7f\\xaf\\xe3\\x10iW\\x839\\xe5\\x1f\\xb5\\xb5\\x9b\\xcc\\n\\xb6\\xce\\xb4\\xf5J\\xc1\\x03\\x84\\x81+`2\\xd8d\\xa4\\x83Q\\x9b8I\\x07\\xb5\\x11\\x8d\\xa2y\\xe6\\xc3\\xc16h|\\xcf]\\x8f\\x88\\x81\\xe4b\\x9f\\xef4P\\xa0^5N\\x92\\r\\xd4\\tw~\\xc4\\x9d\\xd0\\xe7\\x07N#\\xc0V\\xce\\x0bR\\xb6\\xf0K\\r\\xbcg\\\\F\\xbe\\xe5\\xae\\\\ [}\\x96\\xf9\\x15^\\xc9\\x80\\xb6s.&fDkUO\\t\\x9f\\x1e\\xb0\\xf2i\\x81\\n[\\x1fL\\xa8[\\xba\\xff\\xd2\\xad_[\\xc6\\xf4\\x16Q\\xbe$\\xd8\\xd96}\\xb7\\xa7\\xa9\\xe0\\xe4\\xc4\\xe6\\x02\\xd9=\\xc37\\xfe\\xda\\xe4U\\xeb?UJE\\xe3\\x9e$I\\x9fC\\x9c\\xe3j\\xc5RD+\\x1f\\x192\\x8f\\'\\xc0\\xd3\\xb1\\xe2\\x12\\xcb\\xa3\\x8f@\\xd9xk\\t-\\x8f\\x0c\\xb5\\t~\\xe0\\xb1\\xb4iX\\x7fm\\xe4\\xf0\\x1e\\xc5wB:\\xce53\\x1f\\x9el\\xa5\\xbf9\\xb8\\xf7\\xc1F\\xe9g\\xb2\\x14\\x81\\xaa\\x18\\xab\\xd1\\xf7\\xafv\\xfc\\x1aa\\x1dNZ\\xb8\\xceT5+\\x1fM\\x07\\x95\\xf4\\xe2\\xfdQt\\xafI]\\xeb\\xed\\xe4\\x13\\x14\\xb3CN\\x80\\x03J\\xa7\\x16Y\\x0b\\x04$X\\xf1\\x16&\\xa2\\xb1h\\xe8\\r\"\\x871\\xa2\\xfc\\xf9\\nx0\\x02p\\xa0\\xbf\\xa6@N\\x0eK\\xf2\\x06%yW\\x10\\xecS\\x1c\\xe7\\x7fR*j\\xdb\\x18\\x0e\\xe0c\\x82\\xd4\\xa0^\\xfeL\\xbb\\x16\\x92\\xe8\\xff\\xc4\\xd6@2\\xdb \\xb9}^\\xd9Q%\\x03r\\xcbo\\x08\\xb30\\x85$%.\\xb07\\xebc\\xc5q\\xb5\\x94\\x0cJ\\x93h\\xbb\\x0b\\x16:\\xdb\\xb9\\xf6\\xe6\\xc7q\\x03D\\xa0W\\xf6\\x0c\\xd7 \\x10\\tu\\xca\\xdeS\\xe6\\x9f8\\xe4Q\\xb4\\x84\\xaa\\x86\\xc3\\xedn2\\x84\\xf6-\\xf5S\\xa7\\xec\\x12\\xc3\\xed\\xeb\\x92\\xc7\\x9a\\xa7\\xd5(7\\xe5\\x89\\xcc\\xff\\xe4\\xadE\\x95~\\xe4\\x81/\\xb0\\xe60\"bg\\xef\\'\\xd3\\x8d\\t&o\\xc8\\x01i\\x0e\\xa8!S\\x1e\\x90i\\xe8\\x11\\x8a\\x86\\xf5\\x112UxA\\xe9\\xe5\\x0e#N\\xa4\\x9f\\x0b\\xe6\\x07\\\\s\\x9c\\x1c\\x08\\xe1\\x88\\xb3\\x0f\\xc6\\xb6\\x92\\x80\\xd9\\x82Xk\\xc1\\x06\\xe8\\x01*\\x9d\\x03\\xd5t\\x01\\xd6\\xc7\\xa6\\xe8e\\x14=\\xfd\\xcfn\\x1a\\xf4\\x8a/\\xca|\\x83\\x86\\x11/pD\\x1d\\xf6%(\\xf5_Q\\x1a\\x16\\xfc%b\\x83JK\\xe3_\\xa9\\xf3<\\xd1\\x19\\xba\\x11\\xbc\\xd6\\x88\\x04\\x96\\xddH\\xc9\\xb7\\xc9\\xe0\\xb4\\xd0q+S\\x83(\\x0f\\xe8\\xca\\x8ew\\xa7\\x85\\x17q!q+\\xe2\\x89\\x08^9H\\xbb\\xb6\\xf8*|\\xdb&\\xfd\\xb3\\x0f\\xeb\\x06]\\x12{B\\x8d\\x02G\\xbd%\\x93\\xd7\\xa5\\x8a\\x8ft\\x00J\\t\\x831\\xf4\\xcd\\x9f\\xe1\\xa9\\x7fN\\xad\\xc3\\xba\\xf5\\xa2\\x0f\\xff\\\\)m\\x8b:\\xc7\\x00v\\xd3*:a\\xc3\\xc8,\\x9cFV\\xfdY6\\x06\\x11\\xc28\\xb0\\x14\\xdd4\\xd8\\xed\\x04\\x87Fu\\xe8\\xe7\\xd6\\xb2\\xc7}\\xfc\\x15\\xf5\\x0f\\xca\\xc9\\xd8\\x17\"c\\xf5\\x03>\\xf6\\xbf\\x7fr\\xe8\\xb9\\x10b\\xd2\\xaf\\xf1#\\t0\\xc5mf\\xb5\\xba\\xae\\x19\\t\\x9e\\xfcU\\xaa\\x10O\\x0e\\xd5\\xfa\\x0e\\xa7\\x96\\x90\\r{\\xfe(\\x0e\\xd8?2\\x18\\xa9\\xf34\\xe1_*\\xda\\xe7\\xa4W#v\\xacv}\\xb3\\xc8\"R\\xe2\\x8f9\\x83\\xa7\\xd7\\x1f\\x19\\xdc\\xbe\"\\xe5\\xbe\\xeaQ\\x96\\xd4n\\xe9\\xf9\\xa5C9\\x1e\\x0e\\xf9\\xfcV\\xf4t<6\\xa7\\x9a]Y2\\xdc\\x1e_wh\\x96#~q\\xec\\xc4\\xcb\\xb5\\xc3:\\xb9X+=\\x06][\\x89\\x19\\x12\\xae\\x0c\\xc9QX\\x92y}\\xa8\\xc3Q\\xdf\\x87o\\x9dTK\\xc8L\\x1a&\\x84\\r\\x9a\\x80%\\xd6`-\\xc3\\x9c\\t2\\xa3\\x1a\\x0cUsLs\\x89\\xfa\\xe7\\xcc\\xd7\\xf1\\xbaa\\x1cG\\x8a\\xc1X\\xf5\\x06,\\x87Wv\\x84\\xaa7q\\xd0\\xc1^\\xcbaL\\x91\\x10\\x0c\\x06\\xd2e6J\\x8f\\x99\\xcc\\xc9\\xb3\\xd7\\x11\\xeb\\xb76g\\x06j\\xb6\\xd2\\x9e\\xbe\\xc9\\x86n\\x8e\\xf0\\xafo\\xf0\\xb1$\\xbb\\xe0F2Q\\x99\\xd9\\r\\xc3\\xeavI~]4\\xb8P\\xc9\\xf2\\xda/\\xf1\\xd1/\\x15\\xf6KD\\xbb\\xaaa\\xee\\x96\\x183\\x89\\x05IO\\xbc%\\xd9I\\xaa\\xab\\xe78\\xbaU\\xe2\\x03\\xb37\\x1a\\xb4\\x89R\\xabC\\x83\\xf9ps\\x85k\\xa6\\xda\\xef\\x00K\\xd1\\xc7\\xf7\\x06|\\x98\\xdfl\\x8a\\xc46\\xe6a\\\\\\xa0\\xe6Cd\\x9d\\x95\\x98\\xae\\x94XV1\\x8d\\xe0\\xcd\\xeb\\x12e\\xc4\\x00%\\xe6\\xbd\\xe3\\xc7\\xc2zn\\x80\\xb2b\\x88\\xf2\\xc7w!)\\xbeP\\x84>\\x00\\x80%\\xdb\\xb7A\\xb0#;\\x18\\xe7Xc]\\xbc\\xd7m\\xcaz\\xf4gp\\x0f\\xde\\xa00\\xfevs\\xfb=\\x1b\\xf3\\xaf\\xce\\xff\\x89\\x93K\\x11\\x1c\\xb3\\x9b\\xf1\\xba&\\xc1\\xa5D}\\xae\\xf4i\\xd7\\x04\\x89|\\x03\\x06Y\\xd0\\x02a\\x9ax\\xae\\xa6\\x9be\\x98\\xa6\\x06\\xc9\\x85\\xa4S\\xa9\\xfdoFEo\\xcb\\xf0\\xed\\xf5\\xecO\\x12\\xc8+\\xc7\\xba\\xdb\\xba\\xdeK6\\xf8\\x96\\xb2\\x9b\\xf2,\\xa2\\x8d6\\x10\\xfd\\x97\\xe2O\\x16\\xf2\\x93n\\x8fVr\\x85\\x10\\x91\\xdcjO\\xaf^\\\\)\\xaan.t-\\xe2\\x95\\x04A\\xee!gg\\x104\\'\\xb8\\x1e\\xda:\\x90\\xc3\\x1eZM\\x06,<\\xb2\\x87J^\\x02\\xc0\\xc3x%O\\xe8\\xd7_i\\xe8\\x12/\\xedFC\\x14\\x1b\\xd3:\\xad\\x03\\x94\\x05\\xb7i8\\x0e\\xf1\\xc2\\x1f!\\xfb\\xef\\xcf3G\\x91\\x19\\xd6\\'0\\xd8\\x82\\xa40!\\x83\\xbc*F!\\xa7\\x8d)l\\x9e\\xef\\xa1\\xa3\\x1c\\x8a\\xae\\xd7\\x87K\\'\\x862\\x90#[\\x943\\x90$\\xa7\\xd8a\\x0b\\xa5\\xf7\\\\\\xf7r\\x95\\x04\\xd26\\xd44\\x07\\x1a&2\\xbb5\\xf1d\\xdf\\x15\\xc1\\x87\\xcfM\\xa7\\xb8\\xb4\\xfb\\xdd\\xe3 \"\\xd7W\\xc4\\x84\\xb0\\xd7'"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.frombuffer(tmp_array[0], dtype=np.uint8) == udp_numpy[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([168,  83, 220, ..., 132, 176, 215], dtype=uint8)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.frombuffer(tmp_array[0], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_a = weights_dict['transformer.layers.0.0.fn.temperature'].tobytes()\n",
    "bit_array = np.frombuffer(tmp_a, dtype=np.uint8)\n",
    "if bit_array.shape[0] % 64 != 0:\n",
    "    padding = np.ones((64 - bit_array.shape[0] % 64), dtype=bit_array.dtype) * 255\n",
    "    bit_array = np.concatenate((bit_array, padding))\n",
    "bit_array = np.unpackbits(bit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bytes to numpy array of bool\n",
    "bit_array = np.frombuffer(tmp_a, dtype=np.uint8)\n",
    "bit_array = torch.from_numpy(np.unpackbits(bit_array))\n",
    "# bool_array = bit_array.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([130, 512])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit_array.view(-1,512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    bit_array = numpy_to_bit(weights_dict['pos_embedding'])\n",
    "    array_len = len(bit_array)\n",
    "\n",
    "    # Calculate the number of rows in the resulting matrix\n",
    "    num_rows = (array_len + 511) // 512  # Round up to cover all elements\n",
    "\n",
    "    # Create a matrix of size (num_rows, 512) initialized with ones (for padding)\n",
    "    matrix = np.ones((num_rows, 512), dtype=bit_array.dtype)\n",
    "\n",
    "    # Fill in the matrix with slices of bit_array\n",
    "    matrix[:array_len // 512, :] = bit_array[:array_len].reshape(-1, 512)\n",
    "\n",
    "    # For the last partial row, copy the remaining elements\n",
    "    remaining_elements = array_len % 512\n",
    "    if remaining_elements > 0:\n",
    "        matrix[array_len // 512, :remaining_elements] = bit_array[-remaining_elements:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recv_packet = {i:[] for i in range(num_nodes)}\n",
    "for tmp_id in range(num_nodes):\n",
    "    for i in range(len(udp_packet)):\n",
    "        tmp_packet = struct.pack('I',0) + udp_packet[i]\n",
    "        node_s[0].sendall(struct.pack('I',len(tmp_packet))+tmp_packet)\n",
    "        if (i+1) % 16 == 0:\n",
    "            try:\n",
    "                while True:\n",
    "                    server_r.settimeout(3)\n",
    "                    data = server_r.recv(len(tmp_packet))\n",
    "                    server_r.settimeout(0.5)\n",
    "                    node_id = struct.unpack('I',data[:4])[0]\n",
    "                    recv_packet[node_id].append(data[4:])\n",
    "                    # recv_packet.append(data)\n",
    "            except socket.timeout:\n",
    "                # print('Timeout')\n",
    "                # print(len(recv_packet[0]))\n",
    "                server_r.settimeout(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_array = packet_aggregation(udp_packet, c_1024, 8, info_ni, freeze_ni, codeword_idx, bit_array_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_dict = {}\n",
    "# restored_array = [torch.tensor(arr).to(device) for arr in restored_array]\n",
    "for i, name in enumerate(weights_dict):\n",
    "    restored_dict[name] = torch.tensor(restored_array[i].reshape(weights_dict[name].shape)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(restored_array)):\n",
    "    assert np.array_equal(restored_array[i], list(weights_dict.values())[i].flatten())\n",
    "# np.array_equal(restored_array[1], list(weights_dict.values())[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(restored_array)):\n",
    "    assert np.array_equal(restored_array[i], tmp_restore[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
