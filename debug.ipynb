{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lwh/.conda/envs/wireless/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import struct\n",
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torchvision.transforms import ToTensor\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "from numba import njit, jit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cython_decoder import cython_sc_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind((host, port))\n",
    "num_nodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 20\n",
    "server_socket.listen((num_nodes+1)*2)\n",
    "node_s = []\n",
    "node_r = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        client_socket, addr = server_socket.accept()\n",
    "        server_socket.settimeout(10)\n",
    "        data = client_socket.recv(1024).decode()\n",
    "        if data == \"Server-R\":\n",
    "            server_s = client_socket\n",
    "        elif data == \"Server-S\":\n",
    "            server_r = client_socket\n",
    "        elif data == \"Node-R\":\n",
    "            node_s.append(client_socket)\n",
    "        elif data == \"Node-S\":\n",
    "            node_r.append(client_socket)\n",
    "        client_socket.sendall(struct.pack('I',len(b\"start\"))+b\"start\")\n",
    "except socket.timeout:\n",
    "    print('Timeout')\n",
    "    server_socket.settimeout(None)\n",
    "\n",
    "for tmp_socket in node_r:\n",
    "    tmp_socket.recv(1024)\n",
    "server_r.recv(65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp_socket in node_r:\n",
    "    tmp_socket.close()\n",
    "for tmp_socket in node_s:\n",
    "    tmp_socket.close()\n",
    "server_s.close()\n",
    "server_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"cifar10\",\n",
    "    partitioners={\n",
    "        \"train\": DirichletPartitioner(\n",
    "            num_partitions=50,\n",
    "            partition_by=\"label\",\n",
    "            alpha=0.1,\n",
    "            seed=42,\n",
    "            min_partition_size=0,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "  transforms = transform_train\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "def test_transforms(batch):\n",
    "    transforms = transform_test\n",
    "    batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "train_loader=[]\n",
    "test_loader=[]\n",
    "for i in range(50):\n",
    "    partition_train_test = fds.load_partition(i, \"train\").train_test_split(0.1)\n",
    "    partition_train = partition_train_test[\"train\"].with_transform(train_transforms)\n",
    "    partition_test = partition_train_test[\"test\"].with_transform(test_transforms)\n",
    "    # centralized_dataset = fds.load_split(\"test\").with_transform(test_transforms)\n",
    "    train_loader.append(DataLoader(partition_train, batch_size=256, shuffle=True, num_workers=16))\n",
    "    test_loader.append(DataLoader(partition_test, batch_size=128, shuffle=False, num_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_small import ViT\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = []\n",
    "optimizer = []\n",
    "scheduler = []\n",
    "criterion = []\n",
    "scaler = []\n",
    "for i in range(num_nodes):\n",
    "    net.append(ViT(\n",
    "        image_size = 32,\n",
    "        patch_size = 4,\n",
    "        num_classes = 10,\n",
    "        dim = 32,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 32,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    ).to(device))\n",
    "\n",
    "\n",
    "    optimizer.append(optim.Adam(net[i].parameters(), lr=0.001))\n",
    "    scheduler.append(torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[i], 5))\n",
    "    criterion.append(nn.CrossEntropyLoss())\n",
    "    scaler.append(torch.cuda.amp.GradScaler(enabled=True))\n",
    "\n",
    "server_net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 32,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 32,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                criterion: nn.Module, \n",
    "                device: torch.device, \n",
    "                scaler: torch.cuda.amp.GradScaler, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                nodes: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Nodes: {nodes}, Epoch: {epoch},Train Loss: {total_loss / total_samples:.4f}, Train Accuracy: {total_correct / total_samples:.4f}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   test_loader: DataLoader, \n",
    "                   criterion: nn.Module, \n",
    "                   device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Validation Loss: {total_loss / total_samples:.4f}, Validation Accuracy: {total_correct / total_samples:.4f}\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 0, Epoch: 0,Train Loss: 0.0060, Train Accuracy: 0.4276\n",
      "Nodes: 0, Epoch: 1,Train Loss: 0.0060, Train Accuracy: 0.4252\n",
      "Nodes: 0, Epoch: 2,Train Loss: 0.0060, Train Accuracy: 0.4336\n",
      "Nodes: 0, Epoch: 3,Train Loss: 0.0060, Train Accuracy: 0.4216\n",
      "Nodes: 0, Epoch: 4,Train Loss: 0.0060, Train Accuracy: 0.4222\n",
      "Nodes: 0, Epoch: 5,Train Loss: 0.0059, Train Accuracy: 0.4216\n",
      "Nodes: 0, Epoch: 6,Train Loss: 0.0060, Train Accuracy: 0.4192\n",
      "Nodes: 0, Epoch: 7,Train Loss: 0.0060, Train Accuracy: 0.4216\n",
      "Nodes: 0, Epoch: 8,Train Loss: 0.0059, Train Accuracy: 0.4192\n",
      "Nodes: 0, Epoch: 9,Train Loss: 0.0058, Train Accuracy: 0.4312\n",
      "Nodes: 0, Epoch: 10,Train Loss: 0.0057, Train Accuracy: 0.4360\n",
      "Nodes: 0, Epoch: 11,Train Loss: 0.0057, Train Accuracy: 0.4312\n",
      "Nodes: 0, Epoch: 12,Train Loss: 0.0057, Train Accuracy: 0.4498\n",
      "Nodes: 0, Epoch: 13,Train Loss: 0.0056, Train Accuracy: 0.4547\n",
      "Nodes: 0, Epoch: 14,Train Loss: 0.0055, Train Accuracy: 0.4703\n",
      "Nodes: 0, Epoch: 15,Train Loss: 0.0054, Train Accuracy: 0.4661\n",
      "Nodes: 0, Epoch: 16,Train Loss: 0.0054, Train Accuracy: 0.4787\n",
      "Nodes: 0, Epoch: 17,Train Loss: 0.0054, Train Accuracy: 0.4883\n",
      "Nodes: 0, Epoch: 18,Train Loss: 0.0053, Train Accuracy: 0.5039\n",
      "Nodes: 0, Epoch: 19,Train Loss: 0.0052, Train Accuracy: 0.5045\n",
      "Nodes: 0, Epoch: 20,Train Loss: 0.0051, Train Accuracy: 0.5009\n",
      "Nodes: 0, Epoch: 21,Train Loss: 0.0050, Train Accuracy: 0.5363\n",
      "Nodes: 0, Epoch: 22,Train Loss: 0.0048, Train Accuracy: 0.5315\n",
      "Nodes: 0, Epoch: 23,Train Loss: 0.0048, Train Accuracy: 0.5483\n",
      "Nodes: 0, Epoch: 24,Train Loss: 0.0047, Train Accuracy: 0.5538\n",
      "Nodes: 0, Epoch: 25,Train Loss: 0.0047, Train Accuracy: 0.5544\n",
      "Nodes: 0, Epoch: 26,Train Loss: 0.0047, Train Accuracy: 0.5532\n",
      "Nodes: 0, Epoch: 27,Train Loss: 0.0047, Train Accuracy: 0.5477\n",
      "Nodes: 0, Epoch: 28,Train Loss: 0.0047, Train Accuracy: 0.5628\n",
      "Nodes: 0, Epoch: 29,Train Loss: 0.0047, Train Accuracy: 0.5447\n",
      "Nodes: 0, Epoch: 30,Train Loss: 0.0047, Train Accuracy: 0.5369\n",
      "Nodes: 0, Epoch: 31,Train Loss: 0.0045, Train Accuracy: 0.5610\n",
      "Nodes: 0, Epoch: 32,Train Loss: 0.0045, Train Accuracy: 0.5676\n",
      "Nodes: 0, Epoch: 33,Train Loss: 0.0044, Train Accuracy: 0.5688\n",
      "Nodes: 0, Epoch: 34,Train Loss: 0.0044, Train Accuracy: 0.5976\n",
      "Nodes: 0, Epoch: 35,Train Loss: 0.0044, Train Accuracy: 0.5706\n",
      "Nodes: 0, Epoch: 36,Train Loss: 0.0044, Train Accuracy: 0.5736\n",
      "Nodes: 0, Epoch: 37,Train Loss: 0.0043, Train Accuracy: 0.5904\n",
      "Nodes: 0, Epoch: 38,Train Loss: 0.0043, Train Accuracy: 0.5994\n",
      "Nodes: 0, Epoch: 39,Train Loss: 0.0043, Train Accuracy: 0.5862\n",
      "Nodes: 0, Epoch: 40,Train Loss: 0.0045, Train Accuracy: 0.5538\n",
      "Nodes: 0, Epoch: 41,Train Loss: 0.0045, Train Accuracy: 0.5742\n",
      "Nodes: 0, Epoch: 42,Train Loss: 0.0043, Train Accuracy: 0.5820\n",
      "Nodes: 0, Epoch: 43,Train Loss: 0.0042, Train Accuracy: 0.6060\n",
      "Nodes: 0, Epoch: 44,Train Loss: 0.0041, Train Accuracy: 0.6150\n",
      "Nodes: 0, Epoch: 45,Train Loss: 0.0041, Train Accuracy: 0.6168\n",
      "Nodes: 0, Epoch: 46,Train Loss: 0.0042, Train Accuracy: 0.5934\n",
      "Nodes: 0, Epoch: 47,Train Loss: 0.0041, Train Accuracy: 0.6150\n",
      "Nodes: 0, Epoch: 48,Train Loss: 0.0042, Train Accuracy: 0.5952\n",
      "Nodes: 0, Epoch: 49,Train Loss: 0.0042, Train Accuracy: 0.6048\n",
      "Nodes: 0, Epoch: 50,Train Loss: 0.0042, Train Accuracy: 0.5940\n",
      "Nodes: 0, Epoch: 51,Train Loss: 0.0041, Train Accuracy: 0.6090\n",
      "Nodes: 0, Epoch: 52,Train Loss: 0.0041, Train Accuracy: 0.6132\n",
      "Nodes: 0, Epoch: 53,Train Loss: 0.0040, Train Accuracy: 0.6036\n",
      "Nodes: 0, Epoch: 54,Train Loss: 0.0040, Train Accuracy: 0.6066\n",
      "Nodes: 0, Epoch: 55,Train Loss: 0.0039, Train Accuracy: 0.6234\n",
      "Nodes: 0, Epoch: 56,Train Loss: 0.0039, Train Accuracy: 0.6186\n",
      "Nodes: 0, Epoch: 57,Train Loss: 0.0041, Train Accuracy: 0.6114\n",
      "Nodes: 0, Epoch: 58,Train Loss: 0.0039, Train Accuracy: 0.6222\n",
      "Nodes: 0, Epoch: 59,Train Loss: 0.0041, Train Accuracy: 0.6096\n",
      "Nodes: 0, Epoch: 60,Train Loss: 0.0042, Train Accuracy: 0.6054\n",
      "Nodes: 0, Epoch: 61,Train Loss: 0.0041, Train Accuracy: 0.6042\n",
      "Nodes: 0, Epoch: 62,Train Loss: 0.0040, Train Accuracy: 0.6078\n",
      "Nodes: 0, Epoch: 63,Train Loss: 0.0037, Train Accuracy: 0.6468\n",
      "Nodes: 0, Epoch: 64,Train Loss: 0.0038, Train Accuracy: 0.6258\n",
      "Nodes: 0, Epoch: 65,Train Loss: 0.0038, Train Accuracy: 0.6396\n",
      "Nodes: 0, Epoch: 66,Train Loss: 0.0038, Train Accuracy: 0.6336\n",
      "Nodes: 0, Epoch: 67,Train Loss: 0.0039, Train Accuracy: 0.6228\n",
      "Nodes: 0, Epoch: 68,Train Loss: 0.0038, Train Accuracy: 0.6330\n",
      "Nodes: 0, Epoch: 69,Train Loss: 0.0038, Train Accuracy: 0.6348\n",
      "Nodes: 0, Epoch: 70,Train Loss: 0.0038, Train Accuracy: 0.6258\n",
      "Nodes: 0, Epoch: 71,Train Loss: 0.0038, Train Accuracy: 0.6276\n",
      "Nodes: 0, Epoch: 72,Train Loss: 0.0038, Train Accuracy: 0.6348\n",
      "Nodes: 0, Epoch: 73,Train Loss: 0.0038, Train Accuracy: 0.6318\n",
      "Nodes: 0, Epoch: 74,Train Loss: 0.0036, Train Accuracy: 0.6480\n",
      "Nodes: 0, Epoch: 75,Train Loss: 0.0037, Train Accuracy: 0.6474\n",
      "Nodes: 0, Epoch: 76,Train Loss: 0.0037, Train Accuracy: 0.6360\n",
      "Nodes: 0, Epoch: 77,Train Loss: 0.0036, Train Accuracy: 0.6577\n",
      "Nodes: 0, Epoch: 78,Train Loss: 0.0036, Train Accuracy: 0.6384\n",
      "Nodes: 0, Epoch: 79,Train Loss: 0.0038, Train Accuracy: 0.6306\n",
      "Nodes: 0, Epoch: 80,Train Loss: 0.0038, Train Accuracy: 0.6294\n",
      "Nodes: 0, Epoch: 81,Train Loss: 0.0037, Train Accuracy: 0.6402\n",
      "Nodes: 0, Epoch: 82,Train Loss: 0.0038, Train Accuracy: 0.6090\n",
      "Nodes: 0, Epoch: 83,Train Loss: 0.0036, Train Accuracy: 0.6492\n",
      "Nodes: 0, Epoch: 84,Train Loss: 0.0035, Train Accuracy: 0.6541\n",
      "Nodes: 0, Epoch: 85,Train Loss: 0.0036, Train Accuracy: 0.6607\n",
      "Nodes: 0, Epoch: 86,Train Loss: 0.0035, Train Accuracy: 0.6757\n",
      "Nodes: 0, Epoch: 87,Train Loss: 0.0035, Train Accuracy: 0.6619\n",
      "Nodes: 0, Epoch: 88,Train Loss: 0.0035, Train Accuracy: 0.6601\n",
      "Nodes: 0, Epoch: 89,Train Loss: 0.0035, Train Accuracy: 0.6583\n",
      "Nodes: 0, Epoch: 90,Train Loss: 0.0036, Train Accuracy: 0.6589\n",
      "Nodes: 0, Epoch: 91,Train Loss: 0.0035, Train Accuracy: 0.6673\n",
      "Nodes: 0, Epoch: 92,Train Loss: 0.0035, Train Accuracy: 0.6553\n",
      "Nodes: 0, Epoch: 93,Train Loss: 0.0035, Train Accuracy: 0.6715\n",
      "Nodes: 0, Epoch: 94,Train Loss: 0.0035, Train Accuracy: 0.6661\n",
      "Nodes: 0, Epoch: 95,Train Loss: 0.0034, Train Accuracy: 0.6673\n",
      "Nodes: 0, Epoch: 96,Train Loss: 0.0034, Train Accuracy: 0.6775\n",
      "Nodes: 0, Epoch: 97,Train Loss: 0.0034, Train Accuracy: 0.6805\n",
      "Nodes: 0, Epoch: 98,Train Loss: 0.0033, Train Accuracy: 0.6835\n",
      "Nodes: 0, Epoch: 99,Train Loss: 0.0035, Train Accuracy: 0.6613\n",
      "Time taken: 66.4124345779419\n"
     ]
    }
   ],
   "source": [
    "for cli in range(0,1):\n",
    "    start_time = time.time()\n",
    "    for i in range(100):\n",
    "        train_model(net[cli], train_loader[cli], criterion[cli], device, scaler[cli], optimizer[cli], i, cli)\n",
    "        # evaluate_model(net[cli], test_loader[cli], criterion[cli], device)\n",
    "        scheduler[cli].step()\n",
    "    print(f\"Time taken: {time.time()-start_time}\")\n",
    "    scheduler[cli] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[cli], 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_embedding',\n",
       " 'cls_token',\n",
       " 'to_patch_embedding.to_patch_tokens.1.weight',\n",
       " 'to_patch_embedding.to_patch_tokens.1.bias',\n",
       " 'to_patch_embedding.to_patch_tokens.2.weight',\n",
       " 'to_patch_embedding.to_patch_tokens.2.bias',\n",
       " 'transformer.layers.0.0.norm.weight',\n",
       " 'transformer.layers.0.0.norm.bias',\n",
       " 'transformer.layers.0.0.fn.temperature',\n",
       " 'transformer.layers.0.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.0.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.0.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.0.1.norm.weight',\n",
       " 'transformer.layers.0.1.norm.bias',\n",
       " 'transformer.layers.0.1.fn.net.0.weight',\n",
       " 'transformer.layers.0.1.fn.net.0.bias',\n",
       " 'transformer.layers.0.1.fn.net.3.weight',\n",
       " 'transformer.layers.0.1.fn.net.3.bias',\n",
       " 'transformer.layers.1.0.norm.weight',\n",
       " 'transformer.layers.1.0.norm.bias',\n",
       " 'transformer.layers.1.0.fn.temperature',\n",
       " 'transformer.layers.1.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.1.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.1.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.1.1.norm.weight',\n",
       " 'transformer.layers.1.1.norm.bias',\n",
       " 'transformer.layers.1.1.fn.net.0.weight',\n",
       " 'transformer.layers.1.1.fn.net.0.bias',\n",
       " 'transformer.layers.1.1.fn.net.3.weight',\n",
       " 'transformer.layers.1.1.fn.net.3.bias',\n",
       " 'transformer.layers.2.0.norm.weight',\n",
       " 'transformer.layers.2.0.norm.bias',\n",
       " 'transformer.layers.2.0.fn.temperature',\n",
       " 'transformer.layers.2.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.2.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.2.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.2.1.norm.weight',\n",
       " 'transformer.layers.2.1.norm.bias',\n",
       " 'transformer.layers.2.1.fn.net.0.weight',\n",
       " 'transformer.layers.2.1.fn.net.0.bias',\n",
       " 'transformer.layers.2.1.fn.net.3.weight',\n",
       " 'transformer.layers.2.1.fn.net.3.bias',\n",
       " 'transformer.layers.3.0.norm.weight',\n",
       " 'transformer.layers.3.0.norm.bias',\n",
       " 'transformer.layers.3.0.fn.temperature',\n",
       " 'transformer.layers.3.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.3.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.3.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.3.1.norm.weight',\n",
       " 'transformer.layers.3.1.norm.bias',\n",
       " 'transformer.layers.3.1.fn.net.0.weight',\n",
       " 'transformer.layers.3.1.fn.net.0.bias',\n",
       " 'transformer.layers.3.1.fn.net.3.weight',\n",
       " 'transformer.layers.3.1.fn.net.3.bias',\n",
       " 'transformer.layers.4.0.norm.weight',\n",
       " 'transformer.layers.4.0.norm.bias',\n",
       " 'transformer.layers.4.0.fn.temperature',\n",
       " 'transformer.layers.4.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.4.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.4.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.4.1.norm.weight',\n",
       " 'transformer.layers.4.1.norm.bias',\n",
       " 'transformer.layers.4.1.fn.net.0.weight',\n",
       " 'transformer.layers.4.1.fn.net.0.bias',\n",
       " 'transformer.layers.4.1.fn.net.3.weight',\n",
       " 'transformer.layers.4.1.fn.net.3.bias',\n",
       " 'transformer.layers.5.0.norm.weight',\n",
       " 'transformer.layers.5.0.norm.bias',\n",
       " 'transformer.layers.5.0.fn.temperature',\n",
       " 'transformer.layers.5.0.fn.to_qkv.weight',\n",
       " 'transformer.layers.5.0.fn.to_out.0.weight',\n",
       " 'transformer.layers.5.0.fn.to_out.0.bias',\n",
       " 'transformer.layers.5.1.norm.weight',\n",
       " 'transformer.layers.5.1.norm.bias',\n",
       " 'transformer.layers.5.1.fn.net.0.weight',\n",
       " 'transformer.layers.5.1.fn.net.0.bias',\n",
       " 'transformer.layers.5.1.fn.net.3.weight',\n",
       " 'transformer.layers.5.1.fn.net.3.bias',\n",
       " 'mlp_head.0.weight',\n",
       " 'mlp_head.0.bias',\n",
       " 'mlp_head.1.weight',\n",
       " 'mlp_head.1.bias']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(weights_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_bit(array: np.ndarray) -> np.ndarray:\n",
    "    tmp_byte=np.frombuffer(array.tobytes(),dtype=np.uint8)\n",
    "    bit_stream = ''.join(format(byte, '08b') for byte in tmp_byte)\n",
    "    bit_array = np.array([int(bit) for bit in bit_stream], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def bit_to_numpy(bit_array: np.ndarray) -> np.ndarray:\n",
    "    bit_stream = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    byte_array_back = np.array([int(bit_stream[i:i+8], 2) for i in range(0, len(bit_stream), 8)], dtype=np.uint8)\n",
    "    int_array_back = np.frombuffer(byte_array_back.tobytes(), dtype=np.float32)\n",
    "    return int_array_back\n",
    "\n",
    "'''\n",
    "Encoding and decoding function\n",
    "'''\n",
    "@jit(nopython=True)\n",
    "def encode(u: np.ndarray) -> np.ndarray:\n",
    "    N = u.shape[0]  # Get the length of u\n",
    "    n = int(np.log2(N))  # Calculate the log base 2 of N\n",
    "\n",
    "    if n == 1:\n",
    "        x = np.array([(u[0] + u[1]) % 2, u[1]],dtype=np.int8)\n",
    "        return x\n",
    "    else:\n",
    "        x1 = encode(np.mod(u[:N//2] + u[N//2:], 2))\n",
    "        x2 = encode(u[N//2:])\n",
    "        x = np.concatenate((x1, x2))\n",
    "        return x\n",
    "\n",
    "@jit(nopython=True)\n",
    "def rvsl(y: np.ndarray) -> np.ndarray:\n",
    "    N = y.shape[0]\n",
    "    if N == 2:\n",
    "        return y\n",
    "    else:\n",
    "        return np.concatenate((rvsl(y[0:N:2]), rvsl(y[1:N:2])))\n",
    "\n",
    "def data_process(array):\n",
    "    bit_array = numpy_to_bit(array)\n",
    "    array_len = len(bit_array)\n",
    "    current_array = []\n",
    "    for i in range(0, array_len, 512):\n",
    "        sub_array = bit_array[i:i+512]\n",
    "        if len(sub_array) < 512:\n",
    "            padding = np.ones((512 - len(sub_array)), dtype=bit_array.dtype)\n",
    "            sub_array = np.concatenate((sub_array, padding))\n",
    "        current_array.append(sub_array)\n",
    "    current_idx = i // 512 + 1\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def numpy_array_to_udp_packet(bit_array):\n",
    "    # Convert the numpy array of bits to a string of bits\n",
    "    bit_string = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    # Convert the bit string to bytes\n",
    "    byte_array = bytearray(int(bit_string[i:i+8], 2) for i in range(0, len(bit_string), 8))\n",
    "    return byte_array\n",
    "\n",
    "def udp_packet_to_numpy_array(packet):\n",
    "    # Convert the byte array back to a bit string\n",
    "    bit_string = ''.join(format(byte, '08b') for byte in packet)\n",
    "    # Convert the bit string to a numpy array of floats\n",
    "    bit_array = np.array([int(bit) for bit in bit_string], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def data_generate(bit_array:np.ndarray, data_idx:np.ndarray) -> np.ndarray:\n",
    "    u=np.zeros(1024,dtype=np.int8)\n",
    "    u[data_idx] = bit_array\n",
    "    x = encode(u)\n",
    "    x = rvsl(x)\n",
    "    return x\n",
    "\n",
    "def codeword_generate(array_dict, data_idx):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "    array_process = partial(data_process)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for current_array, current_idx, array_len in executor.map(array_process, [array_dict[name] for name in array_dict]):\n",
    "            split_bit.extend(current_array)\n",
    "            codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "            bit_array_len.append(array_len)\n",
    "    executor.shutdown(wait=True)\n",
    "    del executor, array_process\n",
    "    time1 = time.time()\n",
    "    encode_partial = partial(data_generate, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        codeword = list(executor.map(encode_partial, split_bit))\n",
    "    executor.shutdown(wait=True)\n",
    "    time2 = time.time()\n",
    "    print(f'Encode time: {time2-time1}')\n",
    "\n",
    "    del executor, encode_partial\n",
    "    return np.array(codeword,dtype=np.int8), codeword_idx, bit_array_len\n",
    "\n",
    "def packet_diffusion(codeword, block_len, packet_idx):\n",
    "    codeword_len = codeword[0].shape[0]\n",
    "    udp_packet = []\n",
    "    for idx, i in enumerate(range(0, codeword_len, block_len)):\n",
    "        # tmp_packet = np.concatenate([tmp_codeword[i:i+block_len] for tmp_codeword in codeword])\n",
    "        tmp_packet = codeword[:, packet_idx[i:i+block_len]].flatten()\n",
    "        tmp_udp_packet = struct.pack(\"I\",idx) + numpy_array_to_udp_packet(tmp_packet)\n",
    "        udp_packet.append(tmp_udp_packet)\n",
    "    return udp_packet\n",
    "\n",
    "def encoder_udp(array_dict, data_idx, block_len, packet_idx):\n",
    "    codeword, codeword_idx, bit_array_len = codeword_generate(array_dict, data_idx)\n",
    "    udp_packet = packet_diffusion(codeword, block_len, packet_idx)\n",
    "    return udp_packet, codeword_idx, bit_array_len\n",
    "\n",
    "\n",
    "\n",
    "def packet_aggregation(udp_packet, packet_idx, block_len, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    sort_idx = [struct.unpack(\"I\", tmp_packet[:4])[0] for tmp_packet in udp_packet]\n",
    "    packet_data_del = np.array([udp_packet_to_numpy_array(tmp_packet[4:]) for _, tmp_packet in sorted(zip(sort_idx, udp_packet))])\n",
    "    packet_data = np.ones((int(1024/block_len), len(packet_data_del[0])))*0.5\n",
    "    for i, tmp_idx in enumerate(sorted(sort_idx)):\n",
    "        packet_data[tmp_idx] = packet_data_del[i]\n",
    "    \n",
    "    restore_codeword = []\n",
    "    inverse_packet_idx = np.argsort(packet_idx)\n",
    "    for i in range(0, packet_data.shape[1],block_len):\n",
    "        tmp_codeword = packet_data[:,i:i+block_len].flatten()\n",
    "        restore_codeword.append(tmp_codeword[inverse_packet_idx])\n",
    "\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        restore_array.append(bit_to_numpy(tmp_array))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "def decoding(bit_array, freeze_idx, data_idx):\n",
    "    # Prepare the necessary arrays and values\n",
    "    bit_array = 1-2*bit_array\n",
    "    lr0 = np.exp(-(bit_array - 1)**2)\n",
    "    lr1 = np.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "    delete_num = 1024 - len(bit_array)\n",
    "    hd_dec = np.zeros(1024, dtype=np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    pro_prun = np.zeros((1, 2 * 1024 + 1), dtype=np.float64)\n",
    "\n",
    "    # Call the optimized Cython function\n",
    "    i_scen_sum, hd_dec_result = cython_sc_decoding(\n",
    "        lr0_post, lr1_post, freeze_idx.astype(np.float64),\n",
    "        hd_dec, 1024, 10, 512, frozen_val, delete_num, 0, pro_prun\n",
    "    )\n",
    "\n",
    "    # Extract the output for data_idx from hd_dec_result\n",
    "    data_out = hd_dec_result[data_idx]\n",
    "    return data_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_array = np.random.randint(2, size=(1024,))\n",
    "bool_array = np.array(random_array, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.array([[1,0],[1,1]], dtype=bool)\n",
    "n = int(np.log2(1024))\n",
    "g_n = f\n",
    "for _ in range(n-1):\n",
    "    f = np.kron(f, g_n)\n",
    "range_n = rvsl(np.arange(1024))\n",
    "f = f[:,range_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(random_array, f) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(rvsl(encode(random_array)) == np.dot(random_array, f) % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 7.239225625991821\n"
     ]
    }
   ],
   "source": [
    "# Export all weights to numpy arrays\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}\n",
    "N = 1024\n",
    "n = 10\n",
    "rate = 0.5\n",
    "K = round(N*rate)\n",
    "c_1024 = np.load('c_1024.npy')\n",
    "coding_list = scipy.io.loadmat(\"1024-3db-d=2-mean.mat\")[\"count_number\"]\n",
    "coding_index = np.argsort(coding_list[:,1])\n",
    "info_idx = coding_index[:K]\n",
    "freeze_idx = coding_index[K:]\n",
    "\n",
    "# sort the final index\n",
    "info_ni = np.sort(info_idx)\n",
    "freeze_ni = np.sort(freeze_idx)\n",
    "\n",
    "udp_packet, codeword_idx, bit_array_len = encoder_udp(weights_dict, info_ni, 8, c_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 0.00016832351684570312\n",
      "(26103, 1024)\n"
     ]
    }
   ],
   "source": [
    "def get_gn(length:int) -> np.ndarray:\n",
    "    n = int(np.log2(length))\n",
    "    f=np.array([[1,0],[1,1]], dtype=bool)\n",
    "    g_n = f\n",
    "    for _ in range(n-1):\n",
    "        g_n = np.kron(g_n, f)\n",
    "    g_order = rvsl(np.arange(length))\n",
    "    g_n = g_n[:, g_order]\n",
    "    return g_n\n",
    "\n",
    "def matrix_process(array, data_idx):\n",
    "    bit_array = np.frombuffer(array.tobytes(), dtype=np.uint8)\n",
    "    array_len = bit_array.shape[0] * 8\n",
    "    if bit_array.shape[0] % 64 != 0:\n",
    "        padding = 255 * np.ones((64 - bit_array.shape[0] % 64), dtype=bit_array.dtype)\n",
    "        bit_array = np.concatenate((bit_array, padding))\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array.reshape(-1, 512)\n",
    "    current_array = np.zeros((bit_array.shape[0], 1024), dtype=np.uint8)\n",
    "    current_array[:, data_idx] = bit_array\n",
    "    current_idx = bit_array.shape[0]\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def matrix_packet_diffusion(codeword):\n",
    "    if codeword.shape[0] % 8 != 0:\n",
    "        padding = np.ones((8 - codeword.shape[0] % 8, 1024), dtype=codeword.dtype)\n",
    "        udp_numpy = np.concatenate((codeword, padding)).T\n",
    "    udp_numpy = np.packbits(udp_numpy.flatten()).reshape(1024,-1)\n",
    "    udp_packet = [struct.pack(\"I\", 0) + struct.pack(\"I\", idx) + udp_numpy[idx].tobytes() for idx in range(udp_numpy.shape[0])]\n",
    "    return udp_packet\n",
    "    \n",
    "\n",
    "def matrix_encode(array_dict, data_idx):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "\n",
    "    for name in array_dict:\n",
    "        current_array, current_idx, array_len = matrix_process(array_dict[name], data_idx)\n",
    "        split_bit.extend(current_array)\n",
    "        codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "        bit_array_len.append(array_len)\n",
    "    split_bit = torch.tensor(np.array(split_bit), dtype=torch.float32).to(\"cuda:0\")\n",
    "    \n",
    "    f=np.array([[1,0],[1,1]], dtype=np.int8)\n",
    "    n = int(np.log2(1024))\n",
    "    g_n = f\n",
    "    \n",
    "    for _ in range(n-1):\n",
    "        f = np.kron(g_n,f)\n",
    "    g_order = rvsl(np.arange(1024))\n",
    "    f = f[:,g_order]\n",
    "    f = torch.tensor(f, dtype=torch.float32).to(\"cuda:0\")\n",
    "    \n",
    "    time1 = time.time()\n",
    "    # codeword = np.matmul(split_bit, f) % 2\n",
    "    tmp_codeword = torch.matmul(split_bit, f) % 2\n",
    "    time2 = time.time()\n",
    "    codeword = np.array(tmp_codeword.detach().cpu().numpy(), dtype=np.int8)\n",
    "    print(f'Encode time: {time2-time1}')\n",
    "    print(codeword.shape)\n",
    "    return codeword, codeword_idx, bit_array_len\n",
    "\n",
    "def matrix_udp(array_dict, data_idx):\n",
    "    codeword, codeword_idx, bit_array_len = matrix_encode(array_dict, data_idx)\n",
    "    udp_packet = matrix_packet_diffusion(codeword)\n",
    "    return udp_packet, codeword_idx, bit_array_len, codeword.shape[0]\n",
    "\n",
    "def matrix_udp_numpy(packet, codeword_num):\n",
    "    bit_array = np.frombuffer(packet, dtype=np.uint8)\n",
    "    bit_array = np.unpackbits(bit_array)\n",
    "    bit_array = bit_array[:codeword_num]\n",
    "    return bit_array\n",
    "\n",
    "def matrix_packet_aggregation(udp_packet, codeword_num, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    # sort_idx = [struct.unpack(\"I\", tmp_packet[:4])[0] for tmp_packet in udp_packet]\n",
    "    # packet_data_del = np.array([matrix_udp_numpy(tmp_packet[4:], codeword_num) for _, tmp_packet in sorted(zip(sort_idx, udp_packet))])\n",
    "    udp_idx = []\n",
    "    packet_del = []\n",
    "    for tmp_packet in udp_packet:\n",
    "        udp_idx.append(struct.unpack(\"I\", tmp_packet[:4])[0])\n",
    "        packet_del.append(matrix_udp_numpy(tmp_packet[4:], codeword_num))\n",
    "    packet_del = np.array(packet_del)\n",
    "    packet_data = np.ones((1024, codeword_num)) * 0.5\n",
    "    packet_data[udp_idx] = packet_del\n",
    "\n",
    "    restore_codeword = packet_data.T\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        bit_array = np.packbits(tmp_array)\n",
    "        restore_array.append(np.frombuffer(bit_array.tobytes(), dtype=np.float32))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "# tmp_a, tmp_b, tmp_c = matrix_encode(weights_dict, info_ni)\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}\n",
    "# para_list = [\"mlp_head.0.weight\", \"mlp_head.0.bias\", \"mlp_head.1.weight\", \"mlp_head.1.bias\"]\n",
    "# para_dict = {name: weights_dict[name] for name in para_list}\n",
    "# bn_dict = {name: weights_dict[name] for name in weights_dict if \"norm\" not in name}\n",
    "\n",
    "tmp_a, tmp_b, tmp_c, tmp_d = matrix_udp(weights_dict, info_ni)\n",
    "tmp_restore = matrix_packet_aggregation(tmp_a, tmp_d, info_ni, freeze_ni, tmp_b, tmp_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3265"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 1628.0625 KB\n",
      "Transfer Size: 1628.515625 KB\n",
      "Ferp Size: 3265.0 KB\n"
     ]
    }
   ],
   "source": [
    "total_size = 0\n",
    "transfer_size = 0\n",
    "ferp_size = 0\n",
    "for name in bn_dict:\n",
    "    total_size += bn_dict[name].nbytes\n",
    "    transfer_size += bn_dict[name].nbytes + 4 + 4\n",
    "for i in tmp_a:\n",
    "    ferp_size += len(i)\n",
    "\n",
    "print(f'Total Size: {total_size/1024} KB')\n",
    "print(f'Transfer Size: {transfer_size/1024} KB')\n",
    "print(f'Ferp Size: {ferp_size/1024} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dict['cls_token'].nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_i = np.ones((1024,1000)) * 0.5\n",
    "b_i = np.zeros((3, 1000))\n",
    "b_idx = [3,6,9]\n",
    "a_i[b_idx] = b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_i[b_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26136"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_a[0]*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode time: 6.217371940612793\n"
     ]
    }
   ],
   "source": [
    "tmp_d, tmp_e, tmp_f = codeword_generate(weights_dict, info_ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_c == tmp_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp_a.shape[0] % 8 != 0:\n",
    "    padding = np.ones((8 - tmp_a.shape[0] % 8, 1024), dtype=tmp_a.dtype)\n",
    "    udp_numpy = np.concatenate((tmp_a, padding)).T\n",
    "udp_numpy = np.packbits(udp_numpy.flatten()).reshape(1024,-1)\n",
    "tmp_array = [udp_numpy[i].tobytes() for i in range(udp_numpy.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xa8S\\xdc\\xc0q5\\xd8Z\\xd9\\xea\\x12z\\x820\"\\xd4\\xd5\\xe0\\x95s\\xfd(\\x7f\\xb3\\'\\xa5w\\xd4K\\xa4\\xa7\\xfd\\x07bN\\xa9\\xfe\\x0e\\xa2\\xf6G\\xb1\\x9b~N\\xdf&D\\xc3\\xde\\x86\\xdf\\xbcJ\\xed@]WNV\\x1dO\\xd5\\x9d\\xaa\\x13\\xe5nzQQ\\x91V\\xbaq\\x9b\\xcd\\xa7U\\xbe\\xc6\\x16\\x1f^\\xaeg\\xaf\\xcb\\xa2t,\\x08\\x15e\\xab\\xa1\\x80H\\x84u\\x80\\xe6\\xde\\xe8\\xed\\xc4\\xa9\\x94\\x97\\xa6\\x8c\\xf6\\xc0\\x98\\xe5_\\xaf\\xf3\\xe3\\xb3x\\xb7\\xc8\\x14\\xc13\\xf8e\\xa2x\\xfc\\x8be\\xa0G\\xb48\\x9aCN^r\\xcc\\x13\\x8f\\xdd\\x8c\\xbbp/\\xc7R\\xc4<0q\\x95\\x00*\\xbf+\\x8dJ]\\xaf\\x13<cJ\\x04jP\\xca5f\\xdb\\xb6\\x96\\x17-n2QVlU9\\xee\\x85X\\xb7\\x16\\xb3G\\xbf\\xfb\\x7f\\xfdF9\\x8b\\xd2w\\xdd\\x1d\\xe6\\x08:G\\x11\\xe1\\xbe\\x8f~\\xca\\x84rO\\xc1\\xc4\\x96\\xf7\\xab+;.\\x8czM~{I{\\xec\\xb0\\xe6\\xb4_\\xca#\\x046\\xbd\\x9a\\xa7_\\x193\\xfd\\x04k\\x10P\\\\\\x80:\\xc1=bN\\x0by\\xc8\\xc2\\xfcA\\xa3\\xce\\xc0\\xd5\\x1f\\x81\\x82#\\x95\\x839&\\xad\\x83P\\x89p-V\\x88\\x8d\\xce@\\x02\\x81\\xb2{}=\\xf4\\xd0\\x7f\\xbc\\xdc\\x8ce\\xad\\xb9vL\\x80g\\xa4Q\\nu\\xe0\\x7f\\xcd\\x95t\\x1a\\xf6q\\xb5\\xfc\\x08\\xe5~\\xb9p*Q\\xb2\\x00m\\x92\\x82\\xd3\\xd6\\\\\\xcc\\t\\xa3q0\\xd1\\x17\\x85Tip\\xebz\\xd0I\\xe4\\x8e\\x0bJ3\\xaf\\xc8\\xa4\\x83>wY\\xb2L!\\xc6\\xf7\\xd8\\x18_8\\xade<g\\xa1XA\\xfc\\xaf\\x1a\\xc5\\xab^\\xa7\\\\\\x83\\x18p\\xae\\xb1\\xcd}_\\x08l\\xbb\\xbf\\x1e<(b\\xc4\\x8a\\x95\\xd6J\\x98J\\xb8\\x9aF\\xa6S\\x97\\x9dV#Z\\xae\\x90\\xef\\xd8\\x85\\x1fw\\x0e\\x91^\\xd0Yb\\xe7\\xc9\\x06J4\\xc4\\x192!3\\x14\\x14T\\xeb5K\\x97\\xe5\\x1c\\xcc&\\xe7\\xa2b\\xcaP\\xcfiF\\x0c\\x0b*\\xee\\x0e\\x86\\xf6\\xb9v\\xfe\\xd4\\xb4\\xe3\\x99l\\t\\xb6(\\xc2\\x13c\\xa6\\xddV\\xa6VXU^=\\xc1S\\xc95\\x9f&\\xf6|\\x83\\xc7\\x05 |\\xb4[\\x96%\\x17\\xe7\\x07\\xd7\\xbc\\x86\\xb7@\\t\\xb3[\\x04J\\xcd\\x89\\xb1\\xd9\\xe3\\xa7NI\\xdf\\x1cx\\x18\\xcf\\xdcc\\x85\\x8f\\x80%\\xb4\\x86\\xa8ul\\xb2\\xba$\\xd3\\x88\\x00j\\xfe\\xe4j\\x10@5`5H\\x01\\xa8\\x80:\\xe6rj\\x94\\x05\\x93s\\xfer\\x1dG5K2Ir\\xb4\\xc7\\xfeT^\\xacRV\\x8fkm|4j\\x86\\x03`Irc\\xd6e\\x05\\xb7\\xbb\\xaa\\x81R\\xf3P\\x93\\x04\\x01z\\xd7\\x97Uvn^\\x0e\\xd4i[\\xd3Q\\xf6N\\x1f\\x81\\xa8\\x1e\\x9d\\x10\\xed\\x1d\\x88+\\xb8\\xb3\\x03\\x90\\xa1\\xd9T\\xe6(\\xfa\\xf6O\\xf8\\x80\\x84`\\x1d\\x07\\x98b\\x96\\x05\\xf9r\\x1d0\\xf4\\xb1\\xc3\\xf3\\x80B\\xea0\\xac\\x10\\xe4W\\x92\\r\\xe5w\\xb0\\xc3\\xa3r\\xeb6\\xc9\\xed\\x92D\\xea\\xf7\\'AB\\x82\\xe7,`q\\xa4\\xc1\\xe4\\xba\\xf9\\xbdo;\\x05\\xd6\\xcd\\xfcO\\x80\\xfbJ5~\\xcb\\n\\xd3\\xa1\\xa0~\\xfd5\\xc6=\\xb2\\xaa\\xacK*@>\\xc4\\xd3\\x16M6\\x1dd|\\xe9\\xd5%\\x17\\x944\\xac&\\xb5Vn\\xf7\\xae\\xdeq\\x0b\\x1b\\xd65\\xdbU\\xfb\\xad\\xe7g\\x05*\\xa2V\\xcb\\xa9\\xeb\\xaa\\xfc\\x12OO\\x91\\x87\\xcb\\x91:gkrCJ\\xd3\\xd8\\xec!\\xd8\\xb9\"\\xb6\\x83rW6\\xf5\\x07\\xe4\\xa4\\xa2\\x12<)\\xe9\\x93\\xef\\xce3\\x85\\x11b7\\xf09\\x97v1Eo\\x0c.\\xf4]w\\xbe;pY\\xc5l{\\x0b\\xf9>\\xf7X*W\\x91\\xc1`\\xe1+\\xb5\\xb1\\xac\\xeck\\x89p\\xd2\\xa1\\x92\\xc1\\xc6\\xfe\\x13\\x95b\\xe2\\x03\\xa3\\xc8h\\x00\\x05\\x94\\x13\\xbb\\xe4\\r\\x0c\\x8aj\\x0b\\xda\\xee2\\xfd\\x0f\\xad\\xaf\\x8aP\\x19t\\xf7/*\\xdb\\xd6\\xae\\xef\\xe4\\xb5K\\xcbw\\xdc\\xbd1\\x96\\xc9\\xd2s\\x86A4X\\xd3,\\xd5!\\x95DE\\t\\xc8Qg\\x16\\xcc^\\x16\\x9a\\xad\\x18?\\xe8yC>C?B\\\\73\\x02od\\x15\\xd8\\x7fMu\\x8b\\x8azSb\\x15i7.u\\x17_\\xfbv;\\xe1\\xb9\\xdb\\xc0aA\\x90\\xe9I\\xcc\\xecG\\x1a5\\r\\\\\\x1d\\x16)[Q,\\xd4%\\x93\\xbf\\xe1\\xe45\\x0f]\\xe2\\x1b\\xe0\\xc6\\xfe#\\x95\\x90l\\xbf\\x99W\\x9a\\x11\\xa5\\x9c\\xacy\\xfe\\x1beI\\xbf\\xba\\xce\\xa8\\x82j\\xe97\\xe5\\x0e\\xb8r\\x02\\xdd\\xd8X\\x95\\xee\\x06fx\\xa0\\x1f\\xfb\\xe9c\\xa8\\x89Kwm\\x84\\xbd\\xc1[4\\xdc2\\xc4\\xc8\\\\\\\\\\x03-Q4\\xf6&\\xe2\\xfc=\\x99\\x99\\xe4\\xcfQl\\xc8\\xe5hg\\xfa|\\xf6\\xea-\\xed=\\xfc\\xd8\\x86N\\xc82\\xac39wC\\xd7H\\xf9tt\\x04\\x9bg\\xb4\\xe5\\xfb6x\\xa8L<\\xcb\\x82\\xb7\\n\\x10M\\x8cIf<\\x0b\\xd5\\xec\\x1d\\\\\\xcbg\\x1a\\x97\\xa8\\xee\\x9b{\\xa6J\\x97\\xd8\\xd4As>|\\xa8q\\x1d\\x96\\xa2-K\\x92j)0\\x8aA s\\xf32\\x00\\r\\xbc\\xfe\\xdb\\xdb\\x0e\\xf9s\\xabK^\\xc9(\\x82#\\xf1[\\x00\\x97\\x89e\\x9bm\\xfd<\\xe8\\xa4\\xe7\\x1b\\xff\\xed\\xfa<\\xdc\\xe2\\xd3)&\\xcd\\x93K\\x07\\xe5\\'\\x1c8\\xd7\\xbfp\\xa2I^\\x97\\xd3\\x05\\xaek\\xb4e4\\xdd\\xe5\\xbfn\\x13\\xaaV&_\\x1b\\x11\\xb5\\x82\\x88\\x07\\xc8\\x1c\\x16V\\x82A\\x99\\x13K\\xfd\\xce\\x18\\xe5\\x85\\x93\\x17\\xb7\\xf5\\xe2\\x9f\\x08\\x91\\xe0_\\xb1y\\xe2Whq\\x9f<\\xbb\\x00\\xf6\\x9c!zf\\x89*\\xb8\\xa2/\\x85\\x92\\xd0\\xed\\x0f\\x93\\x0e\\x1f\\xce\\xc83\\x9a\\xe1\\xb8\\xf0z\\x1b\\xbe\\x05\\\\\\xa3\\x01\\x1f]\\x12i\\x94S\\x1e\\xc5\\x19\\x0c:,\\xed\\xadY\\xd3\\x88IZQ)[\\xf3\\x1a\\xc8\\xf9\\x8fk=\\x8c\\x19|\\x8c\\x9f\\xbf|\\x82Acq\\xe93#\\xb2\\xb6:f\\xdfa\\xf2\\xa6r\\xd2\\xe1U\\xbfL\\xb7\\xd0\\xa5\\xb0E\\xa0\\xa2\\x987R/\\x1b*\\xfaN\\xe6\\xbaw$\\x91n\\xc1\\xac\\x1e\\x08x\\xe7\\xa4k\\x008\\x95\\xef\\xe2i.T<p\\n\\xce\\x19,t\\x99Yr\\x86\\x88\\x183\\xf3\\xa4N\\x1f\\x8b\\x0cP\\x8a\\xb52)Dq\\r\\xd5\\xbcS\\xa7\\xc8; FF\\xa8!\\neX\\xcf\\xa7\\xff\\xc0\\xd2i\\xb8\\xc3\\xe7\\xa7W\\xa2\\xfd\\x06\\xe0s\\xba\\x16\\xe3\\xe7\\x12\\x9a6\\x0b9\\xe3\\xd4\\xb6{&\\xef\\xe5M\\x13\\xdc\\x00g`y\\xf1\\xc0\\x14\\x9b\\x1d\\x9a\\xf5\\x1a3\\x14\\x19\\xba\\xcb\\xa1\\x16~fu\\xee\\xbf\\xcf\\xfe\\xbd\\x84\\xa9_s\\xac3\\x08\\xc8c\\xbe\\xee\\xc8Cu\\xf4\\xba8\\xdeO\\xbf\\xef-?N\\xbe\\xdb\\xfc\\xb6\\x19\\x7f\\x84\\x05Tq\\x07\\xda\\xd6\\xefNg\\xa3\\xf5\\xa7&g\\x06\\xabi]\\xcb\\xc1\\x86\\xa5\\x9cL\\xc5K\\xbb\\x01\\xa1\\xb2\\xd7\\xbb\\xa0w1)\\x17oR\\x9aj8:c\\xa3\\x0e\\xd7\\x15+\\xdd\\x84-\\x18\\xf6Xc\\xcf\\xa4O,r\\xb8!\\xc2\\xe1<\\x16\\x88R*\\xa1\\x1e\\xaeu\\xa06o\\x0eC\\x16\\xf8\\xc4\\x83G\\x93W\\xbaer\\x8f\\n\\xfd\\x95\\xd5\\x9f\\xe1D\\x82\\xda\\xf4I\\tb|:6\\xf0\\xc1\\t\\x18\\xf9\\xe9/\\xa8\\x05S0\\x7f~*\\xc4\\xfe\\xe3\\xf0xz\\x9d\\xe6\\xb8:HW\\x9bW\\xea:g\\x82@\\xc1\\x91\\x08\\x9e\\xa1Q\\x1d\\x06\\xf8\\x97I\\x93i\\xe5b\\xefr\\\\-68K\\xeb\\xe0q?R\\x9f\\xac\\x8b\\x0b\\xbb(\\x1b\\x7f\\xfa\\xdd4\\xfe\\x84\\x17\\x89!\\x0bc\\xfd\\xdcK\\xdb\\x932\\xe2\\xc6\\x94\\xa9\\xce\\xf9K\\x9efIq\\xd2ck\\x13\\x83\\x88\\xb4LJ\\x9f\\x82\\x18\\xd2J@\\xd7\\x94\\xfb\\x82\\xfb\\xdfi\\xb2\\xc6\\xfd\\x1b\\xf8L G\\xc9\\xa1\\x81\\x10\\xb9;\\x84\\xf9\\xd4\\x9d\\x93\\xdcdb\\xc3\\n[\\x00\\xa67\\x08\\x89\\xcd\\xf9e\\xa8\\xa6\\xfd\\x04\\xb1\\xa3\\x0fh`\\xccv\\xb9\\x8d~\\xd2\\x1b_\\xc1\\xab\\x04\\x17\\x15z\\x85I\\xbc\\xb7\\xc7\\xd9<\\xaa~S`\\x82\\x9d7\\xb5\\x86\\xa5/\\x16\\xea\\x8d\\x9cueS;\\x86T5\\x9bZ\\xc3\\xba+\\xd7\\x17\\x14\\xe4w\\x80\\xca\\xb2p\\x1b.\\xfcLKi \\\\\\xca\\xde\\x11\\xcd\\x03\\xe5\\x14\\xd5\\x12H\\x13\\x06)\\x01\\xe81\\xc9\\xc9\\xa3h%\\x9d\\xc2f:\\x1am\\xe7\\x1f\\xc3\\xb0\\x16\\xdf\\xe1\\x05\\xb3\\x87\\x1e\\x1c\\xdf\\xfb\\x84\\x80<Y\\x00N{U\\xb6\\xf7\\x8e\\x1e\\xea\\xb4\\xf1\\xe1\\x08q\\xd48\\xb4\\xd2\\xb1\\x0b.\\x96\\xe7Q|B\\x03\\xa0\\x7f\\xaf\\xe3\\x10iW\\x839\\xe5\\x1f\\xb5\\xb5\\x9b\\xcc\\n\\xb6\\xce\\xb4\\xf5J\\xc1\\x03\\x84\\x81+`2\\xd8d\\xa4\\x83Q\\x9b8I\\x07\\xb5\\x11\\x8d\\xa2y\\xe6\\xc3\\xc16h|\\xcf]\\x8f\\x88\\x81\\xe4b\\x9f\\xef4P\\xa0^5N\\x92\\r\\xd4\\tw~\\xc4\\x9d\\xd0\\xe7\\x07N#\\xc0V\\xce\\x0bR\\xb6\\xf0K\\r\\xbcg\\\\F\\xbe\\xe5\\xae\\\\ [}\\x96\\xf9\\x15^\\xc9\\x80\\xb6s.&fDkUO\\t\\x9f\\x1e\\xb0\\xf2i\\x81\\n[\\x1fL\\xa8[\\xba\\xff\\xd2\\xad_[\\xc6\\xf4\\x16Q\\xbe$\\xd8\\xd96}\\xb7\\xa7\\xa9\\xe0\\xe4\\xc4\\xe6\\x02\\xd9=\\xc37\\xfe\\xda\\xe4U\\xeb?UJE\\xe3\\x9e$I\\x9fC\\x9c\\xe3j\\xc5RD+\\x1f\\x192\\x8f\\'\\xc0\\xd3\\xb1\\xe2\\x12\\xcb\\xa3\\x8f@\\xd9xk\\t-\\x8f\\x0c\\xb5\\t~\\xe0\\xb1\\xb4iX\\x7fm\\xe4\\xf0\\x1e\\xc5wB:\\xce53\\x1f\\x9el\\xa5\\xbf9\\xb8\\xf7\\xc1F\\xe9g\\xb2\\x14\\x81\\xaa\\x18\\xab\\xd1\\xf7\\xafv\\xfc\\x1aa\\x1dNZ\\xb8\\xceT5+\\x1fM\\x07\\x95\\xf4\\xe2\\xfdQt\\xafI]\\xeb\\xed\\xe4\\x13\\x14\\xb3CN\\x80\\x03J\\xa7\\x16Y\\x0b\\x04$X\\xf1\\x16&\\xa2\\xb1h\\xe8\\r\"\\x871\\xa2\\xfc\\xf9\\nx0\\x02p\\xa0\\xbf\\xa6@N\\x0eK\\xf2\\x06%yW\\x10\\xecS\\x1c\\xe7\\x7fR*j\\xdb\\x18\\x0e\\xe0c\\x82\\xd4\\xa0^\\xfeL\\xbb\\x16\\x92\\xe8\\xff\\xc4\\xd6@2\\xdb \\xb9}^\\xd9Q%\\x03r\\xcbo\\x08\\xb30\\x85$%.\\xb07\\xebc\\xc5q\\xb5\\x94\\x0cJ\\x93h\\xbb\\x0b\\x16:\\xdb\\xb9\\xf6\\xe6\\xc7q\\x03D\\xa0W\\xf6\\x0c\\xd7 \\x10\\tu\\xca\\xdeS\\xe6\\x9f8\\xe4Q\\xb4\\x84\\xaa\\x86\\xc3\\xedn2\\x84\\xf6-\\xf5S\\xa7\\xec\\x12\\xc3\\xed\\xeb\\x92\\xc7\\x9a\\xa7\\xd5(7\\xe5\\x89\\xcc\\xff\\xe4\\xadE\\x95~\\xe4\\x81/\\xb0\\xe60\"bg\\xef\\'\\xd3\\x8d\\t&o\\xc8\\x01i\\x0e\\xa8!S\\x1e\\x90i\\xe8\\x11\\x8a\\x86\\xf5\\x112UxA\\xe9\\xe5\\x0e#N\\xa4\\x9f\\x0b\\xe6\\x07\\\\s\\x9c\\x1c\\x08\\xe1\\x88\\xb3\\x0f\\xc6\\xb6\\x92\\x80\\xd9\\x82Xk\\xc1\\x06\\xe8\\x01*\\x9d\\x03\\xd5t\\x01\\xd6\\xc7\\xa6\\xe8e\\x14=\\xfd\\xcfn\\x1a\\xf4\\x8a/\\xca|\\x83\\x86\\x11/pD\\x1d\\xf6%(\\xf5_Q\\x1a\\x16\\xfc%b\\x83JK\\xe3_\\xa9\\xf3<\\xd1\\x19\\xba\\x11\\xbc\\xd6\\x88\\x04\\x96\\xddH\\xc9\\xb7\\xc9\\xe0\\xb4\\xd0q+S\\x83(\\x0f\\xe8\\xca\\x8ew\\xa7\\x85\\x17q!q+\\xe2\\x89\\x08^9H\\xbb\\xb6\\xf8*|\\xdb&\\xfd\\xb3\\x0f\\xeb\\x06]\\x12{B\\x8d\\x02G\\xbd%\\x93\\xd7\\xa5\\x8a\\x8ft\\x00J\\t\\x831\\xf4\\xcd\\x9f\\xe1\\xa9\\x7fN\\xad\\xc3\\xba\\xf5\\xa2\\x0f\\xff\\\\)m\\x8b:\\xc7\\x00v\\xd3*:a\\xc3\\xc8,\\x9cFV\\xfdY6\\x06\\x11\\xc28\\xb0\\x14\\xdd4\\xd8\\xed\\x04\\x87Fu\\xe8\\xe7\\xd6\\xb2\\xc7}\\xfc\\x15\\xf5\\x0f\\xca\\xc9\\xd8\\x17\"c\\xf5\\x03>\\xf6\\xbf\\x7fr\\xe8\\xb9\\x10b\\xd2\\xaf\\xf1#\\t0\\xc5mf\\xb5\\xba\\xae\\x19\\t\\x9e\\xfcU\\xaa\\x10O\\x0e\\xd5\\xfa\\x0e\\xa7\\x96\\x90\\r{\\xfe(\\x0e\\xd8?2\\x18\\xa9\\xf34\\xe1_*\\xda\\xe7\\xa4W#v\\xacv}\\xb3\\xc8\"R\\xe2\\x8f9\\x83\\xa7\\xd7\\x1f\\x19\\xdc\\xbe\"\\xe5\\xbe\\xeaQ\\x96\\xd4n\\xe9\\xf9\\xa5C9\\x1e\\x0e\\xf9\\xfcV\\xf4t<6\\xa7\\x9a]Y2\\xdc\\x1e_wh\\x96#~q\\xec\\xc4\\xcb\\xb5\\xc3:\\xb9X+=\\x06][\\x89\\x19\\x12\\xae\\x0c\\xc9QX\\x92y}\\xa8\\xc3Q\\xdf\\x87o\\x9dTK\\xc8L\\x1a&\\x84\\r\\x9a\\x80%\\xd6`-\\xc3\\x9c\\t2\\xa3\\x1a\\x0cUsLs\\x89\\xfa\\xe7\\xcc\\xd7\\xf1\\xbaa\\x1cG\\x8a\\xc1X\\xf5\\x06,\\x87Wv\\x84\\xaa7q\\xd0\\xc1^\\xcbaL\\x91\\x10\\x0c\\x06\\xd2e6J\\x8f\\x99\\xcc\\xc9\\xb3\\xd7\\x11\\xeb\\xb76g\\x06j\\xb6\\xd2\\x9e\\xbe\\xc9\\x86n\\x8e\\xf0\\xafo\\xf0\\xb1$\\xbb\\xe0F2Q\\x99\\xd9\\r\\xc3\\xeavI~]4\\xb8P\\xc9\\xf2\\xda/\\xf1\\xd1/\\x15\\xf6KD\\xbb\\xaaa\\xee\\x96\\x183\\x89\\x05IO\\xbc%\\xd9I\\xaa\\xab\\xe78\\xbaU\\xe2\\x03\\xb37\\x1a\\xb4\\x89R\\xabC\\x83\\xf9ps\\x85k\\xa6\\xda\\xef\\x00K\\xd1\\xc7\\xf7\\x06|\\x98\\xdfl\\x8a\\xc46\\xe6a\\\\\\xa0\\xe6Cd\\x9d\\x95\\x98\\xae\\x94XV1\\x8d\\xe0\\xcd\\xeb\\x12e\\xc4\\x00%\\xe6\\xbd\\xe3\\xc7\\xc2zn\\x80\\xb2b\\x88\\xf2\\xc7w!)\\xbeP\\x84>\\x00\\x80%\\xdb\\xb7A\\xb0#;\\x18\\xe7Xc]\\xbc\\xd7m\\xcaz\\xf4gp\\x0f\\xde\\xa00\\xfevs\\xfb=\\x1b\\xf3\\xaf\\xce\\xff\\x89\\x93K\\x11\\x1c\\xb3\\x9b\\xf1\\xba&\\xc1\\xa5D}\\xae\\xf4i\\xd7\\x04\\x89|\\x03\\x06Y\\xd0\\x02a\\x9ax\\xae\\xa6\\x9be\\x98\\xa6\\x06\\xc9\\x85\\xa4S\\xa9\\xfdoFEo\\xcb\\xf0\\xed\\xf5\\xecO\\x12\\xc8+\\xc7\\xba\\xdb\\xba\\xdeK6\\xf8\\x96\\xb2\\x9b\\xf2,\\xa2\\x8d6\\x10\\xfd\\x97\\xe2O\\x16\\xf2\\x93n\\x8fVr\\x85\\x10\\x91\\xdcjO\\xaf^\\\\)\\xaan.t-\\xe2\\x95\\x04A\\xee!gg\\x104\\'\\xb8\\x1e\\xda:\\x90\\xc3\\x1eZM\\x06,<\\xb2\\x87J^\\x02\\xc0\\xc3x%O\\xe8\\xd7_i\\xe8\\x12/\\xedFC\\x14\\x1b\\xd3:\\xad\\x03\\x94\\x05\\xb7i8\\x0e\\xf1\\xc2\\x1f!\\xfb\\xef\\xcf3G\\x91\\x19\\xd6\\'0\\xd8\\x82\\xa40!\\x83\\xbc*F!\\xa7\\x8d)l\\x9e\\xef\\xa1\\xa3\\x1c\\x8a\\xae\\xd7\\x87K\\'\\x862\\x90#[\\x943\\x90$\\xa7\\xd8a\\x0b\\xa5\\xf7\\\\\\xf7r\\x95\\x04\\xd26\\xd44\\x07\\x1a&2\\xbb5\\xf1d\\xdf\\x15\\xc1\\x87\\xcfM\\xa7\\xb8\\xb4\\xfb\\xdd\\xe3 \"\\xd7W\\xc4\\x84\\xb0\\xd7'"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.frombuffer(tmp_array[0], dtype=np.uint8) == udp_numpy[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([168,  83, 220, ..., 132, 176, 215], dtype=uint8)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.frombuffer(tmp_array[0], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_a = weights_dict['transformer.layers.0.0.fn.temperature'].tobytes()\n",
    "bit_array = np.frombuffer(tmp_a, dtype=np.uint8)\n",
    "if bit_array.shape[0] % 64 != 0:\n",
    "    padding = np.ones((64 - bit_array.shape[0] % 64), dtype=bit_array.dtype) * 255\n",
    "    bit_array = np.concatenate((bit_array, padding))\n",
    "bit_array = np.unpackbits(bit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bytes to numpy array of bool\n",
    "bit_array = np.frombuffer(tmp_a, dtype=np.uint8)\n",
    "bit_array = torch.from_numpy(np.unpackbits(bit_array))\n",
    "# bool_array = bit_array.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([130, 512])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit_array.view(-1,512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(500):\n",
    "    bit_array = numpy_to_bit(weights_dict['pos_embedding'])\n",
    "    array_len = len(bit_array)\n",
    "\n",
    "    # Calculate the number of rows in the resulting matrix\n",
    "    num_rows = (array_len + 511) // 512  # Round up to cover all elements\n",
    "\n",
    "    # Create a matrix of size (num_rows, 512) initialized with ones (for padding)\n",
    "    matrix = np.ones((num_rows, 512), dtype=bit_array.dtype)\n",
    "\n",
    "    # Fill in the matrix with slices of bit_array\n",
    "    matrix[:array_len // 512, :] = bit_array[:array_len].reshape(-1, 512)\n",
    "\n",
    "    # For the last partial row, copy the remaining elements\n",
    "    remaining_elements = array_len % 512\n",
    "    if remaining_elements > 0:\n",
    "        matrix[array_len // 512, :remaining_elements] = bit_array[-remaining_elements:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recv_packet = {i:[] for i in range(num_nodes)}\n",
    "for tmp_id in range(num_nodes):\n",
    "    for i in range(len(udp_packet)):\n",
    "        tmp_packet = struct.pack('I',0) + udp_packet[i]\n",
    "        node_s[0].sendall(struct.pack('I',len(tmp_packet))+tmp_packet)\n",
    "        if (i+1) % 16 == 0:\n",
    "            try:\n",
    "                while True:\n",
    "                    server_r.settimeout(3)\n",
    "                    data = server_r.recv(len(tmp_packet))\n",
    "                    server_r.settimeout(0.5)\n",
    "                    node_id = struct.unpack('I',data[:4])[0]\n",
    "                    recv_packet[node_id].append(data[4:])\n",
    "                    # recv_packet.append(data)\n",
    "            except socket.timeout:\n",
    "                # print('Timeout')\n",
    "                # print(len(recv_packet[0]))\n",
    "                server_r.settimeout(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_array = packet_aggregation(udp_packet, c_1024, 8, info_ni, freeze_ni, codeword_idx, bit_array_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_dict = {}\n",
    "# restored_array = [torch.tensor(arr).to(device) for arr in restored_array]\n",
    "for i, name in enumerate(weights_dict):\n",
    "    restored_dict[name] = torch.tensor(restored_array[i].reshape(weights_dict[name].shape)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(restored_array)):\n",
    "    assert np.array_equal(restored_array[i], list(weights_dict.values())[i].flatten())\n",
    "# np.array_equal(restored_array[1], list(weights_dict.values())[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(restored_array)):\n",
    "    assert np.array_equal(restored_array[i], tmp_restore[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
