{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torchvision.transforms import ToTensor\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cython_decoder import cython_sc_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind((host, port))\n",
    "server_socket.listen(8)\n",
    "\n",
    "array = np.array([1.0, 2.5, 3.5, 4.5, 5.5], dtype=np.float32)\n",
    "shape = array.shape\n",
    "shape_data = struct.pack('!' + 'I' * len(shape), *shape)\n",
    "shape_size = len(shape_data)\n",
    "array_data = array.tobytes()\n",
    "array_length = len(array_data)\n",
    "packet=struct.pack('!I', shape_size) + shape_data + struct.pack('!I', array_length) + array_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_s = []\n",
    "node_r = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        client_socket, addr = server_socket.accept()\n",
    "        server_socket.settimeout(1)\n",
    "        data = client_socket.recv(1024).decode()\n",
    "        if data == \"Server-R\":\n",
    "            server_s = client_socket\n",
    "        elif data == \"Server-S\":\n",
    "            server_r = client_socket\n",
    "        elif data == \"Node-R\":\n",
    "            node_s.append(client_socket)\n",
    "        elif data == \"Node-S\":\n",
    "            node_r.append(client_socket)\n",
    "        client_socket.sendall(struct.pack('I',len(b\"start\"))+b\"start\")\n",
    "except socket.timeout:\n",
    "    print('Timeout')\n",
    "    server_socket.settimeout(None)\n",
    "\n",
    "for tmp_socket in node_r:\n",
    "    tmp_socket.recv(1024)\n",
    "server_r.recv(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    server_s.sendall(struct.pack('I',len(packet))+packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp_socket in node_r:\n",
    "    tmp_socket.close()\n",
    "for tmp_socket in node_s:\n",
    "    tmp_socket.close()\n",
    "server_s.close()\n",
    "server_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"cifar10\",\n",
    "    partitioners={\n",
    "        \"train\": DirichletPartitioner(\n",
    "            num_partitions=10,\n",
    "            partition_by=\"label\",\n",
    "            alpha=0.1,\n",
    "            seed=42,\n",
    "            min_partition_size=0,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "  transforms = transform_train\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "def test_transforms(batch):\n",
    "    transforms = transform_test\n",
    "    batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "partition = fds.load_partition(0, \"train\").with_transform(train_transforms)\n",
    "centralized_dataset = fds.load_split(\"test\").with_transform(test_transforms)\n",
    "train_loader = DataLoader(partition, batch_size=512, shuffle=True, num_workers=16)\n",
    "test_loader = DataLoader(centralized_dataset, batch_size=100, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_small import ViT\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 20)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                criterion: nn.Module, \n",
    "                device: torch.device, \n",
    "                scaler: torch.cuda.amp.GradScaler, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Epoch: {epoch},Train Loss: {total_loss / total_samples:.4f}, Train Accuracy: {total_correct / total_samples:.4f}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   test_loader: DataLoader, \n",
    "                   criterion: nn.Module, \n",
    "                   device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Validation Loss: {total_loss / total_samples:.4f}, Validation Accuracy: {total_correct / total_samples:.4f}\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,Train Loss: 0.0029, Train Accuracy: 0.5645\n",
      "Validation Loss: 0.0534, Validation Accuracy: 0.1576\n",
      "\t\n",
      "Epoch: 1,Train Loss: 0.0020, Train Accuracy: 0.6724\n",
      "Validation Loss: 0.0527, Validation Accuracy: 0.1591\n",
      "\t\n",
      "Epoch: 2,Train Loss: 0.0018, Train Accuracy: 0.7070\n",
      "Validation Loss: 0.0492, Validation Accuracy: 0.1709\n",
      "\t\n",
      "Epoch: 3,Train Loss: 0.0017, Train Accuracy: 0.7263\n",
      "Validation Loss: 0.0471, Validation Accuracy: 0.1800\n",
      "\t\n",
      "Epoch: 4,Train Loss: 0.0016, Train Accuracy: 0.7385\n",
      "Validation Loss: 0.0459, Validation Accuracy: 0.2234\n",
      "\t\n",
      "Epoch: 5,Train Loss: 0.0016, Train Accuracy: 0.7464\n",
      "Validation Loss: 0.0453, Validation Accuracy: 0.2072\n",
      "\t\n",
      "Epoch: 6,Train Loss: 0.0015, Train Accuracy: 0.7581\n",
      "Validation Loss: 0.0453, Validation Accuracy: 0.2102\n",
      "\t\n",
      "Epoch: 7,Train Loss: 0.0014, Train Accuracy: 0.7609\n",
      "Validation Loss: 0.0427, Validation Accuracy: 0.2464\n",
      "\t\n",
      "Epoch: 8,Train Loss: 0.0014, Train Accuracy: 0.7712\n",
      "Validation Loss: 0.0438, Validation Accuracy: 0.2535\n",
      "\t\n",
      "Epoch: 9,Train Loss: 0.0014, Train Accuracy: 0.7734\n",
      "Validation Loss: 0.0413, Validation Accuracy: 0.2752\n",
      "\t\n",
      "Epoch: 10,Train Loss: 0.0013, Train Accuracy: 0.7824\n",
      "Validation Loss: 0.0436, Validation Accuracy: 0.2475\n",
      "\t\n",
      "Epoch: 11,Train Loss: 0.0012, Train Accuracy: 0.7940\n",
      "Validation Loss: 0.0424, Validation Accuracy: 0.2644\n",
      "\t\n",
      "Epoch: 12,Train Loss: 0.0012, Train Accuracy: 0.7994\n",
      "Validation Loss: 0.0412, Validation Accuracy: 0.2784\n",
      "\t\n",
      "Epoch: 13,Train Loss: 0.0012, Train Accuracy: 0.8062\n",
      "Validation Loss: 0.0396, Validation Accuracy: 0.3042\n",
      "\t\n",
      "Epoch: 14,Train Loss: 0.0012, Train Accuracy: 0.8058\n",
      "Validation Loss: 0.0401, Validation Accuracy: 0.2916\n",
      "\t\n",
      "Epoch: 15,Train Loss: 0.0012, Train Accuracy: 0.8106\n",
      "Validation Loss: 0.0410, Validation Accuracy: 0.2847\n",
      "\t\n",
      "Epoch: 16,Train Loss: 0.0011, Train Accuracy: 0.8082\n",
      "Validation Loss: 0.0417, Validation Accuracy: 0.2864\n",
      "\t\n",
      "Epoch: 17,Train Loss: 0.0011, Train Accuracy: 0.8181\n",
      "Validation Loss: 0.0401, Validation Accuracy: 0.3052\n",
      "\t\n",
      "Epoch: 18,Train Loss: 0.0011, Train Accuracy: 0.8141\n",
      "Validation Loss: 0.0409, Validation Accuracy: 0.2953\n",
      "\t\n",
      "Epoch: 19,Train Loss: 0.0011, Train Accuracy: 0.8163\n",
      "Validation Loss: 0.0408, Validation Accuracy: 0.2961\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_model(net, train_loader, criterion, device, scaler, optimizer, i)\n",
    "    evaluate_model(net, test_loader, criterion, device)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_bit(number):\n",
    "    # Ensure the input number is a NumPy int32\n",
    "    int32_number = np.array([number], dtype=np.int32)\n",
    "    # Convert the int32 number to a byte buffer\n",
    "    byte_representation = int32_number.tobytes()\n",
    "    # Convert each byte to an 8-bit binary string and concatenate them\n",
    "    bit_stream = ''.join(format(byte, '08b') for byte in byte_representation)\n",
    "    # Convert the bit stream to a NumPy array of floats (as in your original bit_array)\n",
    "    bit_array = np.array([float(bit) for bit in bit_stream], dtype=np.float64)\n",
    "    return bit_array\n",
    "\n",
    "def bit_to_int(bit_array):\n",
    "    # Convert the bit array (floats) to a string of bits\n",
    "    bit_string = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    # Convert the bit string to an integer\n",
    "    int_value = int(bit_string, 2)  # Convert binary string to an integer\n",
    "    # To get the original int32 value, interpret the bits as an int32 value\n",
    "    # Handle cases where the original number might be negative by using np.int32\n",
    "    int32_value = np.frombuffer(int_value.to_bytes(4, byteorder='big'), dtype=np.int32)[0]\n",
    "    return int32_value\n",
    "\n",
    "def tensor_to_bit(tensor):\n",
    "    if tensor.dim() == 0:\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    byte_tensor = tensor.view(torch.uint8).flatten()\n",
    "    bit_stream = ''.join(format(byte.item(), '08b') for byte in byte_tensor)\n",
    "    bit_tensor = torch.tensor([float(bit) for bit in bit_stream], dtype=torch.float64)\n",
    "    return bit_tensor\n",
    "\n",
    "def bit_to_tensor(bit_tensor):\n",
    "    bit_stream = ''.join(str(int(bit.item())) for bit in bit_tensor)\n",
    "    byte_tensor = torch.tensor([int(bit_stream[i:i+8], 2) for i in range(0, len(bit_stream), 8)], dtype=torch.uint8)\n",
    "    float_tensor = byte_tensor.view(torch.float32)\n",
    "    return float_tensor\n",
    "\n",
    "def numpy_to_bit(array):\n",
    "    tmp_byte=np.frombuffer(array.tobytes(),dtype=np.uint8)\n",
    "    bit_stream = ''.join(format(byte, '08b') for byte in tmp_byte)\n",
    "    bit_array = np.array([float(bit) for bit in bit_stream], dtype=np.float64)\n",
    "    return bit_array\n",
    "\n",
    "def bit_to_numpy(bit_array):\n",
    "    bit_stream = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    byte_array_back = np.array([int(bit_stream[i:i+8], 2) for i in range(0, len(bit_stream), 8)], dtype=np.uint8)\n",
    "    float_array_back = np.frombuffer(byte_array_back.tobytes(), dtype=np.float32)\n",
    "    return float_array_back\n",
    "\n",
    "'''\n",
    "Encoding and decoding function\n",
    "'''\n",
    "def encode(u):\n",
    "    N = u.shape[0]  # Get the length of u\n",
    "    n = int(np.log2(N))  # Calculate the log base 2 of N\n",
    "\n",
    "    if n == 1:\n",
    "        x = np.array([(u[0] + u[1]) % 2, u[1]])\n",
    "        return x\n",
    "    else:\n",
    "        x1 = encode(np.mod(u[:N//2] + u[N//2:], 2))\n",
    "        x2 = encode(u[N//2:])\n",
    "        x = np.concatenate((x1, x2))\n",
    "        return x\n",
    "\n",
    "def rvsl(y):\n",
    "    N = y.shape[0]\n",
    "    if N == 2:\n",
    "        return y\n",
    "    else:\n",
    "        return np.concatenate((rvsl(y[0:N:2]), rvsl(y[1:N:2])))\n",
    "\n",
    "\n",
    "def data_process(array):\n",
    "    bit_array = numpy_to_bit(array)\n",
    "    array_len = len(bit_array)\n",
    "    for i in range(0, array_len, 512):\n",
    "        sub_array = bit_array[i:i+512]\n",
    "        if len(sub_array) < 512:\n",
    "            padding = np.ones((512 - len(sub_array)), dtype=bit_array.dtype)\n",
    "            sub_array = np.concatenate((sub_array, padding))\n",
    "    current_idx = i // 512 + 1\n",
    "    return sub_array, current_idx\n",
    "\n",
    "def data_generate_torch(bit_tensor, data_idx):\n",
    "    u = torch.zeros(1024, dtype=torch.float32)\n",
    "    u[data_idx] = bit_tensor\n",
    "    x = encode(u)\n",
    "    x = rvsl(x)\n",
    "    x = 1 - 2 * x\n",
    "    return x\n",
    "\n",
    "def data_generate(bit_array, data_idx):\n",
    "    u=np.zeros(1024)\n",
    "    u[data_idx] = bit_array\n",
    "    x = encode(u)\n",
    "    x = rvsl(x)\n",
    "    x = 1-2*x\n",
    "    return x\n",
    "\n",
    "def decoding(bit_array, freeze_idx, data_idx):\n",
    "    # Prepare the necessary arrays and values\n",
    "    lr0 = np.exp(-(bit_array - 1)**2)\n",
    "    lr1 = np.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "    delete_num = 1024 - len(bit_array)\n",
    "    hd_dec = np.zeros(1024, dtype=np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    pro_prun = np.zeros((1, 2 * 1024 + 1), dtype=np.float64)\n",
    "\n",
    "    # Call the optimized Cython function\n",
    "    i_scen_sum, hd_dec_result = cython_sc_decoding(\n",
    "        lr0_post, lr1_post, freeze_idx.astype(np.float64),\n",
    "        hd_dec, 1024, 10, 512, frozen_val, delete_num, 0, pro_prun\n",
    "    )\n",
    "\n",
    "    # Extract the output for data_idx from hd_dec_result\n",
    "    data_out = hd_dec_result[data_idx]\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all weights to numpy arrays\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net.state_dict().items()}\n",
    "N = 1024\n",
    "n = 10\n",
    "rate = 0.5\n",
    "K = round(N*rate)\n",
    "coding_list = scipy.io.loadmat(\"1024-3db-d=2-mean.mat\")[\"count_number\"]\n",
    "coding_index = np.argsort(coding_list[:,1])\n",
    "info_idx = coding_index[:K]\n",
    "freeze_idx = coding_index[K:]\n",
    "\n",
    "# sort the final index\n",
    "info_ni = np.sort(info_idx)\n",
    "freeze_ni = np.sort(freeze_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_bit = []\n",
    "codeword_idx = []\n",
    "bit_array_len = []\n",
    "codeword_idx.append(0)\n",
    "total_idx = 0\n",
    "\n",
    "for tmp_key in weights_dict.keys():\n",
    "    tmp_array = weights_dict[tmp_key]\n",
    "    bit_array = numpy_to_bit(tmp_array)\n",
    "    bit_array_len.append(len(bit_array))\n",
    "    for i in range(0,len(bit_array),512):\n",
    "        sub_array = bit_array[i:i+512]\n",
    "        # Add 1 at the end of the array\n",
    "        if len(sub_array) < 512:\n",
    "            padding = np.ones((512 - len(sub_array)), dtype=bit_array.dtype)\n",
    "            sub_array = np.concatenate((sub_array, padding))\n",
    "        split_bit.append(sub_array)\n",
    "    total_idx += i // 512 + 1\n",
    "    codeword_idx.append(total_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_partial = partial(data_generate, data_idx=info_ni)\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    codeword_py = list(executor.map(encode_partial, split_bit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
