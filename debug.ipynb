{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import time\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import DirichletPartitioner\n",
    "from torchvision.transforms import ToTensor\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "from numba import njit, jit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from cython_decoder import cython_sc_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '127.0.0.1'\n",
    "port = 5000\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_socket.bind((host, port))\n",
    "num_nodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 20\n",
    "server_socket.listen((num_nodes+1)*2)\n",
    "node_s = []\n",
    "node_r = []\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        client_socket, addr = server_socket.accept()\n",
    "        server_socket.settimeout(10)\n",
    "        data = client_socket.recv(1024).decode()\n",
    "        if data == \"Server-R\":\n",
    "            server_s = client_socket\n",
    "        elif data == \"Server-S\":\n",
    "            server_r = client_socket\n",
    "        elif data == \"Node-R\":\n",
    "            node_s.append(client_socket)\n",
    "        elif data == \"Node-S\":\n",
    "            node_r.append(client_socket)\n",
    "        client_socket.sendall(struct.pack('I',len(b\"start\"))+b\"start\")\n",
    "except socket.timeout:\n",
    "    print('Timeout')\n",
    "    server_socket.settimeout(None)\n",
    "\n",
    "for tmp_socket in node_r:\n",
    "    tmp_socket.recv(1024)\n",
    "server_r.recv(65536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp_socket in node_r:\n",
    "    tmp_socket.close()\n",
    "for tmp_socket in node_s:\n",
    "    tmp_socket.close()\n",
    "server_s.close()\n",
    "server_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FederatedDataset(\n",
    "    dataset=\"cifar10\",\n",
    "    partitioners={\n",
    "        \"train\": DirichletPartitioner(\n",
    "            num_partitions=num_nodes,\n",
    "            partition_by=\"label\",\n",
    "            alpha=0.1,\n",
    "            seed=42,\n",
    "            min_partition_size=0,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "def train_transforms(batch):\n",
    "  transforms = transform_train\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "def test_transforms(batch):\n",
    "    transforms = transform_test\n",
    "    batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "train_loader=[]\n",
    "test_loader=[]\n",
    "for i in range(num_nodes):\n",
    "    partition_train_test = fds.load_partition(i, \"train\").train_test_split(0.1)\n",
    "    partition_train = partition_train_test[\"train\"].with_transform(train_transforms)\n",
    "    partition_test = partition_train_test[\"test\"].with_transform(test_transforms)\n",
    "    # centralized_dataset = fds.load_split(\"test\").with_transform(test_transforms)\n",
    "    train_loader.append(DataLoader(partition_train, batch_size=512, shuffle=True, num_workers=16))\n",
    "    test_loader.append(DataLoader(partition_test, batch_size=100, shuffle=False, num_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vit_small import ViT\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = []\n",
    "optimizer = []\n",
    "scheduler = []\n",
    "criterion = []\n",
    "scaler = []\n",
    "for i in range(num_nodes):\n",
    "    net.append(ViT(\n",
    "        image_size = 32,\n",
    "        patch_size = 4,\n",
    "        num_classes = 10,\n",
    "        dim = 32,\n",
    "        depth = 6,\n",
    "        heads = 8,\n",
    "        mlp_dim = 32,\n",
    "        dropout=0.1,\n",
    "        emb_dropout=0.1\n",
    "    ).to(device))\n",
    "\n",
    "\n",
    "    optimizer.append(optim.Adam(net[i].parameters(), lr=0.001))\n",
    "    scheduler.append(torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[i], 20))\n",
    "    criterion.append(nn.CrossEntropyLoss())\n",
    "    scaler.append(torch.cuda.amp.GradScaler(enabled=True))\n",
    "\n",
    "server_net = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 32,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 32,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader, \n",
    "                criterion: nn.Module, \n",
    "                device: torch.device, \n",
    "                scaler: torch.cuda.amp.GradScaler, \n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                epoch: int,\n",
    "                nodes: int):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Nodes: {nodes}, Epoch: {epoch},Train Loss: {total_loss / total_samples:.4f}, Train Accuracy: {total_correct / total_samples:.4f}\")\n",
    "\n",
    "def evaluate_model(model: nn.Module, \n",
    "                   test_loader: DataLoader, \n",
    "                   criterion: nn.Module, \n",
    "                   device: torch.device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "    print(f\"Validation Loss: {total_loss / total_samples:.4f}, Validation Accuracy: {total_correct / total_samples:.4f}\\n\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cli in range(num_nodes):\n",
    "    for i in range(20):\n",
    "        train_model(net[cli], train_loader[cli], criterion[cli], device, scaler[cli], optimizer[cli], i, cli)\n",
    "        evaluate_model(net[cli], test_loader[cli], criterion[cli], device)\n",
    "        scheduler[cli].step()\n",
    "    scheduler[cli] = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer[cli], 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_bit(array: np.ndarray) -> np.ndarray:\n",
    "    tmp_byte=np.frombuffer(array.tobytes(),dtype=np.uint8)\n",
    "    bit_stream = ''.join(format(byte, '08b') for byte in tmp_byte)\n",
    "    bit_array = np.array([int(bit) for bit in bit_stream], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def bit_to_numpy(bit_array: np.ndarray) -> np.ndarray:\n",
    "    bit_stream = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    byte_array_back = np.array([int(bit_stream[i:i+8], 2) for i in range(0, len(bit_stream), 8)], dtype=np.uint8)\n",
    "    int_array_back = np.frombuffer(byte_array_back.tobytes(), dtype=np.float32)\n",
    "    return int_array_back\n",
    "\n",
    "'''\n",
    "Encoding and decoding function\n",
    "'''\n",
    "@jit(nopython=True)\n",
    "def encode(u: np.ndarray) -> np.ndarray:\n",
    "    N = u.shape[0]  # Get the length of u\n",
    "    n = int(np.log2(N))  # Calculate the log base 2 of N\n",
    "\n",
    "    if n == 1:\n",
    "        x = np.array([(u[0] + u[1]) % 2, u[1]],dtype=np.int8)\n",
    "        return x\n",
    "    else:\n",
    "        x1 = encode(np.mod(u[:N//2] + u[N//2:], 2))\n",
    "        x2 = encode(u[N//2:])\n",
    "        x = np.concatenate((x1, x2))\n",
    "        return x\n",
    "\n",
    "@jit(nopython=True)\n",
    "def rvsl(y: np.ndarray) -> np.ndarray:\n",
    "    N = y.shape[0]\n",
    "    if N == 2:\n",
    "        return y\n",
    "    else:\n",
    "        return np.concatenate((rvsl(y[0:N:2]), rvsl(y[1:N:2])))\n",
    "\n",
    "def data_process(array):\n",
    "    bit_array = numpy_to_bit(array)\n",
    "    array_len = len(bit_array)\n",
    "    current_array = []\n",
    "    for i in range(0, array_len, 512):\n",
    "        sub_array = bit_array[i:i+512]\n",
    "        if len(sub_array) < 512:\n",
    "            padding = np.ones((512 - len(sub_array)), dtype=bit_array.dtype)\n",
    "            sub_array = np.concatenate((sub_array, padding))\n",
    "        current_array.append(sub_array)\n",
    "    current_idx = i // 512 + 1\n",
    "    return current_array, current_idx, array_len\n",
    "\n",
    "def numpy_array_to_udp_packet(bit_array):\n",
    "    # Convert the numpy array of bits to a string of bits\n",
    "    bit_string = ''.join(str(int(bit)) for bit in bit_array)\n",
    "    # Convert the bit string to bytes\n",
    "    byte_array = bytearray(int(bit_string[i:i+8], 2) for i in range(0, len(bit_string), 8))\n",
    "    return byte_array\n",
    "\n",
    "def udp_packet_to_numpy_array(packet):\n",
    "    # Convert the byte array back to a bit string\n",
    "    bit_string = ''.join(format(byte, '08b') for byte in packet)\n",
    "    # Convert the bit string to a numpy array of floats\n",
    "    bit_array = np.array([int(bit) for bit in bit_string], dtype=np.int8)\n",
    "    return bit_array\n",
    "\n",
    "def data_generate(bit_array:np.ndarray, data_idx:np.ndarray) -> np.ndarray:\n",
    "    u=np.zeros(1024,dtype=np.int8)\n",
    "    u[data_idx] = bit_array\n",
    "    x = encode(u)\n",
    "    x = rvsl(x)\n",
    "    return x\n",
    "\n",
    "def codeword_generate(array_dict, data_idx):\n",
    "    split_bit = []\n",
    "    codeword_idx = []\n",
    "    bit_array_len = []\n",
    "    codeword_idx.append(0)\n",
    "    array_process = partial(data_process)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for current_array, current_idx, array_len in executor.map(array_process, [array_dict[name] for name in array_dict]):\n",
    "            split_bit.extend(current_array)\n",
    "            codeword_idx.append(codeword_idx[-1] + current_idx)\n",
    "            bit_array_len.append(array_len)\n",
    "    executor.shutdown(wait=True)\n",
    "    del executor, array_process\n",
    "    time1 = time.time()\n",
    "    encode_partial = partial(data_generate, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        codeword = list(executor.map(encode_partial, split_bit))\n",
    "    executor.shutdown(wait=True)\n",
    "    time2 = time.time()\n",
    "    print(f'Encode time: {time2-time1}')\n",
    "\n",
    "    del executor, encode_partial\n",
    "    return np.array(codeword,dtype=np.int8), codeword_idx, bit_array_len\n",
    "\n",
    "def packet_diffusion(codeword, block_len, packet_idx):\n",
    "    codeword_len = codeword[0].shape[0]\n",
    "    udp_packet = []\n",
    "    for idx, i in enumerate(range(0, codeword_len, block_len)):\n",
    "        # tmp_packet = np.concatenate([tmp_codeword[i:i+block_len] for tmp_codeword in codeword])\n",
    "        tmp_packet = codeword[:, packet_idx[i:i+block_len]].flatten()\n",
    "        tmp_udp_packet = struct.pack(\"I\",idx) + numpy_array_to_udp_packet(tmp_packet)\n",
    "        udp_packet.append(tmp_udp_packet)\n",
    "    return udp_packet\n",
    "\n",
    "def encoder_udp(array_dict, data_idx, block_len, packet_idx):\n",
    "    codeword, codeword_idx, bit_array_len = codeword_generate(array_dict, data_idx)\n",
    "    udp_packet = packet_diffusion(codeword, block_len, packet_idx)\n",
    "    return udp_packet, codeword_idx, bit_array_len\n",
    "\n",
    "\n",
    "\n",
    "def packet_aggregation(udp_packet, packet_idx, block_len, data_idx, freeze_idx, codeword_idx, bit_array_len):\n",
    "    sort_idx = [struct.unpack(\"I\", tmp_packet[:4])[0] for tmp_packet in udp_packet]\n",
    "    packet_data_del = np.array([udp_packet_to_numpy_array(tmp_packet[4:]) for _, tmp_packet in sorted(zip(sort_idx, udp_packet))])\n",
    "    packet_data = np.ones((int(1024/block_len), len(packet_data_del[0])))*0.5\n",
    "    for i, tmp_idx in enumerate(sorted(sort_idx)):\n",
    "        packet_data[tmp_idx] = packet_data_del[i]\n",
    "    \n",
    "    restore_codeword = []\n",
    "    inverse_packet_idx = np.argsort(packet_idx)\n",
    "    for i in range(0, packet_data.shape[1],block_len):\n",
    "        tmp_codeword = packet_data[:,i:i+block_len].flatten()\n",
    "        restore_codeword.append(tmp_codeword[inverse_packet_idx])\n",
    "\n",
    "    decode_partial = partial(decoding, freeze_idx=freeze_idx, data_idx=data_idx)\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        decoding_data = np.array(list(executor.map(decode_partial, restore_codeword)),dtype=np.int8)\n",
    "    del executor, decode_partial\n",
    "\n",
    "    restore_array = []\n",
    "    for i, array_len in enumerate(bit_array_len):\n",
    "        tmp_array = np.concatenate(decoding_data[codeword_idx[i]:codeword_idx[i+1]])[:array_len]\n",
    "        restore_array.append(bit_to_numpy(tmp_array))\n",
    "    return restore_array\n",
    "\n",
    "\n",
    "def decoding(bit_array, freeze_idx, data_idx):\n",
    "    # Prepare the necessary arrays and values\n",
    "    bit_array = 1-2*bit_array\n",
    "    lr0 = np.exp(-(bit_array - 1)**2)\n",
    "    lr1 = np.exp(-(bit_array + 1)**2)\n",
    "    lr0_post = lr0 / (lr0 + lr1)\n",
    "    lr1_post = lr1 / (lr0 + lr1)\n",
    "    delete_num = 1024 - len(bit_array)\n",
    "    hd_dec = np.zeros(1024, dtype=np.float64)\n",
    "    frozen_val = np.zeros(len(freeze_idx), dtype=np.float64)\n",
    "    pro_prun = np.zeros((1, 2 * 1024 + 1), dtype=np.float64)\n",
    "\n",
    "    # Call the optimized Cython function\n",
    "    i_scen_sum, hd_dec_result = cython_sc_decoding(\n",
    "        lr0_post, lr1_post, freeze_idx.astype(np.float64),\n",
    "        hd_dec, 1024, 10, 512, frozen_val, delete_num, 0, pro_prun\n",
    "    )\n",
    "\n",
    "    # Extract the output for data_idx from hd_dec_result\n",
    "    data_out = hd_dec_result[data_idx]\n",
    "    return data_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all weights to numpy arrays\n",
    "weights_dict = {name: param.cpu().detach().numpy() for name, param in net[0].state_dict().items()}\n",
    "N = 1024\n",
    "n = 10\n",
    "rate = 0.5\n",
    "K = round(N*rate)\n",
    "c_1024 = np.load('c_1024.npy')\n",
    "coding_list = scipy.io.loadmat(\"1024-3db-d=2-mean.mat\")[\"count_number\"]\n",
    "coding_index = np.argsort(coding_list[:,1])\n",
    "info_idx = coding_index[:K]\n",
    "freeze_idx = coding_index[K:]\n",
    "\n",
    "# sort the final index\n",
    "info_ni = np.sort(info_idx)\n",
    "freeze_ni = np.sort(freeze_idx)\n",
    "\n",
    "udp_packet, codeword_idx, bit_array_len = encoder_udp(weights_dict, info_ni, 8, c_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(udp_packet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recv_packet = {i:[] for i in range(num_nodes)}\n",
    "for tmp_id in range(num_nodes):\n",
    "    for i in range(len(udp_packet)):\n",
    "        tmp_packet = struct.pack('I',0) + udp_packet[i]\n",
    "        node_s[0].sendall(struct.pack('I',len(tmp_packet))+tmp_packet)\n",
    "        if (i+1) % 16 == 0:\n",
    "            try:\n",
    "                while True:\n",
    "                    server_r.settimeout(3)\n",
    "                    data = server_r.recv(len(tmp_packet))\n",
    "                    server_r.settimeout(0.5)\n",
    "                    node_id = struct.unpack('I',data[:4])[0]\n",
    "                    recv_packet[node_id].append(data[4:])\n",
    "                    # recv_packet.append(data)\n",
    "            except socket.timeout:\n",
    "                # print('Timeout')\n",
    "                # print(len(recv_packet[0]))\n",
    "                server_r.settimeout(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_array = packet_aggregation(recv_packet[0], c_1024, 8, info_ni, freeze_ni, codeword_idx, bit_array_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_dict = {}\n",
    "# restored_array = [torch.tensor(arr).to(device) for arr in restored_array]\n",
    "for i, name in enumerate(weights_dict):\n",
    "    restored_dict[name] = torch.tensor(restored_array[i].reshape(weights_dict[name].shape)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[1].load_state_dict(restored_dict)\n",
    "for i in range(20):\n",
    "    train_model(net[1], train_loader[1], criterion[1], device, scaler[1], optimizer[1], i)\n",
    "    evaluate_model(net[1], test_loader[1], criterion[1], device)\n",
    "    scheduler[1].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[2].load_state_dict(restored_dict)\n",
    "for i in range(20):\n",
    "    train_model(net[2], train_loader[0], criterion[2], device, scaler[2], optimizer[2], i)\n",
    "    evaluate_model(net[2], test_loader[0], criterion[2], device)\n",
    "    scheduler[2].step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(restored_array)):\n",
    "    assert np.array_equal(restored_array[i], list(weights_dict.values())[i].flatten())\n",
    "# np.array_equal(restored_array[1], list(weights_dict.values())[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(weights_dict.values())[1].flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wireless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
